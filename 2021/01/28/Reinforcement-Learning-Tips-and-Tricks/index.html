<!DOCTYPE html>
<html  lang="zh">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Reinforcement Learning:Tips and Tricks - Ebby&#39;s Notes</title>


    <meta name="description" content="摘要： 本篇博文记录强化学习过程中的一些最佳实践的总结，持续更新ing …">
<meta name="keywords" content="人工智能,深度学习,技术,算法">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning:Tips and Tricks">
<meta property="og:url" content="http://blog.a-stack.com/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/index.html">
<meta property="og:site_name" content="Ebby&#39;s Notes">
<meta property="og:description" content="摘要： 本篇博文记录强化学习过程中的一些最佳实践的总结，持续更新ing …">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/blog/rl.jpg">
<meta property="og:updated_time" content="2021-01-29T14:24:28.819Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning:Tips and Tricks">
<meta name="twitter:description" content="摘要： 本篇博文记录强化学习过程中的一些最佳实践的总结，持续更新ing …">
<meta name="twitter:image" content="http://blog.a-stack.com/qnsource/blog/rl.jpg">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/tomorrow-night-eighties.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?684368b6943d7a5b6689dba5e7bf30ad";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo02.png" alt="Reinforcement Learning:Tips and Tricks" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/reading">读书</a>
                
                <a class="navbar-item"
                href="/resources">资源</a>
                
                <a class="navbar-item"
                href="/notebooks">📝</a>
                
                <a class="navbar-item"
                href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/../qnsource/blog/rl.jpg" alt="Reinforcement Learning:Tips and Tricks">
        </span>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                
                    <div class="level-item tag is-danger" style="background-color: #3273dc;">置顶</div>
                

                <time class="level-item has-text-grey" datetime="2021-01-28T02:56:30.000Z">2021-01-28</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/强化学习/">强化学习</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    21 分钟 读完 (大约 3076 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-angle-double-right"></i>Reinforcement Learning:Tips and Tricks
            
        </h1>
        <div class="content">
            <p><strong>摘要：</strong> 本篇博文记录强化学习过程中的一些最佳实践的总结，持续更新ing …</p>
<a id="more"></a>
<h2 id="Current-Limitations-of-RL"><a href="#Current-Limitations-of-RL" class="headerlink" title="Current Limitations of RL"></a>Current Limitations of RL</h2><h3 id="Sample-inefficient"><a href="#Sample-inefficient" class="headerlink" title="Sample inefficient"></a>Sample inefficient</h3><ul>
<li>他们需要大量样本（有时需要数百万次交互）才能学到有用的东西。 这就是为什么RL的大部分成功都是通过游戏或模拟获得的。</li>
<li><p>作为一般建议，为了获得更好的性能，您应该增加Agent的预算（训练时间步数）。</p>
</li>
<li><p>计划谬误说，完成某件事通常比您想象的要花费更长的时间。强化学习有其自身的计划谬误-学习策略通常需要比您想象的更多的样本。</p>
</li>
<li>理论上，强化学习可以适用于任何事物，包括不了解世界模型的环境。但是，这种普遍性是有代价的：很难利用任何可能有助于学习的特定于问题的信息，这迫使您使用大量样本来学习本来可以进行硬编码的内容。</li>
</ul>
<p><img src="/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/rainbow_dqn.png" style="zoom:50%;"></p>
<blockquote>
<p>The y-axis is “median human-normalized score”. This is computed by training 57 DQNs, one for each Atari game, normalizing the score of each agent such that human performance is 100%, then plotting the median performance across the 57 games. RainbowDQN passes the 100% threshold at about <em>18 million</em> frames. This corresponds to about 83 hours of play experience, plus however long it takes to train the model. A lot of time, for an Atari game that most humans pick up within a few minutes.</p>
<p>Mind you, 18 million frames is actually pretty good, when you consider that the previous record (<a href="https://arxiv.org/pdf/1707.06887.pdf">Distributional DQN (Bellemare et al, 2017)</a>) needed 70 million frames to hit 100% median performance, which is about 4x more time. As for the <a href="https://www.nature.com/articles/nature14236">Nature DQN (Mnih et al, 2015)</a>, it never hits 100% median performance, even after 200 million frames of experience.</p>
</blockquote>
<h3 id="Reward函数的设计哲学不是所有人都可以掌握（RewArt）"><a href="#Reward函数的设计哲学不是所有人都可以掌握（RewArt）" class="headerlink" title="Reward函数的设计哲学不是所有人都可以掌握（RewArt）"></a>Reward函数的设计哲学不是所有人都可以掌握（RewArt）</h3><ol>
<li>Atari游戏天生有reward，你不需要为此额外担心，但现实场景却往往并非如此；</li>
<li>设计一个合理的Reward函数不难，难在如何设计一个函数既能实现你的目标又能让算法很好的学到？<ol>
<li>打个OpenAI的比方，做一个电动艇竞赛类游戏，优先到达目的地获胜，为了获胜要收集途中的能量块；</li>
<li>悬崖的复杂版本，可能选择自杀是最有效的方式；</li>
</ol>
</li>
<li>即使你有一个好的reward函数，逃离局部最优仍然是一件困难的事情；</li>
</ol>
<h3 id="稳定性与泛化性能"><a href="#稳定性与泛化性能" class="headerlink" title="稳定性与泛化性能"></a>稳定性与泛化性能</h3><ol>
<li>Overfitting 影响了泛化性能；</li>
<li>训练结果的稳定性没有定论；<ol>
<li>比如Pong的Case，不同开局结果往往不一样，波动比较严重；</li>
<li>RL对您的初始化和培训过程的动态非常敏感，因为您的数据始终是在线收集的，并且您获得的唯一监督就是单个标量来获得奖励。</li>
</ol>
</li>
</ol>
<blockquote>
<p><strong>When your training algorithm is both sample inefficient and unstable, it heavily slows down your rate of productive research.</strong> Maybe it only takes 1 million steps. But when you multiply that by 5 random seeds, and then multiply that with hyperparam tuning, you need an exploding amount of compute to test hypotheses effectively.</p>
</blockquote>
<h2 id="What’s-important-in-RL"><a href="#What’s-important-in-RL" class="headerlink" title="!!What’s important in RL"></a>!!What’s important in RL</h2><ol>
<li><p>在强化学习领域，数据仍然是最重要的一环，我们需要重点考虑的问题是：如何保证在环境交互中获取数据的质量？</p>
<blockquote>
<ol>
<li>比如这些过程中，如果存在一些<code>Trajectories</code>存在零回报；</li>
<li>对于强化学习的训练来说，最耗时的部分是和环境交互获取数据的部分，所以从这个意义上讲，经验回放机制对训练效率提升有一定的价值，很多并行训练的机制也在于保障这个环节性能的提升；</li>
<li>RL的优化目标是一个<code>Trajectories</code>中的累积汇报最大，但中间某些bad case会被放大，解决这个问题也有助于提升算法收敛效率；</li>
</ol>
</blockquote>
</li>
<li><p>超参数优化同样十分重要</p>
<blockquote>
<ol>
<li>不要奢望使用默认参数就可以轻易解决你自己环境的问题；</li>
<li>So 如何高效优化超参数是必须掌握的技能</li>
</ol>
</blockquote>
</li>
<li><p>仿真环境的构建</p>
<blockquote>
<p>尤其是Reward机制的设计，是玄学的玄学 …</p>
</blockquote>
</li>
</ol>
<h2 id="仿真环境构建"><a href="#仿真环境构建" class="headerlink" title="仿真环境构建"></a>仿真环境构建</h2><h3 id="初始阶段："><a href="#初始阶段：" class="headerlink" title="初始阶段："></a>初始阶段：</h3><ul>
<li><strong>不要一步到位，先写一个简化版的训练环境</strong>。把任务难度降到最低，确保一定能正常训练。</li>
<li>记下这个正常训练的智能体的分数，<strong>与随机动作、传统算法得到的分数做比较</strong>。DRL算法的分数应该明显高于随机动作（随机执行动作）。DRL算法不应该低于传统算法的分数。如果没有传统算法，那么也需要自己写一个局部最优的算法（就算只比随机动作的算法高一点点都可以，有能力的情况下，要尽量写好）。</li>
<li>评估策略的性能: 大部分情况下，可以直接是对Reward Function 给出的reward 进行求和得到的每轮收益episode return作为策略评分。有时候可以需要直接拿策略的实际分数作为评分（移动速度/股票收益/目标完成情况 等）。</li>
<li>需要保证这个简化版的代码：高效、简洁、可拓展</li>
</ul>
<h3 id="改进阶段："><a href="#改进阶段：" class="headerlink" title="改进阶段："></a>改进阶段：</h3><ul>
<li><strong>让任务难度逐步提高</strong>，对训练环境env 进行缓慢的修改，时刻保存旧版本的代码</li>
<li><strong>同步微调 Reward Function</strong>，可以直接代入自己的人类视角，为某些行为添加正负奖励。注意奖励的平衡（有正有负）。注意不要为Reward Function 添加太多额外规则，时常回过头取消一些规则，避免过度矫正。</li>
<li><strong>同步微调 DRL算法，只建议微调超参数</strong>，但不建议对算法核心进行修改。因为任务变困难了，所以需要调整超参数让训练变快。同时摸清楚在这个训练环境下，算法对哪几个超参数是敏感的。有时候为了节省时间，甚至可以为 off-policy 算法保存一些典型的 trajectory（不建议在最终验证阶段使用）。</li>
<li>每一次修改，都需要跑一下记录不同方法的分数，确保：<strong>随机动作 &lt; 传统方法 &lt; DRL算法</strong>。这样才能及时发现代码逻辑上的错误。要极力避免代码中出现复数个的错误，因为极难排查。</li>
</ul>
<h2 id="收尾阶段："><a href="#收尾阶段：" class="headerlink" title="收尾阶段："></a>收尾阶段：</h2><ul>
<li>尝试慢慢删掉Reward Function 中一些比较复杂的东西，删不掉就算了。</li>
<li>选择高低两组超参数再跑一次，确认没有优化空间。</li>
</ul>
<h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><ol>
<li>快速开始实验</li>
<li>超参数优化</li>
<li>可视化、可解释性分析</li>
<li>处理过拟合问题</li>
<li>选择难度适中的baseline（Pong？）</li>
</ol>
<blockquote>
<p>算法选择：</p>
<ol>
<li>状态空间、动作空间的连续/离散性；</li>
<li>是否支持分布式并行运算？</li>
</ol>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法名称</th>
<th>类型</th>
<th>动作空间</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q-learning（1992）</td>
<td>Value Based(Classical)</td>
<td>离散</td>
<td></td>
</tr>
<tr>
<td>Deep Q-Network(DQN)（2015）</td>
<td>Value Based</td>
<td>离散</td>
<td></td>
</tr>
<tr>
<td>Dueling DQN（2015）</td>
<td>Value Based</td>
<td>离散</td>
<td></td>
</tr>
<tr>
<td>Double DQN（2016）</td>
<td>Value Based</td>
<td>离散</td>
<td></td>
</tr>
<tr>
<td>REINFORCE（Policy Gradient）（2011）</td>
<td>Policy Based</td>
<td>离散/连续</td>
<td></td>
</tr>
<tr>
<td>TRPO（Trust Region Policy Optimization）（2015）</td>
<td>Policy Based</td>
<td>离散/连续</td>
<td></td>
</tr>
<tr>
<td>PPO（Proximal Policy Optimization）（2017）</td>
<td>Policy Based</td>
<td>离散/连续</td>
<td></td>
</tr>
<tr>
<td>Actor-Critic（AC）(2000)</td>
<td>Actor-Crtic</td>
<td>离散/连续</td>
<td></td>
</tr>
<tr>
<td>Asynchronous Advantage Actor-Critic(A3C)</td>
<td>Actor-Crtic</td>
<td>离散/连续</td>
<td></td>
</tr>
<tr>
<td>DDPG(2018)</td>
<td>Actor-Crtic</td>
<td>离散/连续</td>
<td></td>
</tr>
<tr>
<td>TD3(2018)</td>
<td>Actor-Crtic</td>
<td>离散/连续</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/image-20210128223956522.png" alt="image-20210128223956522"></p>
<h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><h3 id="off-policy"><a href="#off-policy" class="headerlink" title="off-policy"></a>off-policy</h3><ol>
<li><p>【<strong>网络宽度、网络层数】</strong>越复杂的函数就需要越大容量的神经网络去拟合。在需要训练1e6步的任务中，我一般选择 宽度128、256，层数小于8的网络；</p>
</li>
<li><p><strong>过大、过深的神经网络不适合DRL</strong>，因为：</p>
<ul>
<li>深度学习可以在整个训练结束后再使用训练好的模型。而强化学习需要在几秒钟的训练后马上使用刚训好的模型。这导致DRL只能用比较浅的网络来保证<strong>快速拟合</strong>（10层以下）</li>
<li>并且强化学习的训练数据不如有监督学习那么稳定，无法划分出训练集测试集去避免过拟合，因此DRL也不能用太宽的网络（超过1024），避免参数过度冗余导致<strong>过拟合</strong>。</li>
</ul>
</li>
<li><p><strong>【记忆容量】</strong>经验回放缓存 experimence replay buffer 的最大容量 max capacity，如果超过容量限制，它就会删掉最早的记忆。在简单的任务中（训练步数小于1e6），对于探索能力强的DRL算法，通常在缓存被放满前就训练到收敛了，<strong>不需要删除任何记忆</strong>。然而，过大的记忆也会拖慢训练速度，我一般会先从默认值 2 <strong> 17 ~ 2 </strong> 20 开始尝试，如果环境的随机因素大，我会同步增加记忆容量 与 batch size、网络更新次数，直到逼近服务器的内存、显存上限（放在显存训练更快）</p>
</li>
<li><p><strong>【批次大小、更新次数】</strong>一般我会选择与网络宽度相同、或略大的批次大小batch size。我一般从128、256 开始尝试这些2的N次方。</p>
</li>
<li><p><strong>【折扣因子】</strong>discount factor（或者叫 discount-rate parameter），gamma 。这个值很容易确定，请回答“你希望你的智能体每做出一步，至少需要考虑接下来多少步的reward？”如果是t 步：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">gamma ** t = <span class="number">0.1</span>  <span class="comment"># 0.1 对于当前这一步来说，t步后的reward的权重</span></span><br><span class="line">gamma = <span class="number">0.1</span> ** (<span class="number">1</span>/t)</span><br><span class="line"></span><br><span class="line"><span class="number">0.93</span>  ~= <span class="number">0.1</span> ** (<span class="number">1</span>/<span class="number">32</span>)</span><br><span class="line"><span class="number">0.98</span>  ~= <span class="number">0.1</span> ** (<span class="number">1</span>/<span class="number">128</span>)</span><br><span class="line"><span class="number">0.99</span>  ~= <span class="number">0.1</span> ** (<span class="number">1</span>/<span class="number">256</span>) </span><br><span class="line"><span class="number">0.995</span> ~= <span class="number">0.1</span> ** (<span class="number">1</span>/<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">可以看到 <span class="number">0.93</span>, <span class="number">0.98</span>, <span class="number">0.99</span>, <span class="number">0.995</span> 的gamma值</span><br><span class="line">分别对应   <span class="number">32</span>,  <span class="number">128</span>,  <span class="number">256</span>,   <span class="number">512</span> 的步数</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="on-policy"><a href="#on-policy" class="headerlink" title="on-policy"></a>on-policy</h3><ol>
<li><strong>【记忆容量】</strong>on-policy 算法每轮更新后都需要删除“用过的数据”，所以on-policy的记忆容量应该大于等于【单轮更新的采样步数】</li>
<li><strong>【批次大小】</strong>on-policy 算法比off-policy更像深度学习，它可以采用稍大一点的学习率（2e-4）。</li>
</ol>
<h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><h3 id="选择RL来实现你的问题需要慎重考虑，并非只有RL才可以解决序列决策问题；"><a href="#选择RL来实现你的问题需要慎重考虑，并非只有RL才可以解决序列决策问题；" class="headerlink" title="选择RL来实现你的问题需要慎重考虑，并非只有RL才可以解决序列决策问题；"></a>选择RL来实现你的问题需要慎重考虑，并非只有RL才可以解决序列决策问题；</h3><ol>
<li><p>经验法则是，除极少数情况外，特定领域的算法比强化学习更快，更好。</p>
</li>
<li><p>Boston Dynamics并非使用RL？</p>
<ol>
<li><p><a href="https://dspace.mit.edu/openaccess-disseminate/1721.1/110533">time-varying LQR, QP solvers, and convex optimization</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

</li>
</ol>
</li>
</ol>
<h3 id="如果选择强化学习，该满足什么样的要求？"><a href="#如果选择强化学习，该满足什么样的要求？" class="headerlink" title="如果选择强化学习，该满足什么样的要求？"></a>如果选择强化学习，该满足什么样的要求？</h3><ol>
<li>场景很容易产生几乎无限的经验数据。</li>
<li>该问题被简化为更简单的形式。<ul>
<li>由极简的问题开始逐步处理，检查可行性；</li>
</ul>
</li>
<li>有一种方法可以将自我游戏引入学习中。<ul>
<li>So far, that setting seems to have the most stable and well-performing behavior.</li>
</ul>
</li>
<li>有一种干净的方法来定义可学习，而非游戏的奖励。</li>
<li>如果必须确定奖励，它至少应该是丰富的（尽量多的及时奖励）。</li>
</ol>
<h3 id="如果从事RL研究，可以再如下领域多些想象力："><a href="#如果从事RL研究，可以再如下领域多些想象力：" class="headerlink" title="如果从事RL研究，可以再如下领域多些想象力："></a>如果从事RL研究，可以再如下领域多些想象力：</h3><ol>
<li>RL其实不需要遍历或者推理出最优的策略，只要是一个比人类更优的近优策略就足够了！</li>
<li>期待硬件水平的增速，抹平一切对算法的需求；</li>
<li>Model Based + Model Free + Deep Learning</li>
<li>像AlphaGo那样使用RL，RL只是其中的一个环节，解决的是fine-tune的问题；</li>
<li>设计Reward函数太难，设计一个学习reward函数的算法生成reward，比如imitation learning/inverse learning</li>
<li>什么时候可以像使用迁移学习一样使用RL？</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="http://joschu.net/docs/nuts-and-bolts.pdf">The Nuts and Bolts of Deep RL Research</a></li>
<li><a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html">stable baseline3</a></li>
<li><a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Deep Reinforcement Learning Doesn’t Work Yet</a></li>
</ol>

        </div>
        
        <hr style="height:1px;margin:1rem 0"/>
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/人工智能/">人工智能</a>,&nbsp;<a class="has-link-grey -link" href="/tags/强化学习/">强化学习</a>,&nbsp;<a class="has-link-grey -link" href="/tags/心得体会/">心得体会</a>,&nbsp;<a class="has-link-grey -link" href="/tags/算法/">算法</a>
                </div>
            </div>
        </div>
        
        
        
        
<div class="sharethis-inline-share-buttons"></div>
<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=5e762f775039a80012d346d9&amp;product=inline-share-buttons&amp;cms=sop' async='async'></script>

        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/site/alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/site/weixin.png" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/11/19/pytorch模型的导出与部署/">
                <span class="level-item">pytorch模型的导出与部署</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: false,
        verify: false,
        app_id: 'JQMgg0zbFBNWwL0lLnq2s1G7-gzGzoHsz',
        app_key: 'm5FMVedFNxGutQCnMsVMAaXM',
        placeholder: 'Say Something ...'
    });
</script>

    </div>
</div>
</div>
                
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-3 column-right is-sticky">
    
        

    <div class="card widget" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Current-Limitations-of-RL">
        <span class="has-mr-6">1</span>
        <span>Current Limitations of RL</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Sample-inefficient">
        <span class="has-mr-6">1.1</span>
        <span>Sample inefficient</span>
        </a></li><li>
        <a class="is-flex" href="#Reward函数的设计哲学不是所有人都可以掌握（RewArt）">
        <span class="has-mr-6">1.2</span>
        <span>Reward函数的设计哲学不是所有人都可以掌握（RewArt）</span>
        </a></li><li>
        <a class="is-flex" href="#稳定性与泛化性能">
        <span class="has-mr-6">1.3</span>
        <span>稳定性与泛化性能</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#What’s-important-in-RL">
        <span class="has-mr-6">2</span>
        <span>!!What’s important in RL</span>
        </a></li><li>
        <a class="is-flex" href="#仿真环境构建">
        <span class="has-mr-6">3</span>
        <span>仿真环境构建</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#初始阶段：">
        <span class="has-mr-6">3.1</span>
        <span>初始阶段：</span>
        </a></li><li>
        <a class="is-flex" href="#改进阶段：">
        <span class="has-mr-6">3.2</span>
        <span>改进阶段：</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#收尾阶段：">
        <span class="has-mr-6">4</span>
        <span>收尾阶段：</span>
        </a></li><li>
        <a class="is-flex" href="#算法设计">
        <span class="has-mr-6">5</span>
        <span>算法设计</span>
        </a></li><li>
        <a class="is-flex" href="#调参">
        <span class="has-mr-6">6</span>
        <span>调参</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#off-policy">
        <span class="has-mr-6">6.1</span>
        <span>off-policy</span>
        </a></li><li>
        <a class="is-flex" href="#on-policy">
        <span class="has-mr-6">6.2</span>
        <span>on-policy</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#建议">
        <span class="has-mr-6">7</span>
        <span>建议</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#选择RL来实现你的问题需要慎重考虑，并非只有RL才可以解决序列决策问题；">
        <span class="has-mr-6">7.1</span>
        <span>选择RL来实现你的问题需要慎重考虑，并非只有RL才可以解决序列决策问题；</span>
        </a></li><li>
        <a class="is-flex" href="#如果选择强化学习，该满足什么样的要求？">
        <span class="has-mr-6">7.2</span>
        <span>如果选择强化学习，该满足什么样的要求？</span>
        </a></li><li>
        <a class="is-flex" href="#如果从事RL研究，可以再如下领域多些想象力：">
        <span class="has-mr-6">7.3</span>
        <span>如果从事RL研究，可以再如下领域多些想象力：</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#参考">
        <span class="has-mr-6">8</span>
        <span>参考</span>
        </a></li></ul>
            </div>
        </div>
    </div>

    
        <div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/blog/rl.jpg" alt="Reinforcement Learning:Tips and Tricks">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2021-01-28T02:56:30.000Z">2021-01-28</time></div>
                    <a href="/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Reinforcement Learning:Tips and Tricks</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/强化学习/">强化学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/11/19/pytorch模型的导出与部署/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="pytorch模型的导出与部署">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-11-19T05:00:18.000Z">2020-11-19</time></div>
                    <a href="/2020/11/19/pytorch模型的导出与部署/" class="title has-link-black-ter is-size-6 has-text-weight-normal">pytorch模型的导出与部署</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/05/04/读书笔记-《神经网络与深度学习》/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/blog/qxp.jpg" alt="读书笔记-《神经网络与深度学习》">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-05-04T13:50:21.000Z">2020-05-04</time></div>
                    <a href="/2020/05/04/读书笔记-《神经网络与深度学习》/" class="title has-link-black-ter is-size-6 has-text-weight-normal">读书笔记-《神经网络与深度学习》</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/04/19/计算机视觉目标检测研究札记/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/banner/02.jpg" alt="计算机视觉目标检测研究札记">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-04-19T07:10:45.000Z">2020-04-19</time></div>
                    <a href="/2020/04/19/计算机视觉目标检测研究札记/" class="title has-link-black-ter is-size-6 has-text-weight-normal">计算机视觉目标检测研究札记</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/03/30/目标检测中的Anchor与感受野/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/blog/anchor.jpg" alt="目标检测中的Anchor与感受野">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-03-30T13:14:50.000Z">2020-03-30</time></div>
                    <a href="/2020/03/30/目标检测中的Anchor与感受野/" class="title has-link-black-ter is-size-6 has-text-weight-normal">目标检测中的Anchor与感受野</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo02.png" alt="Reinforcement Learning:Tips and Tricks" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Ebby DD&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>  沪ICP备20005404号
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://blog.a-stack.com',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>