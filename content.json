{"meta":{"title":"Ebby's Notes","subtitle":"=Blog for AI Learning=","description":"记录人工智能学习路径","author":"Ebby DD","url":"http://blog.a-stack.com"},"pages":[{"title":"404-找不到页面","date":"2016-09-03T09:17:18.000Z","updated":"2018-05-15T14:59:56.082Z","comments":false,"path":"404.html","permalink":"http://blog.a-stack.com/404.html","excerpt":"","text":"404 Not Found对不起，您所访问的页面不存在或者已删除 你可以点击此处返回首页.我的Github：https://github.com/ddebby或者给我发邮件：ebby.dd@gmail.com ​ 因为期待得太苦，会导致得到的时候分外甜"},{"title":"","date":"2018-07-21T05:43:22.569Z","updated":"2018-07-20T09:10:32.266Z","comments":true,"path":"about/index.html","permalink":"http://blog.a-stack.com/about/index.html","excerpt":"","text":"ProfileStay hungry，stay foolish. —— Steve Jobs 关于本站 为了记录人工智能技术学习路径和分享知识在2018年3月开始尝试建立关于人工智能的博客，连续更换多个Hexo的主题，为了折腾更符合文字特质的内容呈现方式。 如果对混合云技术感兴趣，可以前往2017年创立的混合云博客: 『Azure Stack’s Notes』 关于我 我是『Ebby』，云架构师，人工智能入门级科学家 最近迷恋深度学习技术"},{"title":"","date":"2018-07-21T05:43:22.569Z","updated":"2018-07-20T09:16:27.166Z","comments":true,"path":"reading/books.css","permalink":"http://blog.a-stack.com/reading/books.css","excerpt":"","text":".clear { clear: both } #main { /*padding-left: 105px; padding-top: 47px; padding-bottom: 30px;*/ clear: both; /* background: url(images/paper.jpg) left top #F2EDB4;*/ position: relative; /* z-index: 11;*/ box-shadow: 5px 5px 8px #888; margin-top: 50px; margin-bottom: 50px; } /* #main h2 { line-height: 30px; font-size: 17px; margin-bottom: 10px; position: relative; } #main h2 em { font-size: 14px; margin-left: 20px; } #main h2 em span,#main h2 em img { vertical-align: text-bottom; } #main h2 em a { color: #000 } #main h2 em a:hover { color: #1C5F8C; text-decoration: underline } #main h2 span.ad { position: absolute; top: -22px; left: 250px }*/ #main .tag { margin-bottom: 20px; height: 30px; line-height: 30px; font-size: 15px } #main .tag a { padding: 4px 12px } #book { /* margin: -47px 0 -30px -105px;*/ display: table; width: 100%; /*background: #F5F0B6*/ } #book h2 { height: 16px; margin: 0; /*padding: 45px 0 0 105px;*/ background: url(images/bookshelftop.jpg) no-repeat left top; color: #000!important; background-size: cover; } #book ul { float: left; height: auto; background: url(images/bookshelfmiddle.jpg) repeat-y left top; margin-bottom: 0; background-size: 100% 196px; -webkit-margin-before: 0em; -webkit-margin-after: 0em; -webkit-padding-start: 0em; padding: 0 5%; width: 90%; } #book ul li { list-style-type: none; /* height: 160px;*/ /* line-height: 160px;*/ vertical-align: bottom; width: 110px; float: left; margin: 17px 14px 0; padding-bottom: 19px; } #book ul li a { display: block; height: 160px; line-height: 160px; position: relative; } #book ul li img { width: 110px; vertical-align: bottom; box-shadow: 5px 0 6px #512301; position: absolute; bottom: 0; -webkit-transition: all .2s linear; transition: all .2s linear; } #book ul li img:hover { -webkit-box-shadow: 0 15px 30px rgba(0,0,0,0.1); box-shadow: 5px 15px 30px rgba(0,0,0,0.4); -webkit-transform: translate3d(0, -2px, 0); transform: translate3d(0, -2px, 0); }"},{"title":"","date":"2018-07-21T05:43:22.569Z","updated":"2018-07-18T12:43:04.356Z","comments":false,"path":"categories/index.html","permalink":"http://blog.a-stack.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2018-07-21T05:43:22.570Z","updated":"2018-07-20T09:36:03.611Z","comments":true,"path":"reading/library.html","permalink":"http://blog.a-stack.com/reading/library.html","excerpt":"","text":"我的书架"},{"title":"书单","date":"2018-07-02T03:30:53.000Z","updated":"2018-07-20T09:34:17.743Z","comments":true,"path":"reading/index.html","permalink":"http://blog.a-stack.com/reading/index.html","excerpt":"","text":"function setIframeHeight(iframe) { if (iframe) { var iframeWin = iframe.contentWindow || iframe.contentDocument.parentWindow; if (iframeWin.document.body) { iframe.height = iframeWin.document.documentElement.scrollHeight || iframeWin.document.body.scrollHeight; } } }; setIframeHeight(document.getElementsByTagName('iframe')[0]); window.onload = function () { setIframeHeight(document.getElementsByTagName('iframe')[0]); };"},{"title":"常用资源","date":"2018-07-18T12:10:27.000Z","updated":"2018-07-18T13:09:09.498Z","comments":true,"path":"resources/bak-index.html","permalink":"http://blog.a-stack.com/resources/bak-index.html","excerpt":"","text":"@column-3{ @card{ Website Log History Architecture Dependency } @card{ Resources 人工智能学习资源汇总 Pandas Numpy Scikit-learn算法 } @card{ Reading Plan记录读书规划，抓住时间，提升效率！ } } Reading Plan@card{ 记录读书规划，抓住时间，提升效率！ } Photography Collection@card{ 个人曾经的摄影作品集合，欢迎欣赏~ }"},{"title":"资源导航","date":"2018-07-21T05:43:22.570Z","updated":"2018-07-20T09:43:39.321Z","comments":false,"path":"resources/index.html","permalink":"http://blog.a-stack.com/resources/index.html","excerpt":"","text":"Website Log History Architecture Dependency Resources 人工智能学习资源汇总 Pandas Numpy Scikit-learn算法 Reading Plan记录读书规划，抓住时间，提升效率！ Photography Collection个人曾经的摄影作品集合，欢迎欣赏~"},{"title":"","date":"2018-07-21T05:43:22.576Z","updated":"2018-07-18T12:42:51.313Z","comments":false,"path":"tags/index.html","permalink":"http://blog.a-stack.com/tags/index.html","excerpt":"","text":""},{"title":"NumPy","date":"2018-02-22T15:53:23.000Z","updated":"2018-04-13T04:44:09.000Z","comments":true,"path":"resources/numpy/index.html","permalink":"http://blog.a-stack.com/resources/numpy/index.html","excerpt":"","text":"NumPy: NumPy系统是Python的一种开源的数值计算扩展。这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表（nested list structure)结构要高效的多。 更多信息 NumPy Tutorial NumPy Cheat Sheet: Data Analysis in Python"},{"title":"Pandas","date":"2018-02-22T15:53:23.000Z","updated":"2018-04-13T04:44:09.000Z","comments":true,"path":"resources/pandas/index.html","permalink":"http://blog.a-stack.com/resources/pandas/index.html","excerpt":"","text":"Pandas：Python结构化数据分析利器 … 更多信息 Python Data Analysis Library Pandas Cheat Sheet: Data Wrangling in Python ​"},{"title":"Architecture","date":"2018-07-21T05:43:22.982Z","updated":"2018-04-13T04:44:09.000Z","comments":false,"path":"resources/architecture/index.html","permalink":"http://blog.a-stack.com/resources/architecture/index.html","excerpt":"","text":"本文将用于更新本网站的基础架构及其使用的技术。 概要 使用主要技术 技术 用途 Hexo Hexo is a fast, simple &amp; powerful blog framework powered by Node.js. GitHub 国外服务托管 Coding.net 国内服务托管 + 源代码托管 万网 域名托管 Icons LICENSE Hexo Site CC BY 4.0 GitHub Site Wercker Site"},{"title":"人工智能相关资源汇总","date":"2018-04-14T16:00:00.000Z","updated":"2018-05-19T14:47:55.693Z","comments":true,"path":"resources/resources/index.html","permalink":"http://blog.a-stack.com/resources/resources/index.html","excerpt":"","text":"Every Day Readpyimagesearch blog 机器视觉、人工智能领域相关博客，动手实践性强，也是《Deep Learning for Computer Vision with Python》作者撰写的博客 Reddit-DeepLearning || Reddit-MachineLearning 跟踪最新的Deep Learning新闻咨询 Arxiv Sanity Preserver 最新深度学习领域的文章 THE WILD WEEK IN AI NEWSLETTER The Wild Week in AI is a weekly newsletter with hand-curated stories in Deep Learning and Artificial Intelligence. It highlights interesting articles, code projects and research papers. It’s sent out every Monday. Distill Distill is dedicated to clear explanations of machine learning CoursesCS231n: Convolutional Neural Networks for Visual Recognition Stanford Cs231n,课程大纲详见 笔记：Lecture 2 | Lecture 3 | Lecture 4 | Lecture 5 | Machine Learning Coursera Course 作业解析及总结 笔记 | 实验笔记 BooksDeep Learning: An MIT Press book 深度学习理论学习圣经 常用工具及技术 ​"},{"title":"读书计划","date":"2018-07-21T05:43:22.983Z","updated":"2018-05-25T07:45:33.186Z","comments":false,"path":"resources/reading/index.html","permalink":"http://blog.a-stack.com/resources/reading/index.html","excerpt":"","text":"正在读 《Machine Learning：A Pobabilistic Perspective》 MLAPP, 机器学习经典理论教程。 Start Reading Date： 2018-05-25 关注资源(2018) 开源书单 机器学习入门到进阶 工具 Pandas 10 Minutes to pandas Pandas Cookbook Learn Pandas Matplotlib用户指南 scikit-learn 中译 Markdown-简单的世界 Keras [Python] A Byte of Python | 中译 Think Python Version 2 | 中译 Python 数据科学入门 | 中译 💖[Machine Learning] 💖[Data Analysis with Pandas] 💖[Data Visualization] 🕙Image and Video Analysis 人工智能 ✅Neural Networks and Deep Learning, Michael Nielsen | 中译 面向程序员的数据挖掘指南 ✅Machine learning for humans | 中译 ✅机器学习原来这么有趣 | 中译 🕙Pattern Recognization and Maching Learning | 中译：模式识别与机器学习 算法 💖十大经典排序算法 其它 GitHub Aesome 系列 Awesome Machine Learning ⭐32778 | 中译 《指数基金投资指南2017版》 2018年读的书 Deep Learning for Computer Vision with Python Hands On Machine Learning with Scikit Learn and TensorFlow Deep Learning with Keras Deep Learning with Python Neural Network and Deep Learning——Michale Neilsen 智能时代——吴军 机器学习——周志华 统计学方法———李航 | 读书笔记 Considering TensorFlow for the Enterprise 2018的论文研读计划2017年读书计划正在读 [x] 腾讯传1998-2016：中国互联网公司进化论-吴晓波 | 书评 [x] Pro Git - Cott Chacon,Ben Straub | 书评 [x] 硅谷之谜-吴军 准备读 [ ] SRE：Google运维解密 [x] 腾讯传1998-2016：中国互联网公司进化论-吴晓波 [x] 失控 : 全人类的最终命运和结局 [x] 黑客与画家 : 硅谷创业之父Paul Graham文集 [ ] 创业维艰 : 如何完成比难更难的事 [ ] 重新定义公司 : 谷歌是如何运营的 [ ] 创业时, 我们在知乎聊什么? [ ] SDN and openflow"},{"title":"NumPy","date":"2018-02-22T15:53:23.000Z","updated":"2018-05-15T13:50:39.304Z","comments":true,"path":"resources/scikit-learn/index.html","permalink":"http://blog.a-stack.com/resources/scikit-learn/index.html","excerpt":"","text":"Scikit-learn是基于Python的功能强大的开源科学计算工具包，内含分类、回归、聚类、支持向量机、随机森林与Gradient Boosting等算法。 更多信息 Scikit-Learn 官网 Scikit-Learn Cheat Sheet: Python Machine Learning"},{"title":"建站日志","date":"2018-02-19T15:53:23.000Z","updated":"2018-07-21T04:27:32.797Z","comments":true,"path":"resources/weblog/index.html","permalink":"http://blog.a-stack.com/resources/weblog/index.html","excerpt":"","text":"2018年2月重启hexo博客，用于记录人工智能的学习过程 … 2018年7月20日，再次更换主题：hexo-theme-hiker 安装手册详见： https://github.com/iTimeTraveler/hexo-theme-hiker/blob/master/README.cn.md 镜像博客详见： https://itimetraveler.github.io/ 镜像博客配置详见： https://github.com/iTimeTraveler/iTimeTraveler.github.io/tree/site 2018年7月，再次更换主题hexo-theme-indigo 安装手册详见： https://github.com/yscoder/hexo-theme-indigo/wiki/%E5%AE%89%E8%A3%85 2018年5月，选择使用主题hexo-theme-hueman 2018年4月，发现hexo主题Tranquilpeak更新到2.0版本了，尝试切换。 参考：http://www.istarx.cc/ 参考文档： https://github.com/cherlas/hexo-theme-tranquilpeak/blob/master/docs/user.md hiker主题的个性化修改清单[2018-07]1. 增加评论系统valine 参考： https://valine.js.org/quickstart.html 注册Leancloud账号 我们的评论系统其实是放在Leancloud上的，因此首先需要去注册一个账号 Leancloud官网，点我注册 注册完以后需要创建一个应用，名字可以随便起，然后 进入应用-&gt;设置-&gt;应用key 获取你的appid 和 appkey 同时记得在Leancloud -&gt; 设置 -&gt; 安全中心 -&gt; Web 安全域名 把你的域名加进去,如果本地测试需要增加localhost域名 主题配置文件增加valine配置项 12345678910#@ebby Valine https://valine.js.orgvaline: appid: =??= appkey: =??= verify: false #验证码 notify: false #评论回复提醒 avatar: '' #评论列表头像样式：''/mm/identicon/monsterid/wavatar/retro/hide avatar_cdn: 'https://sdn.geekzu.org/avatar/' #头像CDN placeholder: '瞎白话' #评论框占位符 pageSize: 15 #评论分页 在layoout/_partial/comment.ejs中增加如下内容： 12345678910111213141516171819202122--- a/layout/_partial/comment.ejs+++ b/layout/_partial/comment.ejs@@ -105,4 +105,19 @@ &lt;!-- City版安装代码已完成 --&gt; &lt;/div&gt;+&lt;% &#125; else if (theme.valine &amp;&amp; theme.valine.appid &amp;&amp; theme.valine.appkey) &#123;%&gt;+ &lt;!-- valine @ebby --&gt;+ &lt;section id=\"comments\" class=\"comments\"&gt;+ &lt;style&gt;+ .comments&#123;margin:30px;padding:10px;background:#fff&#125;+ @media screen and (max-width:800px)&#123;.comments&#123;margin:auto;padding:10px;background:#fff&#125;&#125;+ &lt;/style&gt;+ &lt;%- partial('post/valine', &#123;+ key: post.slug,+ title: post.title,+ url: config.url+url_for(post.path)+ &#125;) %&gt;++&lt;/section&gt; &lt;% &#125; %&gt;+ 在layout/_partial/post目录下新增valine.ejs文件： 1234567891011121314151617181920&lt;div id=&quot;vcomment&quot; class=&quot;comment&quot;&gt;&lt;/div&gt;&lt;script src=&quot;//cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;//cdn.jsdelivr.net/npm/leancloud-storage@latest/dist/av-min.js&quot;&gt;&lt;/script&gt;&lt;script src=&apos;//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js&apos;&gt;&lt;/script&gt;&lt;script&gt; var notify = &apos;&lt;%= theme.valine.notify %&gt;&apos; == true ? true : false; var verify = &apos;&lt;%= theme.valine.verify %&gt;&apos; == true ? true : false; new Valine(&#123; av: AV, el: &apos;#vcomment&apos;, notify: notify, verify: verify, app_id: &quot;&lt;%= theme.valine.appid %&gt;&quot;, app_key: &quot;&lt;%= theme.valine.appkey %&gt;&quot;, placeholder: &quot;&lt;%= theme.valine.placeholder %&gt;&quot;, avatar: &quot;&lt;%= theme.valine.avatar %&gt;&quot;, avatar_cdn: &quot;&lt;%= theme.valine.avatar_cdn %&gt;&quot;, pageSize: &lt;%= theme.valine.pageSize %&gt; &#125;);&lt;/script&gt; 在layout/_partial/article.ejs中做简单修改 1234567891011--- a/layout/_partial/article.ejs+++ b/layout/_partial/article.ejs@@ -30,7 +30,7 @@ &lt;% if (!index &amp;&amp; theme.copyright.enable)&#123; %&gt; &lt;%- partial('copyright') %&gt; &lt;% &#125; %&gt;- &lt;% if (!index &amp;&amp; post.comments &amp;&amp; (theme.gentie_productKey || theme.duoshuo_shortname || theme.disqus_shortname || theme.uyan_uid || theme.wumii || theme.livere_shortname))&#123; %&gt;+ &lt;% if (!index &amp;&amp; post.comments &amp;&amp; (theme.gentie_productKey || theme.duoshuo_shortname || theme.disqus_shortname || theme.uyan_uid || theme.wumii || theme.livere_shortname ||theme.valine))&#123; %&gt; &lt;%- partial('comment') %&gt; &lt;% &#125; %&gt; &lt;% if (!theme.post_excerpt)&#123; %&gt; 其它配置 头像配置 目前非自定义头像有以下7种默认值可选: 参数值 表现形式 备注 空字符串&#39;&#39; Gravatar官方图形 mp 神秘人(一个灰白头像) identicon 抽象几何图形 monsterid 小怪物 wavatar 用不同面孔和背景组合生成的头像 retro 八位像素复古头像 robohash 一种具有不同颜色、面部等的机器人 hide 不显示头像 2.自定义的修改内容 针对一些fonts文件访问慢，下载并修改为CDN存储位置，涉及文件包括: 123456789101112++ /source/css/vdonate.css- background: url(\"https://ooo.0o0.ooo/2017/03/09/58c158afac35c.png\")- background: url(\"https://ooo.0o0.ooo/2017/03/09/58c1584d5fd9d.png\")- background: url(\"https://ooo.0o0.ooo/2017/03/09/58c16b1f3eaa4.png\") no-repeat+++ b/source/css/bootstrap.css- src: url('../fonts/glyphicons-halflings-regular.eot');- src: url('../fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../fonts/glyphicons-halflings-regular.woff') format('woff'), url('../fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');+++ b/layout/_partial/head.ejs- &lt;link href=\"//fonts.googleapis.com/css?family=Source+Code+Pro\" rel=\"stylesheet\" type=\"text/css\"&gt;- &lt;link href=\"https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700\" rel=\"stylesheet\" type=\"text/css\"&gt;- &lt;link href=\"https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic\" rel=\"stylesheet\" type=\"text/css\"&gt; Mathjax CDN的修改 12345678--- a/layout/_partial/post/mathjax.ejs+++ b/layout/_partial/post/mathjax.ejs@@ -26,5 +26,5 @@ &#125;); &lt;/script&gt;-&lt;script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"&gt;+&lt;script type=\"text/javascript\" src=\"http://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"&gt; 修改一些字体大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061--- a/source/css/_partial/article.styl+++ b/source/css/_partial/article.styl@@ -180,7 +180,7 @@ blockquote padding: 0.1px 20px; font-family: font-normal- font-size: 1.1em+ font-size: 1.0em color: rgba(44, 44, 44, .6) margin: 1.4em 0px border-left: 2px solid color-theme @@ -137,7 +137,7 @@ .article-entry @extend $base-style clearfix()- color: color-default+ color: rgba(44, 44, 44, .6) margin: 0 article-padding @media mq-mobile margin: 0 @@ -271,7 +271,7 @@ .article-footer clearfix()- font-size: 0.85em+ font-size: 0.90em line-height: line-height border-top: 1px solid rgba(208,211,248,0.25) padding-top: line-heightdiff --git a/source/css/_partial/highlight.styl b/source/css/_partial/highlight.stylindex a198072..007eec0 100644--- a/source/css/_partial/highlight.styl+++ b/source/css/_partial/highlight.styl@@ -101,12 +101,12 @@ $code-block border-width: 1px 0 overflow: auto color: $highlight-foreground- font-size: 0.9em;- line-height: 1em+ font-size: 0.95em;+ line-height: 1.1em $line-numbers color: #666- font-size: 0.85em+ font-size: 0.95emgdiff --git a/source/css/_variables.styl b/source/css/_variables.stylindex 6b9d0a1..83908c8 100644--- a/source/css/_variables.styl+++ b/source/css/_variables.styl@@ -73,7 +73,7 @@ font-icon = FontAwesome font-icon-path = \"fonts/fontawesome-webfont\" font-icon-version = \"4.0.3\" font-size = 14px-line-height = 1.6em+line-height = 1.8em line-height-title = 1em ​ 新主题优化内容清单[2018-05] 增加新的边栏目录 更新mathjax CDN 增加七牛图床同步 增加图像地址转移 修复mathjax的bug 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 修改Hexo渲染源码 这个方法是我目前使用的，相对来说，通用性较高的一种方式。思路就是修改hexo的渲染源码: nodes_modules/lib/marked/lib/marked.js: 去掉\\的额外转义 将em标签对应的符号中，去掉_,因为markdown中有*可以表示斜体，—就去掉了。 具体思路参考了使Marked.js与MathJax共存, 打开nodes_modules/marked/lib/marked.js:第一步: 找到下面的代码: 1escape: /^\\\\([\\\\`*&#123;&#125;\\[\\]()# +\\-.!_&gt;])/, 改为: 1escape: /^\\\\([`*&#123;&#125;\\[\\]()# +\\-.!_&gt;])/, 这样就会去掉\\的转义了。第二步: 找到em的符号: 1em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 改为: 1em:/^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 去掉_的斜体含义,这样就解决了。 更多参考： http://visugar.com/2017/08/01/20170801HexoPlugins/ TO DO List[x] slides over reveal.js hexo-sliding-spoiler 折叠代码和显示内容 目录显示安装hexo插件 12\"hexo-renderer-markdown-it-plus\": \"^1.0.3\",\"markdown-it-toc-and-anchor\": \"^4.1.2\" 在hexo配置文件_config.yml中增加如下配置信息： 12345678910111213141516markdown_it_plus: highlight: true html: true xhtmlOut: true breaks: true langPrefix: linkify: true typographer: quotes: “”‘’ plugins: - plugin: name: markdown-it-toc-and-anchor enable: true options: tocClassName: 'toc js-fixedContent' anchorClassName: 'anchor' 多个模组的代码管理 添加新的submodule 12$ cd blog-hexo$ git submodule add https://github.com/ddebby/hexo-theme-tranquilpeak.git themes/tranq 增加远程更新源 12345git remote -v git remote add upstream git@github.com:xxx/xxx.gitgit fetch upstreamgit merge upstream/mastergit push 安装tips 中国环境安装node-sass有坑，主要是grunt-sass版本低，可以升级grunt-sass版本继续安装。"}],"posts":[{"title":"经典网络复现之SqueezeNet","slug":"经典网络复现之SqueezeNet","date":"2018-07-12T05:58:12.000Z","updated":"2018-07-20T15:52:08.839Z","comments":true,"path":"2018/07/12/经典网络复现之SqueezeNet/","link":"","permalink":"http://blog.a-stack.com/2018/07/12/经典网络复现之SqueezeNet/","excerpt":"摘要： 本文记录利用ImageNet数据集复现经典网络SqueezeNet的过程，并记录在大型数据集训练过程中需要考虑的问题。","text":"摘要： 本文记录利用ImageNet数据集复现经典网络SqueezeNet的过程，并记录在大型数据集训练过程中需要考虑的问题。 为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容： ImageNet数据集及数据准备 AlexNet网络 VGG网络 GoogLeNet网络 ResNet网络 SqueezeNet网络 概述SqueezeNet正如作者在论文题目中特别强调的那样，主要针对模型大小进行了深度优化，在保证模型精度没有太多损失的情况下，极大的缩小了所需模型的尺寸，为在嵌入式设备部署提供了强力支撑。SqueezeNet为我们裁剪模型和设计可在资源受限设备上运行的高性能模型提供了指导，特别是Fire Module的设计原理值得思考。 论文：SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size，2016. URL: https://arxiv.org/abs/1602.07360 代码链接：https://github.com/DeepScale/SqueezeNet 数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见ImageNet-DataSet） 计算框架： MxNet 算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour) 总计训练时间1天左右 每轮迭代时间：~1060秒 在ImageNet数据集上的效果： SqueezeNet microarchitecture and macro architecture 在设计深度网络架构的过程中，如果手动选择每一层的滤波器显得过于繁复。通常先构建由几个卷积层组成的小模块，再将模块堆叠形成完整的网络。定义这种模块的网络为CNN microarchitecture。 与模块相对应，定义完整的网络架构为CNN macroarchitecture。在完整的网络架构中，深度是一个重要的参数。 比如FPGA中只有10MB的片上内存空间，没有片下存储，对模型大小要求较高；ASICs也会有相同的需求； ​ 设计原则作者在论文中给出SqueezeNet的三条设计原则： 尽量用1x1卷积替代3x3卷积，目的自然是降低参数量； 降低输入到3x3卷积对象的channel数目，在SqueezeNet中是使用squeeze模块来实现的； 尽量在网络后面层次中进行降采样，这样可以是卷积层获得更大的激活地图，从而获得更高的精度； Fire Module Fire Module是本文的核心构件，思想非常简单，就是将原来简单的一层conv层变成两层：squeeze层+expand层，各自带上Relu激活层。Fire Moduel主要包括squeeze模块和expand模块，其中squeeze模块全部由1x1卷积组成，而expand模块由1x1卷积和3x3卷积组成： squeeze模块中卷积核个数一定要小于expand模块中卷积核的个数，只有这样才能达到降参和压缩的目的； squeeze中的1x1卷积起到了降维的效果； 为了实现expand模块中1x1卷积和3x3卷积之后的数据能够concatenate在一起，对于输入3x3的对象需要增加1的zero-padding操作； 在squeeze模块和expand模块之后通过ReLU进行激活； 模型架构及参数 如图所示， SqueezeNet中彻底放弃了全连接网络； 作者论文中初始学习率很大，为0.04，实际实验中，发现这么大的学习率震荡太厉害，降为0.01； 降维的操作尽量放在了模型后半段，为了满足第三条设计原则； 参数方面：使用了8个Fire Module，每个Fire Module中，squeeze的卷积核数目是expand的1/8； fire9之后采用了dropout，其中keep_prob=0.5; 分析结果 如图所示，SqueezeNet模型本身是AlexNet的1/50，却可以获得相近的准确率，通过利用Deep Compression技术可以进一步压缩模型，实现1/510模型大小情况下的相同性能（当然实际中，由于Deep Compression采取了编码策略，需要编码本，会带来一定的额外开销）。 [扩展阅读]Deep Compression Deep compression: Compressing DNNs with pruning, trained quantization and huffman coding， 2015 常见的模型压缩技术 奇异值分解(singular value decomposition (SVD))1 网络剪枝（Network Pruning）2：使用网络剪枝和稀疏矩阵 深度压缩（Deep compression）3：使用网络剪枝，数字化和huffman编码 硬件加速器（hardware accelerator）4 重头训练一个SqueezeNet 使用MxNet构建AlexNet代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# import the necessary packagesimport mxnet as mxclass MxSqueezeNet: @staticmethod def squeeze(input, numFilter): # the first part of a FIRE module consists of a number of 1x1 # filter squeezes on the input data followed by an activation squeeze_1x1 = mx.sym.Convolution(data=input, kernel=(1, 1), stride=(1, 1), num_filter=numFilter) act_1x1 = mx.sym.LeakyReLU(data=squeeze_1x1, act_type=\"elu\") # return the activation for the squeeze return act_1x1 @staticmethod def fire(input, numSqueezeFilter, numExpandFilter): # construct the 1x1 squeeze followed by the 1x1 expand squeeze_1x1 = MxSqueezeNet.squeeze(input, numSqueezeFilter) expand_1x1 = mx.sym.Convolution(data=squeeze_1x1, kernel=(1, 1), stride=(1, 1), num_filter=numExpandFilter) relu_expand_1x1 = mx.sym.LeakyReLU(data=expand_1x1, act_type=\"elu\") # construct the 3x3 expand expand_3x3 = mx.sym.Convolution(data=squeeze_1x1, pad=(1, 1), kernel=(3, 3), stride=(1, 1), num_filter=numExpandFilter) relu_expand_3x3 = mx.sym.LeakyReLU(data=expand_3x3, act_type=\"elu\") # the output of the FIRE module is the concatenation of the # activation for the 1x1 and 3x3 expands along the channel # dimension output = mx.sym.Concat(relu_expand_1x1, relu_expand_3x3, dim=1) # return the output of the FIRE module return output @staticmethod def build(classes): # data input data = mx.sym.Variable(\"data\") # Block #1: CONV =&gt; RELU =&gt; POOL conv_1 = mx.sym.Convolution(data=data, kernel=(7, 7), stride=(2, 2), num_filter=96) relu_1 = mx.sym.LeakyReLU(data=conv_1, act_type=\"elu\") pool_1 = mx.sym.Pooling(data=relu_1, kernel=(3, 3), stride=(2, 2), pool_type=\"max\") # Block #2-4: (FIRE * 3) =&gt; POOL fire_2 = MxSqueezeNet.fire(pool_1, numSqueezeFilter=16, numExpandFilter=64) fire_3 = MxSqueezeNet.fire(fire_2, numSqueezeFilter=16, numExpandFilter=64) fire_4 = MxSqueezeNet.fire(fire_3, numSqueezeFilter=32, numExpandFilter=128) pool_4 = mx.sym.Pooling(data=fire_4, kernel=(3, 3), stride=(2, 2), pool_type=\"max\") # Block #5-8: (FIRE * 4) =&gt; POOL fire_5 = MxSqueezeNet.fire(pool_4, numSqueezeFilter=32, numExpandFilter=128) fire_6 = MxSqueezeNet.fire(fire_5, numSqueezeFilter=48, numExpandFilter=192) fire_7 = MxSqueezeNet.fire(fire_6, numSqueezeFilter=48, numExpandFilter=192) fire_8 = MxSqueezeNet.fire(fire_7, numSqueezeFilter=64, numExpandFilter=256) pool_8 = mx.sym.Pooling(data=fire_8, kernel=(3, 3), stride=(2, 2), pool_type=\"max\") # Block #9-10: FIRE =&gt; DROPOUT =&gt; CONV =&gt; RELU =&gt; POOL fire_9 = MxSqueezeNet.fire(pool_8, numSqueezeFilter=64, numExpandFilter=256) do_9 = mx.sym.Dropout(data=fire_9, p=0.5) conv_10 = mx.sym.Convolution(data=do_9, num_filter=classes, kernel=(1, 1), stride=(1, 1)) relu_10 = mx.sym.LeakyReLU(data=conv_10, act_type=\"elu\") pool_10 = mx.sym.Pooling(data=relu_10, kernel=(13, 13), pool_type=\"avg\") # softmax classifier flatten = mx.sym.Flatten(data=pool_10) model = mx.sym.SoftmaxOutput(data=flatten, name=\"softmax\") # return the network architecture return model 几点注意点： 使用ELU替代了ReLU激活函数；在ImageNet数据集中，ELU表现效果由于ReLU； 3x3 expand中有个pad=（1，1） 最有使用的全局平均池化层，kernel=（13，13） 全局池化输出被flatten之后直接丢给softmax SqueezeNet的TensorFLow实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class SqueezeNet(object): def __init__(self, inputs, nb_classes=1000, is_training=True): # conv1 net = tf.layers.conv2d(inputs, 96, [7, 7], strides=[2, 2], padding=\"SAME\", activation=tf.nn.relu, name=\"conv1\") # maxpool1 net = tf.layers.max_pooling2d(net, [3, 3], strides=[2, 2], name=\"maxpool1\") # fire2 net = self._fire(net, 16, 64, \"fire2\") # fire3 net = self._fire(net, 16, 64, \"fire3\") # fire4 net = self._fire(net, 32, 128, \"fire4\") # maxpool4 net = tf.layers.max_pooling2d(net, [3, 3], strides=[2, 2], name=\"maxpool4\") # fire5 net = self._fire(net, 32, 128, \"fire5\") # fire6 net = self._fire(net, 48, 192, \"fire6\") # fire7 net = self._fire(net, 48, 192, \"fire7\") # fire8 net = self._fire(net, 64, 256, \"fire8\") # maxpool8 net = tf.layers.max_pooling2d(net, [3, 3], strides=[2, 2], name=\"maxpool8\") # fire9 net = self._fire(net, 64, 256, \"fire9\") # dropout net = tf.layers.dropout(net, 0.5, training=is_training) # conv10 net = tf.layers.conv2d(net, 1000, [1, 1], strides=[1, 1], padding=\"SAME\", activation=tf.nn.relu, name=\"conv10\") # avgpool10 net = tf.layers.average_pooling2d(net, [13, 13], strides=[1, 1], name=\"avgpool10\") # squeeze the axis net = tf.squeeze(net, axis=[1, 2]) self.logits = net self.prediction = tf.nn.softmax(net) def _fire(self, inputs, squeeze_depth, expand_depth, scope): with tf.variable_scope(scope): squeeze = tf.layers.conv2d(inputs, squeeze_depth, [1, 1], strides=[1, 1], padding=\"SAME\", activation=tf.nn.relu, name=\"squeeze\") # squeeze expand_1x1 = tf.layers.conv2d(squeeze, expand_depth, [1, 1], strides=[1, 1], padding=\"SAME\", activation=tf.nn.relu, name=\"expand_1x1\") expand_3x3 = tf.layers.conv2d(squeeze, expand_depth, [3, 3], strides=[1, 1], padding=\"SAME\", activation=tf.nn.relu, name=\"expand_3x3\") return tf.concat([expand_1x1, expand_3x3], axis=3) 训练SqueezeNet训练用的脚本和前面几个网络一致。 几个注意点： batchSize = config.BATCH_SIZE * config.NUM_DEVICES; 使用k80 12GB先存，选择batchSzie=128； 优化算法使用SGD，初始学习率为1e-2，动量0.9，L2权重正则化参数0.0002；rescale参数尤为关键，根据批的大小放大梯度：rescale_grad=1.0 / batchSize； model中的ctx参数用于指定用于训练的GPU； 使用了Xavier进行参数初始化，与原模型略有不同，Xavier是目前CNN网络常采用的参数初始化方式；initializer=mx.initializer.Xavier()； In Keras, the ELU activation uses a default α value of 1.0. • But in mxnet, the ELU α value defaults to 0.25. 学习率的控制 Epoch 学习率 1-64 1e-2 65-80 1e-3 81-89 1e-4 90-100 1e-5 控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率； 调整学习率 python train_alexnet.py --checkpoints checkpoints --prefix alexnet \\ --start-epoch 50 在8个GPU上，每轮用时1000多秒； 第一遍训练采用1e-2的学习率训练75轮，发现65轮以后，验证集准确率已经不再增加；为此在65轮之后调整学习率为1e-3； 将学习率调整到1e-3之后，验证集准确率有大幅提升，代表调整有效，80轮之后再度饱和，降低学习率到1e-4;到89轮验证集结果如下： ​ 进一步降低学习率到1e-5，发现整个网络没有性能提升，结束该批次参数训练过程。 实验 初始学习率的选择 BN的效果 在SqueezeNet中没有效果 ReLU vs ELU ？ 用ELU替代ReLU，提升1-2%的性能； 选取90轮的训练结果，在测试集数据上进行验证，结果如下： 12[INFO] rank-1: 55.44%[INFO] rank-5: 78.10% 结论今天终于完成了所有预定神经网络的训练过程，虽然有些网络由于本身对网络结构的认知或者是实际训练过程中没有很好的把握节奏，导致训练效果不是十分理想，但从这次的训练过程中还是学到了很多东西。整个过程大概持续了半个月左右的时间，得益于AWS上8 GPU的资源，使得很多网络的训练比原想进度要快很多，但费用也是惊人，看了下账单，发生在虚拟机上的费用大概有1万3千多，主要由于8 GPU服务器要85元人民币每个小时的费用不是所有人都可以承受的起的。所以我一直认为短期内，深度学习还是很难真正产业化的一个泡沫，很难有应用业务可以抵消这个昂贵的训练成本。我这只是复现网络，真实训练一个产品级的网络、所需要测试的参数十倍百倍于此。 付了昂贵的学费，学习的动力也更加充足，将几种网络的训练过程进行一下简单的横向比较，同时总结一下大型网络训练过程中注意的主要内容。 几种网络在训练过程的简单比较 网络名称 模型大小 每轮训练时间（8GPU Tesla K80） 总迭代数+训练时间 初始化参数选择 初始学习率 激活函数 最终测试集准确率（Top-1/Top-5） AlexNet 239MB ~670s ~1.5天 Xavier 1e-2,动量0.9，L2 0.0005 ELU 60.20%/81.99% VGG-16 529MB ~ ~10天 MSRA 1e-2,动量0.9，L2 0.0005 PReLU GoogLeNet 28MB ~1200s ~2天 Xavier 1e-3，Adam，L2 0.0002 ReLU 69.58%/89.01% ResNet 98MB ~3500s ~3天 MSRA 1e-1,动量0.9，L2 0.0001 ReLU 71.49%/89.96% SqueezeNet 5MB ~1060s ~1.5天 Xavier 1e-2，动量0.9，L2 0.0002 ELU 55.44%/78.10% 训练心得 训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源； 原则：训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型； 网络参数如何入手？ 逆机器学习流程的思考：在机器学习算法的应用逻辑里，我们总是强调要快速的开始搭建第一个模型，设定测试的基准，然后再迭代更新模型。而训练大型神经网络，由于训练过程对时间和资源消耗十分巨大，为充分利用资源，切不可盲目起步； 由于可调参数终端，所以必须要想办法缩小可调参数范围，优先选择重要参数进行调试，同时通过查阅相似数据集文献中使用的参数来进一步缩小自己调试参数的范围； 初始学习率选择： 学习率参数是整个参数空间中最重要的一个，如果你只想调试一个参数，那么请选择它； 初始学习率需要根据网络模型的特点来决定，一般而言1e-2可能是一个合适的学习率，但不是对所有的网络都是最佳的学习率，比如ResNet支持更大的学习率，比如1e-1,GoogLeNet而言，1e-2有点太大，验证集准确率会发生比较大的震荡； 学习率控制： 初始学习率选择好以后，需要在第一次迭代中让网络多运行一段时间，观察验证集上的表现决定是否变更学习率，一般当验证集误差出现阻塞或者出现明显过拟合现象时，需要停止训练，调整学习率，从打断位置或前面位置继续训练； 学习率手段衰减一般采取对数衰减的策略，比如每次衰减十倍； 优化算法选择： 关于优化算法的选择，一般先从尝试经典的SGD算法、配合动量设置开始训练看网络是否能够有效学习参数空间信息； 进一步的可以调整为Adam来加快梯度的更新； 在大型网络训练中还经常使用梯度的rescale操作，根据batchsize大小，放大梯度：rescale_grad=1.0 / batchSize； 激活函数选择：先使用ReLU作为激活函数获得baseline，再选择提花为ELU来获得提升； 初始化参数选择：训练深层网络时，考虑使用Xavier,MSRA/HE的初始化参数，并配合PReLU一起使用，效果更好。 训练过程的控制： 训练过程要利用callback机制，随时记录训练参数、checkpoints； 通过Tensorboard等工具观察训练过程的精度、误差、损失函数变化情况； 有些网络需要预训练过程，通过预训练可以加速网络学习； 参数调优的其它技巧： BN DropOut Data Augmentation 计算框架：TensorFlow or MxNet 论文阅读与讨论 ​… 参考 ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"网络复现","slug":"网络复现","permalink":"http://blog.a-stack.com/tags/网络复现/"}]},{"title":"经典网络复现之VGGNet","slug":"2018-07-9-经典网络复现之VGG","date":"2018-07-12T05:58:12.000Z","updated":"2018-07-20T14:53:24.141Z","comments":true,"path":"2018/07/12/2018-07-9-经典网络复现之VGG/","link":"","permalink":"http://blog.a-stack.com/2018/07/12/2018-07-9-经典网络复现之VGG/","excerpt":"摘要： 本文记录利用ImageNet数据集复现经典网络VGGNet的过程，并记录在大型数据集训练过程中需要考虑的问题。","text":"摘要： 本文记录利用ImageNet数据集复现经典网络VGGNet的过程，并记录在大型数据集训练过程中需要考虑的问题。 为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容： ImageNet数据集及数据准备 AlexNet网络 VGG网络 GoogLeNet网络 ResNet网络 SqueezeNet网络 概述VGGNet是英国牛津大学2014年ImageNet ILSVRC第二名的网络，由于其良好的泛化性能，在被迁移到其它深度学习任务中体现出来良好的效果，是目前迁移学习中用的最多的网络。但由于网络规模和参数量实在庞大，从头训练一个网络充满难度。 论文： Very Deep Convolutional Networks for Large-Scale Image Recognition 数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见ImageNet-DataSet） 计算框架： MxNet 算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour) 总计训练时间&gt;10天左右 重头训练一个VGGNet 由于网络深度太深、参数太多，所以从头按部就班的训练VGGNet是很漫长的过程，为此VGG的作者使用了一种pre-training的方式。通过不断训练小一号的网络架构，将训练后的参数作为后续较大网络的初始参数来逐步逼近完整的网络。比如为了训练VGG16，作者先训练了A网络VGG11，然后利用A网络的参数逐步训练B VGG13，最后才是D网络 VGG16。这种方式使用了warmed pre-trained up层参数来推进后续的网络训练。 虽然上述方法是个很好的技巧，但训练N个网络才能达到目的实在是一个痛苦的过程，随着深度学习技术的发展，尤其是参数初始化技术的发展，为训练VGG提供了更加高效的策略——使用高级的初始化策略，如Xavier或MSRA，同时配合使用参数化ReLU可以抛弃1中的预训练方式。 训练VGGNet训练用的脚本和前面几个网络一致。 几个注意点： batchSize = config.BATCH_SIZE * config.NUM_DEVICES; 使用k80 12GB先存，选择batchSzie=32； 初始学习率为1e-2，动量0.9，L2权重正则化参数0.0005；rescale参数尤为关键，根据批的大小放大梯度：rescale_grad=1.0 / batchSize； 使用了MSRA进行参数初始化，initializer=mx.initializer.MSRAPrelu()； 同时使用参数PReLU代替ReLU作为激活函数； 学习率的控制 Epoch 学习率 1-50 1e-2 51-70 1e-3 71-80 1e-4 控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率； 调整学习率 python train_alexnet.py --checkpoints checkpoints --prefix alexnet \\ --start-epoch 50 实验1.选取40轮的训练结果，在测试集数据上进行验证，结果如下： 12[INFO] rank-1: 71.42% [INFO] rank-5: 90.03% 结论 为了训练VGGNet，一定要使用高级初始化参数方法，并配合参数化ReLU使用； 训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源； 训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型； 先使用ReLU作为激活函数获得baseline，再选择提花为ELU来获得提升； 训练深层网络时，考虑使用MSRA/HE的初始化参数，并配合PReLU一起使用，效果更好；","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"网络复现","slug":"网络复现","permalink":"http://blog.a-stack.com/tags/网络复现/"}]},{"title":"经典网络复现之GoogleNet","slug":"经典网络复现之GoogleNet","date":"2018-07-12T05:58:12.000Z","updated":"2018-07-20T15:52:31.842Z","comments":true,"path":"2018/07/12/经典网络复现之GoogleNet/","link":"","permalink":"http://blog.a-stack.com/2018/07/12/经典网络复现之GoogleNet/","excerpt":"摘要： 本文记录利用ImageNet数据集复现经典网络GoogLeNet的过程，并记录在大型数据集训练过程中需要考虑的问题。","text":"摘要： 本文记录利用ImageNet数据集复现经典网络GoogLeNet的过程，并记录在大型数据集训练过程中需要考虑的问题。 为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容： ImageNet数据集及数据准备 AlexNet网络 VGG网络 GoogLeNet网络 ResNet网络 SqueezeNet网络 概述GoogleNet作为2012年ILSVRC的冠军，最近几年在原先Inception v1版本上也在不断的演化发展，目前已经到了v4版本。GoogleNet模型设计在保证准确率的同时大大降低了模型的大小，与VGG接近500MB相比，只有28MB左右的模型大小。 论文： Christian Szegedy et al. “Going Deeper with Convolutions”. In: Computer Vision and Pattern Recognition (CVPR). 2015. URL: http://arxiv.org/abs/1409.4842 数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见ImageNet-DataSet） 计算框架： MxNet 算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour) 总计训练时间2天左右 每轮迭代时间：~1200秒(~20分钟) GoogleNetGoogleNet的主要创新点体现在： 一、打破了常规的卷积层串联的模式，提出了将1x1，3x3，5x5的卷积层和3x3的pooling池化层并联组合后concatenate组装在一起的设计思路。 二、InceptionV1降低了参数量，目的有两个：1 参数越多模型越大，需要提供模型学习的数据量就越大，数据量不大的情况下容易过拟合；2 参数越多，耗费的计算资源也会更大。 三、去除了最后的全连接层，用全局平均池化层来取代它。全连接层几乎占据了AlexNet或VGGNet中90%的参数量，而且会引起过拟合，去除全连接层后模型训练更快并且减轻了过拟合。用全局平均池化层取代全连接层的做法借鉴了NetworkIn Network（以下简称NIN）论文。 四、InceptionV1中精心设计的InceptionModule提高了参数的利用效率。这一部分也借鉴了NIN的思想，形象的解释就是InceptionModule本身如同大网络中的一个小网络，其结构可以反复堆叠在一起形成大网络。InceptionV1比NIN更进一步的是增加了分支网络，NIN则主要是级联的卷积层和MLPConv层。一般来说卷积层要提升表达能力，主要依靠增加输出通道数，但副作用是计算量增大和过拟合。每一个输出通道对应一个滤波器，同一个滤波器共享参数，只能提取一类特征，因此一个输出通道只能做一种特征处理。而NIN中的MLPConv则拥有更强大的能力，允许在输出通道之间组合信息，因此效果明显。可以说，MLPConv基本等效于普通卷积层后再连接1*1的卷积和ReLU激活函数。 重头训练一个GoogLeNet 使用MxNet构建GoogleNet代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import mxnet as mxclass MxGoogLeNet: @staticmethod def conv_module(data, K, kX, kY, pad=(0, 0), stride=(1, 1)): # define the CONV =&gt; BN =&gt; RELU pattern conv = mx.sym.Convolution(data=data, kernel=(kX, kY), num_filter=K, pad=pad, stride=stride) #bn = mx.sym.BatchNorm(data=conv) act = mx.sym.Activation(data=conv, act_type=\"relu\") bn = mx.sym.BatchNorm(data=act) # return the block return bn @staticmethod def inception_module(data, num1x1, num3x3Reduce, num3x3, num5x5Reduce, num5x5, num1x1Proj): # the first branch of the Inception module consists of 1x1 # convolutions conv_1x1 = MxGoogLeNet.conv_module(data, num1x1, 1, 1) # the second branch of the Inception module is a set of 1x1 # convolutions followed by 3x3 convolutions conv_r3x3 = MxGoogLeNet.conv_module(data, num3x3Reduce, 1, 1) conv_3x3 = MxGoogLeNet.conv_module(conv_r3x3, num3x3, 3, 3, pad=(1, 1)) # the third branch of the Inception module is a set of 1x1 # convolutions followed by 5x5 convolutions conv_r5x5 = MxGoogLeNet.conv_module(data, num5x5Reduce, 1, 1) conv_5x5 = MxGoogLeNet.conv_module(conv_r5x5, num5x5, 5, 5, pad=(2, 2)) # the final branch of the Inception module is the POOL + # projection layer set pool = mx.sym.Pooling(data=data, pool_type=\"max\", pad=(1, 1), kernel=(3, 3), stride=(1, 1)) conv_proj = MxGoogLeNet.conv_module(pool, num1x1Proj, 1, 1) # concatenate the filters across the channel dimension concat = mx.sym.Concat(*[conv_1x1, conv_3x3, conv_5x5, conv_proj]) # return the block return concat @staticmethod def build(classes): # data input data = mx.sym.Variable(\"data\") # Block #1: CONV =&gt; POOL =&gt; CONV =&gt; CONV =&gt; POOL conv1_1 = MxGoogLeNet.conv_module(data, 64, 7, 7, pad=(3, 3), stride=(2, 2)) pool1 = mx.sym.Pooling(data=conv1_1, pool_type=\"max\", pad=(1, 1), kernel=(3, 3), stride=(2, 2)) conv1_2 = MxGoogLeNet.conv_module(pool1, 64, 1, 1) conv1_3 = MxGoogLeNet.conv_module(conv1_2, 192, 3, 3, pad=(1, 1)) pool2 = mx.sym.Pooling(data=conv1_3, pool_type=\"max\", pad=(1, 1), kernel=(3, 3), stride=(2, 2)) # Block #3: (INCEP * 2) =&gt; POOL in3a = MxGoogLeNet.inception_module(pool2, 64, 96, 128, 16, 32, 32) in3b = MxGoogLeNet.inception_module(in3a, 128, 128, 192, 32, 96, 64) pool3 = mx.sym.Pooling(data=in3b, pool_type=\"max\", pad=(1, 1), kernel=(3, 3), stride=(2, 2)) # Block #4: (INCEP * 5) =&gt; POOL in4a = MxGoogLeNet.inception_module(pool3, 192, 96, 208, 16, 48, 64) in4b = MxGoogLeNet.inception_module(in4a, 160, 112, 224, 24, 64, 64) in4c = MxGoogLeNet.inception_module(in4b, 128, 128, 256, 24, 64, 64) in4d = MxGoogLeNet.inception_module(in4c, 112, 144, 288, 32, 64, 64) in4e = MxGoogLeNet.inception_module(in4d, 256, 160, 320, 32, 128, 128,) pool4 = mx.sym.Pooling(data=in4e, pool_type=\"max\", pad=(1, 1), kernel=(3, 3), stride=(2, 2)) # Block #5: (INCEP * 2) =&gt; POOL =&gt; DROPOUT in5a = MxGoogLeNet.inception_module(pool4, 256, 160, 320, 32, 128, 128) in5b = MxGoogLeNet.inception_module(in5a, 384, 192, 384, 48, 128, 128) pool5 = mx.sym.Pooling(data=in5b, pool_type=\"avg\", kernel=(7, 7), stride=(1, 1)) do = mx.sym.Dropout(data=pool5, p=0.4) # softmax classifier flatten = mx.sym.Flatten(data=do) fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=classes) model = mx.sym.SoftmaxOutput(data=fc1, name=\"softmax\") # return the network architecture return model 几点注意点： 与原网络不同，我们将BN放在了activation之后，提升了模型效果； Dropout原网络使用40%，当然现在更常见的是使用50%； ​Inception Module中几个分支网络通过concat连接在一起，这点与ResNet的本质区别； 训练GoogLeNet训练用的脚本和前面几个网络一致。 几个注意点： batchSize = config.BATCH_SIZE * config.NUM_DEVICES; 使用k80 12GB先存，选择batchSzie=128； 优化算法使用Adam，初始学习率为1e-3，L2权重正则化参数0.0002；rescale参数尤为关键，根据批的大小放大梯度：rescale_grad=1.0 / batchSize；（使用SGD训练效果一般） 使用了Xavier进行参数初始化，initializer=mx.initializer.Xavier(),； 学习率的控制 训练ResNet由于初始学习率高，所以最终完成训练所需的迭代数目要小很多； Epoch 学习率 1-31 1e-3 32-40 1e-4 41-47 1e-5 控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率； 调整学习率 python train_alexnet.py --checkpoints checkpoints --prefix alexnet \\ --start-epoch 50 在8个GPU上，每轮用时1200多秒； 第一遍训练采用1e-3的学习率配合Adam算法训练31轮，发现饱和现象，降低学习率到1e-4，继续训练到42轮； ​ 发现40轮之后就已经出现了验证准确率的饱和现象，在40轮之后调整学习率为1e-5继续训练几轮完成： 实验 Adam or SGD 如果使用SGD + 1e-2的经典开始配合，在GoogleNet上效果并不好，抖动剧烈，虽然这是作者推荐的参数，如下： 首先修改初始学习率为1e-3，尽管抖动降低了，但学习太慢，效果也不佳，为此进一步的调整了优化算法为Adam算法。 使用BN在激活函数之后 取得了1%以上的性能提升 选取40轮的训练结果，在测试集数据上进行验证，结果如下： 12[INFO] rank-1: 69.58%[INFO] rank-5: 89.01% 结论 GoogleNet很难复现作者的效果，实际上还没有VGG的效果好，可能是作者论文中某些参数没有有效标注出来； 训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源； 训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型； 先使用ReLU作为激活函数获得baseline，再选择提花为ELU来获得提升； 训练深层网络时，考虑使用MSRA/HE的初始化参数，并配合PReLU一起使用，效果更好； 参考 ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"网络复现","slug":"网络复现","permalink":"http://blog.a-stack.com/tags/网络复现/"}]},{"title":"经典网络复现之ResNet","slug":"经典网络复现之ResNet","date":"2018-07-12T05:58:12.000Z","updated":"2018-07-20T15:52:21.624Z","comments":true,"path":"2018/07/12/经典网络复现之ResNet/","link":"","permalink":"http://blog.a-stack.com/2018/07/12/经典网络复现之ResNet/","excerpt":"摘要： 本文记录利用ImageNet数据集复现经典网络ResNet的过程，并记录在大型数据集训练过程中需要考虑的问题。","text":"摘要： 本文记录利用ImageNet数据集复现经典网络ResNet的过程，并记录在大型数据集训练过程中需要考虑的问题。 为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容： ImageNet数据集及数据准备 AlexNet网络 VGG网络 GoogLeNet网络 ResNet网络 SqueezeNet网络 概述ResNet在整个深度神经网络发展过程中国起到了里程碑式的意义，残差模块的提出为训练数千层的神经网络提供了方法，后续许多网络也纷纷在其网络架构中增加残差模块，力图提升训练效率。 论文： Deep Residual Learning for Image Recognition Identity Mappings in Deep Residual Networks 数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见ImageNet-DataSet） 计算框架： MxNet 算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour) 总计训练时间3天左右 每轮迭代时间：~3500秒(~1小时) ResNet由于之前准备写过一篇博客来分析ResNet，再次不再展开说明了，详细内容参见：经典网络归纳： ResNet 重头训练一个ResNet我们训练的标的是ResNet50，即50层的残差网络，50代表了整个网络中所有含参数层的数目： 1 + （3x3） + (4x3) + (6x3) + (3x3) + 1 = 50 第一个卷积层使用7x7的卷积核，用于快速降低网络尺寸，配合紧接着的池化层，将输入224x224的尺寸降低到56x56。 使用MxNet构建ResNet代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import mxnet as mxclass MxResNet: # uses \"bottleneck\" module with pre-activation (He et al. 2016) @staticmethod def residual_module(data, K, stride, red=False, bnEps=2e-5, bnMom=0.9): # the shortcut branch of the ResNet module should be # initialized as the input (identity) data shortcut = data # the first block of the ResNet module are 1x1 CONVs bn1 = mx.sym.BatchNorm(data=data, fix_gamma=False, eps=bnEps, momentum=bnMom) act1 = mx.sym.Activation(data=bn1, act_type=\"relu\") conv1 = mx.sym.Convolution(data=act1, pad=(0, 0), kernel=(1, 1), stride=(1, 1), num_filter=int(K * 0.25), no_bias=True) # the second block of the ResNet module are 3x3 CONVs bn2 = mx.sym.BatchNorm(data=conv1, fix_gamma=False, eps=bnEps, momentum=bnMom) act2 = mx.sym.Activation(data=bn2, act_type=\"relu\") conv2 = mx.sym.Convolution(data=act2, pad=(1, 1), kernel=(3, 3), stride=stride, num_filter=int(K * 0.25), no_bias=True) # the third block of the ResNet module is another set of 1x1 # CONVs bn3 = mx.sym.BatchNorm(data=conv2, fix_gamma=False, eps=bnEps, momentum=bnMom) act3 = mx.sym.Activation(data=bn3, act_type=\"relu\") conv3 = mx.sym.Convolution(data=act3, pad=(0, 0), kernel=(1, 1), stride=(1, 1), num_filter=K, no_bias=True) # if we are to reduce the spatial size, apply a CONV layer # to the shortcut if red: shortcut = mx.sym.Convolution(data=act1, pad=(0, 0), kernel=(1, 1), stride=stride, num_filter=K, no_bias=True) # add together the shortcut and the final CONV add = conv3 + shortcut # return the addition as the output of the ResNet module return add @staticmethod def build(classes, stages, filters, bnEps=2e-5, bnMom=0.9): # data input data = mx.sym.Variable(\"data\") # Block #1: BN =&gt; CONV =&gt; ACT =&gt; POOL, then initialize the # \"body\" of the network bn1_1 = mx.sym.BatchNorm(data=data, fix_gamma=True, eps=bnEps, momentum=bnMom) conv1_1 = mx.sym.Convolution(data=bn1_1, pad=(3, 3), kernel=(7, 7), stride=(2, 2), num_filter=filters[0], no_bias=True) bn1_2 = mx.sym.BatchNorm(data=conv1_1, fix_gamma=False, eps=bnEps, momentum=bnMom) act1_2 = mx.sym.Activation(data=bn1_2, act_type=\"relu\") pool1 = mx.sym.Pooling(data=act1_2, pool_type=\"max\", pad=(1, 1), kernel=(3, 3), stride=(2, 2)) body = pool1 # loop over the number of stages for i in range(0, len(stages)): # initialize the stride, then apply a residual module # used to reduce the spatial size of the input volume stride = (1, 1) if i == 0 else (2, 2) body = MxResNet.residual_module(body, filters[i + 1], stride, red=True, bnEps=bnEps, bnMom=bnMom) # loop over the number of layers in the stage for j in range(0, stages[i] - 1): # apply a ResNet module body = MxResNet.residual_module(body, filters[i + 1], (1, 1), bnEps=bnEps, bnMom=bnMom) # apply BN =&gt; ACT =&gt; POOL bn2_1 = mx.sym.BatchNorm(data=body, fix_gamma=False, eps=bnEps, momentum=bnMom) act2_1 = mx.sym.Activation(data=bn2_1, act_type=\"relu\") pool2 = mx.sym.Pooling(data=act2_1, pool_type=\"avg\", global_pool=True, kernel=(7, 7)) # softmax classifier flatten = mx.sym.Flatten(data=pool2) fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=classes) model = mx.sym.SoftmaxOutput(data=fc1, name=\"softmax\") # return the network architecture return model 几点注意点： 在bottleneck版本的残差模块中，前两个卷积核的数目是第三个的1/4； 本次试验中使用了预激活版本的残差块； 整个网络中没有使用dropout层； 注意，网络结构一开始，先使用一个BN应用到输入数据，起到正则化的作用； 全局池化输出被flatten之后接一个FC层丢给softmax 训练ResNet训练用的脚本和前面几个网络一致。 几个注意点： batchSize = config.BATCH_SIZE * config.NUM_DEVICES; 使用k80 12GB先存，选择batchSzie=32； 优化算法使用SGD，初始学习率为1e-1，动量0.9，L2权重正则化参数0.0001；rescale参数尤为关键，根据批的大小放大梯度：rescale_grad=1.0 / batchSize；（He论文的建议参数） 使用了MSRA进行参数初始化，initializer=mx.initializer.MSRAPrelu()； 学习率的控制 训练ResNet由于初始学习率高，所以最终完成训练所需的迭代数目要小很多； Epoch 学习率 1-64 1e-2 65-80 1e-3 81-89 1e-4 90-100 1e-5 控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率； 调整学习率 python train_alexnet.py --checkpoints checkpoints --prefix alexnet \\ --start-epoch 50 在8个GPU上，每轮用时1000多秒； 第一遍训练采用1e-1的学习率训练16轮，发现饱和现象，而且训练误差和测试误差约拉越大； 在第10个迭代调整学习率到1e-2，验证集准确率有大幅提升，代表调整有效，之后在16轮迭代之后再次降低学习率到1e-3,25轮调整学习率到1e-4，最终top-5的准确率为0.884073，top-1准确率为0.68338： 实验 初始学习率的选择 初始学习率如果选择1e-2，与1e-1比，很早模型就出现过拟合现象，无法到达更高的训练精度； 比如使用1e-1，在第10轮调整学习率到1e-2之后，网络准确率会出现一个10%以上的跳变； 如果不采用预激活 3-5%点的性能下降，原因见作者原文分析； 选取30轮的训练结果，在测试集数据上进行验证，结果如下： 12[INFO] rank-1: 71.49%[INFO] rank-5: 89.96% 结论 效果不是十分理想，主要原因应该是第一阶段1e-1学习率的训练过程过早的认为过拟合打断了训练过程，应该让这个过程再持续一段时间观察效果，无奈训练一次ResNet实在太久，放弃继续尝试了； 训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源； 训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型； 先使用ReLU作为激活函数获得baseline，再选择提花为ELU来获得提升； 训练深层网络时，考虑使用MSRA/HE的初始化参数，并配合PReLU一起使用，效果更好； 对于ResNet要想再进一步提升性能，需要考虑加强正则化、更多的数据放大、合理的应用dropout等方式。 参考 ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"网络复现","slug":"网络复现","permalink":"http://blog.a-stack.com/tags/网络复现/"}]},{"title":"经典网络复现之AlexNet","slug":"经典网络复现之AlexNet","date":"2018-07-08T05:58:12.000Z","updated":"2018-07-20T15:52:42.648Z","comments":true,"path":"2018/07/08/经典网络复现之AlexNet/","link":"","permalink":"http://blog.a-stack.com/2018/07/08/经典网络复现之AlexNet/","excerpt":"摘要： 本文记录利用ImageNet数据集复现经典网络AlexNet的过程，并记录在大型数据集训练过程中需要考虑的问题。","text":"摘要： 本文记录利用ImageNet数据集复现经典网络AlexNet的过程，并记录在大型数据集训练过程中需要考虑的问题。 为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容： ImageNet数据集及数据准备 AlexNet网络 VGG网络 GoogLeNet网络 ResNet网络 SqueezeNet网络 概述2012年的AlexNet是通用CNN的开山之作（早在1986年，Yann LeCun就利用卷积神经网络构建了LeNet并第一次将其运用到了生产环节——邮政编码识别，详见http://yann.lecun.com/exdb/lenet/a35.html ，但相关网络结构未能得到充分复制和发展），从此以后，CNN被广泛应用于各种机器视觉比赛、应用之中，得到了空前的发展。本文，主要根据AlexNet论文回顾其网络结构、设计技巧，并利用ImageNet的图像分类数据集复现在2012年ImageNet挑战赛中的结果。 论文： Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classification with Deep Convolutional Neural Networks”. In: Advances in Neural Information Processing Systems 25. Edited by F. Pereira et al. Curran Associates, Inc., 2012, pages 1097–1105. URL: http://papers.nips.cc/paper/4824-imagenet-classification-with-deepconvolutional-neural-networks.pdf 数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见ImageNet-DataSet） 计算框架： MxNet 算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour) AlexNetAlexNet在2012年ImageNet ILSVRC比赛中top-5的错误率未15.3%，领先第二名10.9%个百分点，整个网络架构总共有8层参数网络层，其中5层卷积网络+3层全连接网络。本节将按照AlexNet论文描述的内容，整理AlexNet的特点，和极具借鉴性的创新成果。 网络架构 AlexNet的网络架构如图所示，输入图像尺寸为（224，224，3），经过5个CNN层，3个FC层输出1000个分类结果，为了便于使网络训练采用GPU，所以整个网络分为上下两部分，分别在两个GPU上进行训练，其中第一层CNN和3个FC层在两个GPU上进行了参数共享。 AlexNet网络中拥有600万个参数，650 000个神经元。 GPU的第一次使用AlexNet是第一个使用GPU进行训练的深度神经网络，受当时硬件条件限制，使用了2块3GB内存的GTX 580 GPU，同时由于全部的网络参数超过了3GB的显存空间，所以将网络分成了两部分。训练时间上实际比单块GPU没有提升太多，但将错误率降低了1.2个百分点。 激活函数的选择——从此ReLU函数得到重用AlexNet中经过实践测试发现，采用ReLU作为激活函数比tanh要快很多（在CIFAR-10上测试前者速度是后者的6倍多），这也奠定了ReLU称为后来CNN网络的首选。 解决过拟合整个AlexNet都是围绕着计算效率提升和解决过拟合进行优化和尝试。AlexNet中的600万参数增强网络学习能力的同时也带来了潜在的过拟合问题，其中对解决过拟合的诸多尝试在未来的网络发展中被反复验证和使用。 1. 数据增强AlexNet中数据增强主要采用了两种方式，一种是随机扣取和水平翻转，另一种是白化。 随机扣取和水平翻转：AlexNet将原始图像处理成256x256大小的统一图像，在训练阶段，随机从原始图像中扣取227x227的像素，并通过水平翻转来提升训练集的差异性，增强网络泛化性能；在测试阶段，对于输入测试图像分别从四个顶角和中心位置获得5张图像，并通过水平翻转形成10张图像，通过评价10张图像的预测结果作为最终输出结果，将网络性能提升了将近1个百分点； 白化： 利用PCA进行主成分抽取 2. Dropout这是Dropout的第一次正式使用，Dropout本身起到了模型组合的作用，同时起到正则化效果，可以避免过拟合，提升效果。当然代价是网络收敛时间被延长。 在前两个FC层之后使用了Dropout 训练参数 优化算法选择SGD， 动量为0.9； batch_size = 128 权重衰减为0.0005，这个很小的权重衰减对于模型学习起到了不可忽视的作用； 权重参数初始化为零均值，0.01标准差的高斯分布； 为了使得初始参数落在ReLU的正值区域，将偏差参数初始化为1； 初始学习率为0.01，当验证集错误率饱和时以对数形式降低学习率，一共降低3次，即1e-3,1e-4,1e-5； 训练了90个epoch，总共用时5-6天 结果 使用5个CNN的组合，top-5错误率为16.4%​ 其它AlexNet还使用了局部响应归一化（Local Response Normalization）、重叠池化等优化策略，不过在未来的网络结构中没有得到良好的效果和应用普及。 重头训练一个AlexNet ImageNet数据准备参见前序博文 由于现在GPU硬件和深度学习计算框架的发展，我们不需要将网络拆成两个部分，单独训练了，所以网络架构进行了微调，详见下图； 使用MxNet训练网络比Keras + TensroFlow要快； 使用MxNet构建AlexNet代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import mxnet as mxclass MxAlexNet: @staticmethod def build(classes): # data input data = mx.sym.Variable(\"data\") # Block #1: first CONV =&gt; RELU =&gt; POOL layer set conv1_1 = mx.sym.Convolution(data=data, kernel=(11, 11), stride=(4, 4), num_filter=96) act1_1 = mx.sym.LeakyReLU(data=conv1_1, act_type=\"elu\") bn1_1 = mx.sym.BatchNorm(data=act1_1) pool1 = mx.sym.Pooling(data=bn1_1, pool_type=\"max\", kernel=(3, 3), stride=(2, 2)) do1 = mx.sym.Dropout(data=pool1, p=0.25) # Block #2: second CONV =&gt; RELU =&gt; POOL layer set conv2_1 = mx.sym.Convolution(data=do1, kernel=(5, 5), pad=(2, 2), num_filter=256) act2_1 = mx.sym.LeakyReLU(data=conv2_1, act_type=\"elu\") bn2_1 = mx.sym.BatchNorm(data=act2_1) pool2 = mx.sym.Pooling(data=bn2_1, pool_type=\"max\", kernel=(3, 3), stride=(2, 2)) do2 = mx.sym.Dropout(data=pool2, p=0.25) # Block #3: (CONV =&gt; RELU) * 3 =&gt; POOL conv3_1 = mx.sym.Convolution(data=do2, kernel=(3, 3), pad=(1, 1), num_filter=384) act3_1 = mx.sym.LeakyReLU(data=conv3_1, act_type=\"elu\") bn3_1 = mx.sym.BatchNorm(data=act3_1) conv3_2 = mx.sym.Convolution(data=bn3_1, kernel=(3, 3), pad=(1, 1), num_filter=384) act3_2 = mx.sym.LeakyReLU(data=conv3_2, act_type=\"elu\") bn3_2 = mx.sym.BatchNorm(data=act3_2) conv3_3 = mx.sym.Convolution(data=bn3_2, kernel=(3, 3), pad=(1, 1), num_filter=256) act3_3 = mx.sym.LeakyReLU(data=conv3_3, act_type=\"elu\") bn3_3 = mx.sym.BatchNorm(data=act3_3) pool3 = mx.sym.Pooling(data=bn3_3, pool_type=\"max\", kernel=(3, 3), stride=(2, 2)) do3 = mx.sym.Dropout(data=pool3, p=0.25) # Block #4: first set of FC =&gt; RELU layers flatten = mx.sym.Flatten(data=do3) fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=4096) act4_1 = mx.sym.LeakyReLU(data=fc1, act_type=\"elu\") bn4_1 = mx.sym.BatchNorm(data=act4_1) do4 = mx.sym.Dropout(data=bn4_1, p=0.5) # Block #5: second set of FC =&gt; RELU layers fc2 = mx.sym.FullyConnected(data=do4, num_hidden=4096) act5_1 = mx.sym.LeakyReLU(data=fc2, act_type=\"elu\") bn5_1 = mx.sym.BatchNorm(data=act5_1) do5 = mx.sym.Dropout(data=bn5_1, p=0.5) # softmax classifier fc3 = mx.sym.FullyConnected(data=do5, num_hidden=classes) model = mx.sym.SoftmaxOutput(data=fc3, name=\"softmax\") # return the network architecture return model 几点注意点： 主要采用了Caffe中实现的AlexNet版本； 使用ELU替代了ReLU激活函数； 每个block都使用了dropout，而不是原文只在两个FC层使用；其中CNN层参数为0.25，FC层为0.5； 使用了BN层加速网络训练，而且BN在激活函数之后使用； 训练AlexNet123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# USAGE# python train_alexnet.py --checkpoints checkpoints --prefix alexnet# python train_alexnet.py --checkpoints checkpoints --prefix alexnet --start-epoch 25# import the necessary packagesfrom config import imagenet_alexnet_config as configfrom pyimagesearch.nn.mxconv import MxAlexNetimport mxnet as mximport argparseimport loggingimport jsonimport os# construct the argument parse and parse the argumentsap = argparse.ArgumentParser()ap.add_argument(\"-c\", \"--checkpoints\", required=True, help=\"path to output checkpoint directory\")ap.add_argument(\"-p\", \"--prefix\", required=True, help=\"name of model prefix\")ap.add_argument(\"-s\", \"--start-epoch\", type=int, default=0, help=\"epoch to restart training at\")args = vars(ap.parse_args())# set the logging level and output filelogging.basicConfig(level=logging.DEBUG, filename=\"training_&#123;&#125;.log\".format(args[\"start_epoch\"]), filemode=\"w\")# load the RGB means for the training set, then determine the batch# sizemeans = json.loads(open(config.DATASET_MEAN).read())batchSize = config.BATCH_SIZE * config.NUM_DEVICES# construct the training image iteratortrainIter = mx.io.ImageRecordIter( path_imgrec=config.TRAIN_MX_REC, data_shape=(3, 227, 227), batch_size=batchSize, rand_crop=True, rand_mirror=True, rotate=15, max_shear_ratio=0.1, mean_r=means[\"R\"], mean_g=means[\"G\"], mean_b=means[\"B\"], preprocess_threads=config.NUM_DEVICES * 2)# construct the validation image iteratorvalIter = mx.io.ImageRecordIter( path_imgrec=config.VAL_MX_REC, data_shape=(3, 227, 227), batch_size=batchSize, mean_r=means[\"R\"], mean_g=means[\"G\"], mean_b=means[\"B\"])# initialize the optimizeropt = mx.optimizer.SGD(learning_rate=1e-2, momentum=0.9, wd=0.0005, rescale_grad=1.0 / batchSize)# construct the checkpoints path, initialize the model argument and# auxiliary parameterscheckpointsPath = os.path.sep.join([args[\"checkpoints\"], args[\"prefix\"]])argParams = NoneauxParams = None# if there is no specific model starting epoch supplied, then# initialize the networkif args[\"start_epoch\"] &lt;= 0: # build the LeNet architecture print(\"[INFO] building network...\") model = MxAlexNet.build(config.NUM_CLASSES)# otherwise, a specific checkpoint was suppliedelse: # load the checkpoint from disk print(\"[INFO] loading epoch &#123;&#125;...\".format(args[\"start_epoch\"])) model = mx.model.FeedForward.load(checkpointsPath, args[\"start_epoch\"]) # update the model and parameters argParams = model.arg_params auxParams = model.aux_params model = model.symbol# compile the modelmodel = mx.model.FeedForward( ctx=[mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3), mx.gpu(4), mx.gpu(5), mx.gpu(6), mx.gpu(7)], symbol=model, initializer=mx.initializer.Xavier(), arg_params=argParams, aux_params=auxParams, optimizer=opt, num_epoch=90, begin_epoch=args[\"start_epoch\"])# initialize the callbacks and evaluation metricsbatchEndCBs = [mx.callback.Speedometer(batchSize, 500)]epochEndCBs = [mx.callback.do_checkpoint(checkpointsPath)]metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5), mx.metric.CrossEntropy()]# train the networkprint(\"[INFO] training network...\")model.fit( X=trainIter, eval_data=valIter, eval_metric=metrics, batch_end_callback=batchEndCBs, epoch_end_callback=epochEndCBs) 几个注意点： batchSize = config.BATCH_SIZE * config.NUM_DEVICES; 优化算法使用SGD，初始学习率为1e-2，动量0.9，L2权重正则化参数0.0005；rescale参数尤为关键，根据批的大小放大梯度； model中的ctx参数用于指定用于训练的GPU； 使用了Xavier进行参数初始化，与原模型略有不同，Xavier是目前CNN网络常采用的参数初始化方式； 学习率的控制 Epoch 学习率 1-80 1e-2 81-100 1e-3 86-100 1e-4 控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率； 调整学习率 python train_alexnet.py --checkpoints checkpoints --prefix alexnet \\ --start-epoch 50 在8个GPU上，每轮用时600多秒（当然我batch size设的比较小，这个速度可以提升一个数量级）； 第一遍训练采用1e-2的学习率训练90轮，发现80轮以后，验证集准确率已经不再增加；为此在80轮之后调整学习率；80轮的性能参数如下： INFO:root:Epoch[78] Validation-accuracy=0.517843 INFO:root:Epoch[78] Validation-top_k_accuracy_5=0.759725 INFO:root:Epoch[78] Validation-cross-entropy=2.123822 INFO:root:Saved checkpoint to “checkpoints/1//alexnet01-0080.params” 将学习率调整到1e-3之后，验证集准确率有大幅提升，代表调整有效，100轮之后再度饱和，降低学习率到1e-4;105轮验证集结果如下： INFO:root:Epoch[105] Validation-accuracy=0.564901 INFO:root:Epoch[105] Validation-top_k_accuracy_5=0.796509 INFO:root:Epoch[105] Validation-cross-entropy=1.880376 保持1e-4学习率，训练到125轮，进一步降低学习率到1e-5，发现整个网络没有性能提升，结束该批次参数训练过程。 实验 BN放在激活之前还是之后？ 调整BN位置，准确率从77.9%—&gt; 79.6% ReLU vs ELU ？ 用ELU替代ReLU，提升1-2%的性能； 选取125轮的训练结果，在测试集数据上进行验证，结果如下： 12[INFO] rank-1: 60.20%[INFO] rank-5: 81.99% ​ 结论 训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源； 训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型； 参考 ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"网络复现","slug":"网络复现","permalink":"http://blog.a-stack.com/tags/网络复现/"}]},{"title":"ImageNet-DataSet","slug":"ImageNet-DataSet","date":"2018-07-06T08:02:00.000Z","updated":"2018-07-20T15:52:52.732Z","comments":true,"path":"2018/07/06/ImageNet-DataSet/","link":"","permalink":"http://blog.a-stack.com/2018/07/06/ImageNet-DataSet/","excerpt":"摘要：","text":"摘要： ImageNet数据集ImageNet数据集是伴随着CNN和机器视觉乃至人工智能发展的里程碑式的数据集，虽然利用该数据集大规模机器视觉比赛2017年以后停止举办，但该数据集对整个人工智能产业革命的影响必将持续下去。 ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. Currently we have an average of over five hundred images per node. We hope ImageNet will become a useful resource for researchers, educators, students and all of you who share our passion for pictures. 22k分类，14M 照片 网址： http://image-net.org/ 这些数据靠人来标记，需要一个人19年时间； 使用Amazon的众包服务，调用了167个国家的49000人次在2007-2010年间完成标记 WordNet ImageNet Large Scale Visual Recognition Challenge(ILSVRC) Models are trained on ≈ 1.2 million training images with another 50,000 images for validation (50 images per synset) and 100,000 images for testing (100 images per synset). 分类难度大：比如ImageNet instead includes 120 different breeds of dogs. ILSVRC2017主要有三项挑战： I: Object localization II: Object detection III: Object detection from video 当然，正是由于ImageNet的难度和种类繁多特性，使得在ImageNet训练的模型可以很容易通过迁移学习用到其它领域的图像识别； 数据集的获取 训练数据大小138 GB 验证数据6.3 GB 测试数据 13GB 下载ImageNet数据集需要使用学校邮箱注册一个账号，获得数据下载权限，然后可以登陆数据下载页面如下图下载红色标记部分。 其中Development Kit包含了对数据结构和分类的描述文件，便于我们通过脚本读取图像数据； 可以使用wget命令下载 1wget -t 0 -c -i urls -o log 其中-c代表端点续传，-t 0代表失败重试，urls文件为所有需要下载目录地址： 12345678www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tarwww.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train_t3.tarwww.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tarwww.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_test.tarwww.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_bbox_train_v2.tar.gzwww.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_bbox_train_dogs.tar.gzwww.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_bbox_val_v3.tgzwww.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_bbox_test_dogs.zip 实际使用过程中，发现国内网络原因使用这种方法只有50KB-100KB/s左右的速度，整个数据集下载需要12-15天。为了快速下载，可以使用如下磁力链接地址： 12magnet:?xt=urn:btih:A306397CCF9C2EAD27155983C254227C0FD938E2magnet:?xt=urn:btih:5D6D0DF7ED81EFD49CA99EA4737E0AE5E3A5F2E5 ImageNet数据的从属权问题 ImageNet中的每张图片属于提供图片的个人 ImageNet数据集可以免费用于学术研究和非商业用途 但不能直接使用这些数据作为产品的一部分 使用ImageNet训练的模型（自己从头训练）的从属权问题是一个目前没有答案的问题，目前从头训练一个模型是可以应用到商业软件中的 官方的给出的介绍性内容如下： Does ImageNet own the images? Can I download the images? No, ImageNet does not own the copyright of the images. ImageNet only provides thumbnails and URLs of images, in a way similar to what image search engines do. In other words, ImageNet compiles an accurate list of web images for each synset of WordNet. For researchers and educators who wish to use the images for non-commercial research and/or educational purposes, we can provide access through our site under certain conditions and terms. 数据准备 下载数据集中每个文件采用WordNetID的方式命名，比如n01440764； 每个分类有732~1300张图片 使用MXNET的.rec格式文件存储数据； ​ 12345678dir=./for x in `ls *.tar` do filename=`basename $x .tar` mkdir $filename tar -xvf $x -C ./$filenamedone 关键文件map_clsloc.txt: 记录了WordNet ID到分类名称的映射关系，格式如下： 12345678910n02119789 1 kit_foxn02100735 2 English_settern02110185 3 Siberian_huskyn02096294 4 Australian_terriern02102040 5 English_springern02066245 6 grey_whalen02509815 7 lesser_pandan02124075 8 Egyptian_catn02417914 9 ibexn02123394 10 Persian_cat 构建一个.lst文件，包括： [图像ID， 标签， 图像存储的完整地址] 12345$ wc -l /raid/datasets/imagenet/lists/*.lst 50000 /raid/datasets/imagenet/lists/test.lst 1231167 /raid/datasets/imagenet/lists/train.lst 48238 /raid/datasets/imagenet/lists/val.lst 1329405 total ​ 去除黑名单数据 创建.rec 文件 mxnet提供一个工具im2rec来准备训练数据，工具地址如下： 1anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/tools/im2rec.py 关于im2rec的更多用法: 1) 生成list文件 12&gt; python ~/mxnet/tools/im2rec.py –list True –recursive True –train-ratio 0.9 myData /home/xxx/data/&gt; &gt; 参数： –list：当要生成list文件时，这个参数一定要设为True，表示当前用来生成的list文件；默认是生成rec文件； –recursive：递归的遍历你的所有数据集，要设为True； –train-ratio：用来将你的全部数据集拆分成两部分：训练集（train）和交叉验证集（val），具体多少作为训练集，多少作为验证集，就由这个参数来确定； –test-ratio：同上，分成训练集和测试集两部分； –exts：这个是你数据的后缀（注，这里我们一般说的图片数据），目前的MXNet只支持两种图片格式：jpg和jpeg prefix：这里指的是你要生成list文件的前缀名，我这里命名为myData； root：这里指的是你的图片数据存放的路径； 2）生成rec文件 12&gt; python ~/mxnet/tools/im2rec.py –num-thread 4 –pass-through 1 myData /home/xxx/data/&gt; 参数： Options for creating database: —pass-through whether to skip transformation and save image as is (default: False) —resize RESIZE resize the shorter edge of image to the newsize, original images will be packed by default. —center-crop specify whether to crop the center image to make it rectangular. (default: False) —quality QUALITY JPEG quality for encoding, 1-100; or PNG compression for encoding, 1-9 (default: 95) —num-thread NUM_THREAD number of thread to use for encoding. order of images will be different from the input list if &gt;1. the input list will be modified to match the resulting order. (default: 1) —color {-1,0,1} specify the color mode of the loaded image. 1: Loads a color image. Any transparency of image will be neglected. It is the default flag. 0: Loads image in grayscale mode. -1:Loads image as such including alpha channel. (default: 1) —encoding {.jpg,.png} specify the encoding of the images. (default: .jpg) —pack-label Whether to also pack multi dimensional label in the record file (default: False) 使用如下命令分别准备训练集、验证集和测试集数据： 12345python im2rec.py /home/ubuntu/data/imagenet/lists/val.lst \"\" --num-thread 16 --resize 256 --encoding '.jpg' --quality 100 python im2rec.py /home/ubuntu/data/imagenet/lists/train.lst \"\" --num-thread 16 --resize 256 --encoding '.jpg' --quality 100python im2rec.py /home/ubuntu/data/imagenet/lists/test.lst \"\" --num-thread 16 --resize 256 --encoding '.jpg' --quality 100 其它相关内容 李飞飞在ILSVRC2017的报告","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"},{"name":"机器视觉","slug":"动手实践营/机器视觉","permalink":"http://blog.a-stack.com/categories/动手实践营/机器视觉/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"数据集","slug":"数据集","permalink":"http://blog.a-stack.com/tags/数据集/"}]},{"title":"深度学习实践方法论","slug":"深度学习实践方法论","date":"2018-07-03T09:34:48.000Z","updated":"2018-07-20T15:53:03.168Z","comments":true,"path":"2018/07/03/深度学习实践方法论/","link":"","permalink":"http://blog.a-stack.com/2018/07/03/深度学习实践方法论/","excerpt":"摘要： 深度学习实践的方法论是一套靠经验而非算法总结的行之有效的指导，后续随着研究深入将不断更新补充相关内容。","text":"摘要： 深度学习实践的方法论是一套靠经验而非算法总结的行之有效的指导，后续随着研究深入将不断更新补充相关内容。 概述从Cousera学习吴恩达的机器学习、深度学习系列课程便开始希望能够根据课程学习内容，逐步总结深度学习算法在项目实践中应该如何应用，是否能够总结一份最佳实践手册，并通过不断的更新维护，方便日后查阅和指导深度学习工程师的项目开发。最近在看Goodfellow的深度学习圣经——《Deep Learning》，第十一章再次讲到了这个问题，于是便以书中内容为基础，总结这篇深度学习实践的博客。 正如书中所说要成功地使用深度学习技术，仅仅知道存在哪些算法和解释他们为何有效的原理是不够的。一个优秀的机器学习实践者还需要知道如何针对具体应用挑选一个合适的算法以及如何监控，并根据实验反馈改进机器学习系统。书中建议参考如下几个实践设计流程： 确定目标——使用什么样的误差度量，并为此误差度量指定目标值。这些目标 和误差度量取决于该应用旨在解决的问题。 尽快建立一个端到端的工作流程，包括估计合适的性能度量。 搭建系统，并确定性能瓶颈。检查哪个部分的性能差于预期，以及是否是因 为过拟合、欠拟合，或者数据或软件缺陷造成的。 根据具体观察反复地进行增量式的改动，如收集新数据、调整超参数或改进算 法。 问题的定义和描述 选定误差度量标准； 科研中，目标通常是在某个确定基准下探讨哪个算法更好，一般会固定训练集，不允许收集更多的数据。 性能期望的设计 度量： 准确率、错误率，召回率，精确率，F1分数，mAP … 覆盖（coverage）： 机器学习系统能够产生响应的样本所占的比率； 基准模型 明确任务的度量和目标之后，需要快速的迭代第一个端到端的系统。 项目开始时，可能无需使用深度学习，构建一个简单的统计模型可以快速实现原型开发，当然如果问题本身属于“AI-完全”的，如对象识别、语音识别、机器翻译，那么可以根据数据的结构选择从一个基本的CNN/RNN/DNN模型开始。 优化算法方面，具有衰减学习率以及动量的SGD是优化算法一个合理的选择（流行的衰减方法有，衰减到固定最低学习率的线性衰减、指数衰减，或每次发生验证错误停滞时 将学习率降低 2 − 10 倍，这些衰减方法在不同问题上好坏不一）。 也可以直接使用Adam算法 批标准化可以视情况逐步采用； 如果项目内容和别人已经完成的模型任务很相像，复制现成的网络结构或利用迁移学习fine-tune也是这阶段应该考虑的； 数据收集 收集更多数据对于深度学习来说是最直接改观性能的方式，但何时收集，是否应该收集需要评估投入-产出比； 也许正则化可以弥补模型的误差期望差距，没有必要投入代价更好的数据收集工作； 如果收集数据，应该收集多少数据？ 如图所示，绘制曲线显示训练集规模和泛化误差之间的关系是很有帮助的。根据走势延伸曲线，可以预测还需要多少训练数据来达到一定的性能。 超参选择大部分深度学习算法都有许多超参数来控制不同方面的算法表现。有些超参数会影响算法运行的时间和存储成本。有些超参数会影响学习到的模型质量，以及 在新输入上推断正确结果的能力。 有两种选择超参数的基本方法：手动选择和自动选择。 手动调节超参手动搜索超参数的目标通常是最小化受限于运行时间和内存预算的泛化误差。通过调整模型的有效容量以匹配任务的复杂性。有效容量受限于三个因素： 模型的表示容量 学习算法成功最小化训练模型代价函数的能力 代价函数和训练过程正则化模型的程度 学习率可能是最重要的超参数，如果你只有时间调整一个超参，那就调整学习率。相比其他超参数，它以一种更复杂的方式控制模型的有效容量——当学习率适合优化问题时，模型的有效容量最高，此时学习率是正确的，既不是特别大也不是特别小。学习率关于训练误差具有 U 形曲线，如图所示。当学习率过大时，梯度下降可能会不经意地增加而非减少训练误差。在理想化的二次情况下，如果学习率是最佳值的两倍大时，会发生这种情况 (LeCun et al., 1998b)。当学习率太小，训练不仅慢，还有可能永久停留在一个很高的训练误差。 实践中能够确保学习有效的暴力方法就是不断提高模型容量和训练 集的大小，直到解决问题。 自动超参优化如果我们仔细想想使用者搜索学习算法合适超参数的方式，我们会意识到这其实是一种优化：我们在试图寻找超参数来优化目标函数，例如验证误差，有时还会有一些约束（如训练时间，内存或识别时间的预算）。 网格搜索（grid search）是超参选择的常用方法，利用几个小的有限集训练参数模型，从中挑选验证集误差最小的超参。超参范围的设定一般采用对数尺度小挑选合适的值。 随机搜索： 为每个超参数定义一个边缘分布，或者对数尺度上的均匀分布。与网格搜索不懂，不需要离散化超参数的值。实践证明，随机搜索效率比网格搜索更高。 调整策略 调试策略 可视化计算中模型的行为：当训练模型检测图像中的对象时，查看一些模型检测到部分重叠的图像。在训练语音生成模型时，试听一些生成的语音样本。这似乎是显而易见的，但在实际中很容易只注意量化性能度量，如准确率或对数似然。直接观察机器学习模型运行其任务，有助于确定其达到的量化性能数据是否看上去合 理。错误评估模型性能可能是最具破坏性的错误之一，因为它们会使你在系统出问题时误以为系统运行良好。 可视化最严重的错误：大多数模型能够输出运行任务时的某种置信度量。例如，基于softmax 函数输出层的分类器给每个类分配一个概率。因此，分配给最有可能的类的概率给出了模型在其分类决定上的置信估计值。通常，相比于正确预测的概率最大似然训练会略有高估。但是由于实际上模型的较小概率不太可能对应着正确 的标签，因此它们在一定意义上还是有些用的。通过查看训练集中很难正确建模的样本，通常可以发现该数据预处理或者标记方式的问题。 根据训练和测试误差检测软件：我们往往很难确定底层软件是否是正确实现。训练和测试误差能够提供一些线索。如果训练误差较低，但是测试误差较高，那么很有可能训练过程是在正常运行，但模型由于算法原因过拟合了。另一种可能是，测试误差没有被正确地度量，可能是由于训练后保存模型再重载去度量测试集时出现 问题，或者是因为测试数据和训练数据预处理的方式不同。如果训练和测试误差都很高，那么很难确定是软件错误，还是由于算法原因模型欠拟合。这种情况需要进一步的测试。 拟合极小的数据集：当训练集上有很大的误差时，我们需要确定问题是真正的欠拟合，还是软件错误。通常，即使是小模型也可以保证很好地拟合一个足够小的数据集。例如，只有一个样本的分类数据可以通过正确设置输出层的偏置来拟合。通常，如果不能训练一个分类器来正确标注一个单独的样本，或不能训练一个自编码 器来成功地精准再现一个单独的样本，或不能训练一个生成模型来一致地生成一个单独的样本，那么很有可能是由于软件错误阻止训练集上的成功优化。此测试可以扩展到只有少量样本的小数据集上。 比较反向传播导数和数值导数：如果读者正在使用一个需要实现梯度计算的软件框架，或者在添加一个新操作到求导库中，必须定义它的 bprop 方法，那么常见的错误原因是没能正确地实现梯度表达。验证这些求导正确性的一种方法是比较实现的自动求导和通过有限差分（finite difference）计算的导数。 监控激活函数值和梯度的直方图：可视化神经网络在大量训练迭代后（也许是一个轮）收集到的激活函数值和梯度的统计量往往是有用的。隐藏单元的预激活值可以告诉我们该单元是否饱和，或者它们饱和的频率如何。例如，对于整流器，它们多久关一次？是否有单元一直关闭？对于双曲正切单元而言，预激活绝对值的平均值可以告诉我们该单元的饱和程度。在深度网络中，传播梯度的快速增长或快速消失，可能会阻碍优化过程。最后，比较参数梯度和参数的量级也是有帮助的。正如 (Bottou, 2015) 所建议的，我们希望参数在一个小批量更新中变化的幅度是参数量值 1% 这样的级别，而不是50% 或者0.001%（这会导致参数移动得太慢）。也有可能是某些参数以良好的步长移动，而另一些停滞。如果数据是稀疏的（比如自然语言），有些参数可能很少更新，检测它们变化时应该记住这一点。 参考 《Deep Learning》Book Ng, A. (2015). Advice for applying machine https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf.","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"实践","slug":"实践","permalink":"http://blog.a-stack.com/tags/实践/"}]},{"title":"Thinking Stats","slug":"thinking-stats","date":"2018-06-20T12:09:03.000Z","updated":"2018-07-20T15:53:25.907Z","comments":true,"path":"2018/06/20/thinking-stats/","link":"","permalink":"http://blog.a-stack.com/2018/06/20/thinking-stats/","excerpt":"","text":"Pandas技巧1. 统计属性的独立值数目1pd[\"outcome\"].value_counts().sort_index() 数据的相关性衡量两组数据或数据的两个变量（维度）线性相关性的方法主要有： 皮尔逊相关系数 斯皮尔曼相关系数 最小二乘拟合 协方差标准分数(standard score): $z_i = (x_i - \\mu)/\\sigma$ 其中$x_i - \\mu$称为离差，描述了$x_i$与均值$\\mu$的差异，除以方差是为了归一化偏差，这样形成的数据集$Z$就是均值为0，方差为1，且与原数据集$X$同分布，且统一单位为1，不需要再考虑原先不同数据维度的单位不同的问题。 协方差(covariance)衡量相关变量变化趋势是否相同，定义如下： Cov(X,Y) = \\frac{1}{n}\\sum(x_i-\\mu_x)(y_i-\\mu_y)由定义可知，协方差描述了两组数据离差相乘的加和，可以描述两个序列的变化是否一致。 但由于X,Y本身单位有可能不同，没有被归一化，导致结果很难反应实际的情况。 皮尔逊相关系数皮尔逊相关系数（Pearson’s correlation）: \\rho = \\frac{1}{n}\\sum\\frac{(x_i-\\mu_x)}{\\sigma_x} \\frac{(y_i-\\mu_y)}{\\sigma_y}=\\frac{Cov(X,Y)}{\\sigma_x \\sigma_y}这样相关系数的单位为1，且取值范围为-1到1。$\\rho$决定值的大小描述了两个变量的线性相关程度。当$\\rho=1$时，两个变量完全正相关，当$\\rho=0$时，两个变量完全负相关。 但是$\\rho=0$并不代表着两个变量毫无关系，皮尔逊系数只能衡量两个变量之间的线性关系，如果两个变量是非线性相关，无法通过该系数描述。 上图描述了具有一定相关性的示例数据和对应的相关系数，第一行为一组有相关性和无线性相关性的数据，第二组为对应的严格相关数据，第三行为非线性相关数据，但相关系数为0。 斯皮尔曼秩相关是为了解决皮尔逊相关系数对异常值敏感的问题，斯皮尔曼秩相关系数（Spearman’s Rank Correlation）可以用在存在异 常值和变量分布非常不对称的情况。 首先计算序列中数值的秩（rank），即某个数据在序列中大小排序的序号，将序列转换为秩之后再计算皮尔逊相关系数。 散点图观察数据相关性既然皮尔逊系数无法完全表征数据的相关性，我们可以通过画散点图的方法来直观的观察数据的相关性。 scatter hexbin 补充相关代码 最小二乘拟合相关系数可以描述两个变量的线性相关性强弱，但无法评估他们的斜率，最小二乘法可以通过数据拟合的方式计算斜率。首先定义数据的预测偏差(残差)：$\\epsilon_i = (\\alpha + \\beta x_i) - y_i$， 通过求解最小化残差的平方和，可以获得相关参数。 选择残差平方和最小作为最优化目标的原因： 平方能将正残差和负残差都变成正数，这符合我们的目标。 平方相当于给残差赋予了一个权重，越大的残差（绝对量）被赋予 的权重越大。但是并不是所有情况下大的残差都应该被赋予大的权 重，因为这样拟合方程就很容易受到异常值的影响。 在残差服从均值为 0、方差为$\\sigma^2$的正态分布，且在残差与 x 独立的假设下，参数的最小二乘估计结果与极大似然估计量相同。 斜率可以计算为： \\hat\\beta = \\frac{Cov(X,Y)}{Var(X)}相关性不等于因果性两个变量的相关关系并不代表他们之间存在因果关系，Correlation does not imply causation。如何确认两个变量的因果关系，一般采用随机对照试验和自然试验(natural experiment)。 随机对照试验多用于实验研究和药物研发，通过设立实验组和对照组来控制变量； 自然试验通过尽量控制群体在各方面是相似的，然后对不同群体实施不同的处理。 估计 极大似然估计（Maximum Likelihood Estimator，MLE）：根据已有信息获得的最大可能估计； 最小误差值估计（最小二乘估计）：分布数据均匀，没有且很少有异常值的情况下； 无偏估计：$\\sigma^2$的无偏估计为$S_{n-1}^2$ 估计是无偏的：估计量的数学期望等于被估计参数的真实值，则称此此估计量为被估计参数的无偏估计，即具有无偏性，是一种用于评价估计量优良性的准则。无偏估计的意义是：在多次重复下，它们的平均数接近所估计的参数真值。 贝叶斯估计 贝叶斯置信区间 经典问题： 火车头问题（德国坦克问题） 铁路公司将它所有的火车头都进行了编号，从1到N。有一天 你看见一个编号为60的火车头，那该铁路公司总共有多少火 车头呢？ 对于一个给定的估计量$\\hat N$ ，观测到编号为 $i(i \\le \\hat N)$的火车的概率为$1/\\hat N$，$i&gt;\\hat N$的概率为 0。所以 N的极大似然估计量是$\\hat N =i$ 。换言之,如果观测到的火车的编号是 60，而且我们要以最大的概率保证结果的 正确性，那么我们就会猜测铁路公司有 60 辆火车。 但从均方误差最小化的角度来看，上述结果不理想，我们需要选择一个$\\hat N = ai$ 使得估计的均方误差最小： \\min \\frac{1}{N}\\sum^N_{i=1} (ai-N)^2从无偏估计角度出发，令平均误差为零，即 ME = \\frac{1}{N}\\sum^N_{i=1} (ai-N)=0贝叶斯估计 假如有足够的先验信息，那么所有的估计量将倾向于收敛到同一个值。 时序数据分析参考 ​","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"}]},{"title":"Feature_Visualization_in_NN","slug":"Feature-Visualization-in-NN","date":"2018-05-22T02:23:04.000Z","updated":"2018-07-20T15:53:39.979Z","comments":true,"path":"2018/05/22/Feature-Visualization-in-NN/","link":"","permalink":"http://blog.a-stack.com/2018/05/22/Feature-Visualization-in-NN/","excerpt":"摘要：","text":"摘要： Feature visualization answers questions about what a network — or parts of a network — are looking for by generating examples. Attribution 1 studies what part of an example is responsible for the network activating a particular way. TODO: 根据参考文献整理相关内容和思路。 参考 Feature Visualization -How neural networks build up their understanding of images The Building Blocks of Interpretability","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"可视化","slug":"可视化","permalink":"http://blog.a-stack.com/tags/可视化/"}]},{"title":"经典网络归纳： ResNet","slug":"ResNet","date":"2018-05-21T12:14:49.000Z","updated":"2018-05-22T01:35:37.537Z","comments":true,"path":"2018/05/21/ResNet/","link":"","permalink":"http://blog.a-stack.com/2018/05/21/ResNet/","excerpt":"摘要： 作为深度卷积神经网络的里程碑式的作品，ResNet为卷积网络往更深层次扩展指明了方向，本文结合相关论文总结一下ResNet中创造性的想法。","text":"摘要： 作为深度卷积神经网络的里程碑式的作品，ResNet为卷积网络往更深层次扩展指明了方向，本文结合相关论文总结一下ResNet中创造性的想法。 概述随着网络深度增加，人们发现出现了Degradation问题，这不是过拟合导致的，因为在训练数据集上发现了同样的问题。ResNet解决了深层神经网络难训练的问题，在此之前要训练一个层次较深的网络需要在参数初始化上下功夫，当然还需要一定的运气。 ResNet网络获得了2015年所有的主流比赛冠军，相关内容被发表在论文《Deep Residual Learning for Image Recognition》中。除了残差网络的引入，在ResNet我们也看到逐渐摒弃了全连接层、池化层。全部网络中只在最开始使用了一个max pooling层，在最后使用了average pooling层。 2016年，在上述论文的基础上，He发表了第二篇改进的论文 2 残差网络 y = F(x, {W_i}) + x “F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如s输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器” 在ResNet中，残差使得网络学习更快，可以使用更高的学习率，比如常用的起始学习率为0.1。 Boottleneck残差网络中，一般最后一个1x1卷积的filter数目是另外两个的4倍； 残差网络可以从另一个角度理解，如下图所示，残差网络可以看成是由多种路径组合的一个网络，即，残差网络其实是很多并行子网络的组合。 网络结构 Highway NetworksResNet的改进及优化 He在2016年发布论文《Identity Mappings in Deep Residual Networks》，对ResNet中的一些内容进行了进一步的优化及完善，并证明了“identity Mapping” 的最优配置：$h(x_l) = x_l$。 论文中提出了在ResNet单元中采取“预激活”的方式，参考图（b）中方案，在权重更新之前先进行BN层和ReLU层操作。实践证明，采用这种方式更容易训练深层网络,同时具备更好的泛化性能。采用（b）的误差为6.36%小于(a)的6.61% 作者使用如下图所示的多种不同的残差单元比较性能，结果表明直连的方式效果最佳。 网络训练时间： 在CIFAR数据集，ResNet-1001，使用2GPU训练27小时； 在ImageNet数据集，ResNet-200使用8块GPU训练3周 实现Keras中已经包含了ResNet50 12from keras.applications.resnet50 import ResNet50ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) 当 include_top=False时，可以通过input_shape 调整输入图像尺寸，但图像高度不小于197；默认大小为(224,24,3) pooling: 可选，当 include_top 为 ‘False’ 时，该参数指定了特征提取时的池化方式。 None 代表不池化，直接输出最后一层卷积层的输出，该输出是一个四维张量。 avg 代表全局平均池化（GLobalAveragePool2D），相当于在最后一层卷积层后面再加一层全局平均池化层，输出是一个二维张量。 max 代表全局最大池化 利用ResNet50的迁移学习方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from keras.preprocessing import imagefrom keras.applications.resnet50 import preprocess_input, decode_predictionsimport numpy as npfrom keras.applications.resnet50 import ResNet50model = ResNet50(weights='imagenet')## 直接使用进行图像分类img_path = 'elephant.jpg'img = image.load_img(img_path, target_size=(224, 224))x = image.img_to_array(img)x = np.expand_dims(x, axis=0)x = preprocess_input(x)preds = model.predict(x)## 迁移学习1image_input = Input(shape=(224, 224, 3))model = ResNet50(input_tensor=image_input, include_top=True,weights='imagenet')last_layer = model.get_layer('avg_pool').outputx= Flatten(name='flatten')(last_layer)out = Dense(num_classes, activation='softmax', name='output_layer')(x)custom_resnet_model = Model(inputs=image_input,outputs= out)for layer in custom_resnet_model.layers[:-1]: layer.trainable = Falsecustom_resnet_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])## Fine Tuneimage_input = Input(shape=(224, 224, 3))model = ResNet50(weights='imagenet',include_top=False)last_layer = model.output# add a global spatial average pooling layerx = GlobalAveragePooling2D()(last_layer)# add fully-connected &amp; dropout layersx = Dense(512, activation='relu',name='fc-1')(x)x = Dropout(0.5)(x)x = Dense(256, activation='relu',name='fc-2')(x)x = Dropout(0.5)(x)# a softmax layer for 4 classesout = Dense(num_classes, activation='softmax',name='output_layer')(x)# this is the model we will traincustom_resnet_model2 = Model(inputs=model.input, outputs=out)for layer in custom_resnet_model2.layers[:-6]: layer.trainable = False custom_resnet_model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) 参考 Deep Residual Learning for Image Recognition Identity Mappings in Deep Residual Networks Keras实现","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"机器视觉","slug":"深度学习/机器视觉","permalink":"http://blog.a-stack.com/categories/深度学习/机器视觉/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"图像分类","slug":"图像分类","permalink":"http://blog.a-stack.com/tags/图像分类/"}]},{"title":"OpenCV and Python","slug":"OpenCV-and-Python","date":"2018-05-19T14:16:29.000Z","updated":"2018-05-22T07:07:39.842Z","comments":true,"path":"2018/05/19/OpenCV-and-Python/","link":"","permalink":"http://blog.a-stack.com/2018/05/19/OpenCV-and-Python/","excerpt":"摘要： 本文整理了常用的OpenCV图像处理操作的python代码，便于后续使用过程中的速查。","text":"摘要： 本文整理了常用的OpenCV图像处理操作的python代码，便于后续使用过程中的速查。 概述OpenCV： is an image and video processing library with bindings in C++, C, Python, and Java. OpenCV is used for all sorts of image and video analysis, like facial recognition and detection, license plate reading, photo editing, advanced robotic vision, optical character recognition, and a whole lot more. python-OpenCV: OpenCV的python版实现 python-OpenCV 只是OpenCV部分功能的实现，一个完整的OpenCV包大小超过3G 图像/视频读写 图像读写 123456789101112import cv2from matplotlib import pyplot as plt#IMREAD_GRAYSCALE（0），IMREAD_COLOR（1），IMREAD_UNCHANGED（-1）img = cv2.imread('test.jpg',cv2.IMREAD_GRAYSCALE)cv2.imshow('image',img)cv2.waitKey(0)cv2.destroyAllWindows()plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')# write an imagecv2.imwrite('watchgray.png',img) ​ 视频操作 123456789101112131415import numpy as npimport cv2cap = cv2.VideoCapture(0) while(True): ret, frame = cap.read() gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) cv2.imshow('frame',gray) if cv2.waitKey(1) &amp; 0xFF == ord('q'): breakcap.release()cv2.destroyAllWindows() 保存视频录像： 123456789101112131415161718import numpy as npimport cv2cap = cv2.VideoCapture(1)fourcc = cv2.VideoWriter_fourcc(*'XVID')out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))while(True): ret, frame = cap.read() gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) out.write(frame) cv2.imshow('frame',gray) if cv2.waitKey(1) &amp; 0xFF == ord('q'): breakcap.release()out.release()cv2.destroyAllWindows() 画图 Line cv2.line(img,(0,0),(150,150),(255,255,255),15) ​ 对象， 起始点， 结束点， 颜色， 粗细 rectangle cv2.rectangle(img,(15,25),(200,150),(0,0,255),15) ​ 对象;起点(x,y); 终点(x,y); 颜色；粗细 circle cv2.circle(img,(100,63), 55, (0,255,0), -1) ​ 中心点；半径； 颜色； 粗细（-1—填充） 多边形 12345pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)# OpenCV documentation had this code, which reshapes the array to a 1 x 2. I did not # find this necessary, but you may:#pts = pts.reshape((-1,1,2))cv2.polylines(img, [pts], True, (0,255,255), 3) 添加文字 123font = cv2.FONT_HERSHEY_SIMPLEXcv2.putText(img,'OpenCV Tuts!',(0,130), font, 1, (200,255,155), 2, cv2.LINE_AA)# 起点 大小 颜色 粗细 图像操作12345print(img.shape)print(img.size)print(img.dtype)# 图像填充img[100:150,100:150] = [255,255,255] 叠加图像（png图像效果佳，调整为same size） 123456789101112import cv2import numpy as np# 500 x 250img1 = cv2.imread('3D-Matplotlib.png')img2 = cv2.imread('mainsvmimage.png')add = img1+img2cv2.imshow('add',add)cv2.waitKey(0)cv2.destroyAllWindows() 与cv2.add(img1,img2)区分，cv2.add()是每个像素大小的求和；可以采用加权求和策略，weighted = cv2.addWeighted(img1, 0.6, img2, 0.4, 0) 图像组合(TODO)Threshold123retval, threshold = cv2.threshold(grayscaled, 10, 255, cv2.THRESH_BINARY)# adaptive thresholdth = cv2.adaptiveThreshold(grayscaled, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1) 模板匹配1234567891011121314151617import cv2 import numpy as npimg = cv2.imread(\"test.jpg\")img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)template = cv2.imread(\"template.png\",0)w, h = template.shaperes = cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED)threshold = 0.8loc = np.where( res &gt;= threshold)for pt in zip(*loc[::-1]): cv2.rectangle(img, pt, (pt[0] + w, pt[1] + h), (0,255,255), 2)cv2.imshow('Detected',img) 其它参考 OpenCV with Python Intro and loading Images tutorial","categories":[{"name":"工具","slug":"工具","permalink":"http://blog.a-stack.com/categories/工具/"},{"name":"机器视觉","slug":"工具/机器视觉","permalink":"http://blog.a-stack.com/categories/工具/机器视觉/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"工具","slug":"工具","permalink":"http://blog.a-stack.com/tags/工具/"},{"name":"OpenCV","slug":"OpenCV","permalink":"http://blog.a-stack.com/tags/OpenCV/"}]},{"title":"cs231n课程笔记:（Lecture 6-7）Training Neural Networks","slug":"cs231n-lecture-6","date":"2018-05-07T08:50:26.000Z","updated":"2018-05-16T14:00:03.181Z","comments":true,"path":"2018/05/07/cs231n-lecture-6/","link":"","permalink":"http://blog.a-stack.com/2018/05/07/cs231n-lecture-6/","excerpt":"","text":"摘要： 计划花一个月的时间刷一遍斯坦福的机器视觉课程cs231n，并做笔记记录每天学习到的内容。 概述cs231n是斯坦福在深度学习和机器视觉领域的入门经典课程，相关资源如下： 课程主页： http://cs231n.stanford.edu/ 课程Notes：http://cs231n.github.io/ Lecture 6 Thursday April 19 Training Neural Networks, part I Activation functions, initialization, dropout, batch normalization [slides] Neural Nets notes 1Neural Nets notes 2Neural Nets notes 3tips/tricks: [1], [2], [3] (optional) Deep Learning [Nature] (optional) Discussion Section Friday April 20 Tips and tricks for tuning NNs [slides] Lecture 7 Tuesday April 24 Training Neural Networks, part II Update rules, ensembles, data augmentation, transfer learning [slides] Neural Nets notes 3 Batch Normalization Batch Normalization的计算 \\begin{align} \\mu_i = \\frac{1}{m} \\sum_{k \\in S_i} x_k\\\\ \\sigma_i = \\sqrt{\\frac{1}{m}\\sum_{k\\in S_i} (x_k-\\mu_i)^2 + \\epsilon}\\\\ y_i = \\lambda \\hat x_i + \\beta \\end{align}由于BN会受到batch size大小的影响，如果batch size太小，算出的均值和方差就会不准确，太大存储可能不够用。所以衍生出了几种优化表达。 其它归一化方法与BN的区别 BatchNorm： batch方向做归一化，计算 N*H*W 的均值； LayerNorm： channel方向做归一化，计算C*H*W 的均值； InstanceNorm： 一个channel内做归一化，计算H*W的均值； GroupNorm： 将Channel方向分为Group，然后每个Group内做归一化，计算(C//G)*H*W的均值 当G=C时，GroupNorm为LayerNorm，当G=1时，GroupNorm为InstanceNorm 训练过程的网络优化技巧 Parammeter tuning is more of an art. 网络优化/提升网络性能方法 获取更多训练数据 增加网络复杂度 选择更多优化算法 训练更长时间 修改批大小 尝试正则化 权衡过拟合/欠拟合 … 参数调优输入（超参）： 系统架构 学习率、优化算法 正则化（Dropout） 批处理/批量归一化（BN） 输出（分析图表）： 损失曲线 梯度基准 准确率 训练/验证数据集性能 其它 架构选择与设计 架构选择 分类问题：AlexNet,VGG, ResNet,DenseNet, … 语义分割：FCN, Dilated Convolution, Mask RCNN 识别： Faster-RCNN, YOLO, SSD 图像生成： UNet, Dilated Convolution, DCGAN, WGAN … 输入适配 数据集适配 输出任务适配 输出结果分析 Loss不变化，网络没有学到任何信息：梯度没有应用到权重上，或者不匹配 过拟合 不收敛：训练时间不足/学习率过低 慢启动 梯度更新方向错误 数据未打乱 损失函数出现nans值：模型中数据不稳定/较高的学习率 验证集效果优于训练集：验证集太小或分布异常 正则化 DropOut本质上也是一种模型组合学习 1234# trainH = np.maximum(0, np.dot(W,X) + b)U = (np.random.rand(*H.shape) &lt; p) / pH *= U ​ 参考 Fei,Nish, Tips and tricks for tuning NNs","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/"},{"name":"课程笔记","slug":"读书笔记/课程笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/课程笔记/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"},{"name":"笔记","slug":"笔记","permalink":"http://blog.a-stack.com/tags/笔记/"}]},{"title":"cs231n课程笔记:（Lecture 5）CNN基础","slug":"cs231n-lecture-5","date":"2018-05-03T08:55:26.000Z","updated":"2018-05-16T13:59:54.044Z","comments":false,"path":"2018/05/03/cs231n-lecture-5/","link":"","permalink":"http://blog.a-stack.com/2018/05/03/cs231n-lecture-5/","excerpt":"","text":"摘要： 计划花一个月的时间刷一遍斯坦福的机器视觉课程cs231n，并做笔记记录每天学习到的内容。 概述cs231n是斯坦福在深度学习和机器视觉领域的入门经典课程，相关资源如下： 课程主页： http://cs231n.stanford.edu/ 课程Notes：http://cs231n.github.io/ Event Type Date Description Course Materials Lecture 5 Tuesday April 17 Convolutional Neural Networks History Convolution and pooling ConvNets outside vision [slides] ConvNet notes Convolutional Neural Network(CNN/ConvNet) 更多内容参见4 FCN vs CNN CNN的层次结构 The brain view. If you’re a fan of the brain/neuron analogies, every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially (since these numbers all result from applying the same filter). Local Connectivity. filter 大小的区域内容，卷积层的每个神经元只与前一层filter中的神经元保持连接，深度为上一层网络的深度。 卷积层的输出： \\frac{W - F + 2P}{S} + 1其中， $W$ 代表输入层大小； $F$ filter的大小； $P$ Zero-Padding的大小； $S$ 滑动窗口滑动步长 当$S=1$, $P=(F-1)/2$ 时，输入输出网络具有相同的大小。 参数共享： 卷积神经网络每一层共享同一套权重参数（W，b），降低参数总量。 全部参数数目为：$(F\\cdot F \\cdot D_1 +1)* K$,其中 $D_1$ 为上一层的深度，$K$ 为filter个数。 直观解释，在一个图像一个filter区域学到的特征（比如纹理），在图像其它地方用来匹配纹理特征也奏效 池化层 一般max pooling常采用的参数为$F=3,S=2$ (Overlapping pooling),$F=2, S=2$ (Max Pooling); 反向传播时，对于$max(x,y)$ 形式，梯度传递最大的激活值； 一般很少使用$F \\gt 3$ 的filter 目前很多新的论文中已经尽量避免使用池化操作，构建全卷积的神经网络架构，除了最后一层采用全局池化层。 FC和CNN可以相互转换 FC表示CNN： 使用一个很大的参数矩阵，矩阵中多数单元为0少数表征本地连接的地方为非0，同时很多单元的权重相同（参数共享的描述）； CNN表示FC： 使用K层的1x1卷积网络可以描述K个节点的FC。 VGG12345678910111213141516171819202122232425INPUT: [224x224x3] memory: 224*224*3=150K weights: 0CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864POOL2: [112x112x64] memory: 112*112*64=800K weights: 0CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456POOL2: [56x56x128] memory: 56*56*128=400K weights: 0CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824POOL2: [28x28x256] memory: 28*28*256=200K weights: 0CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296POOL2: [14x14x512] memory: 14*14*512=100K weights: 0CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296POOL2: [7x7x512] memory: 7*7*512=25K weights: 0FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)TOTAL params: 138M parameters 存储空间的使用主要是前面几层中间变量的存储；权重数目主要集中在后面几个全联通网络中。 存储空间的计算：数据个数 x 4(bytes) 参考 4. cs231n notes: CNN &#8617;","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/"},{"name":"课程笔记","slug":"读书笔记/课程笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/课程笔记/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"},{"name":"笔记","slug":"笔记","permalink":"http://blog.a-stack.com/tags/笔记/"}]},{"title":"cs231n课程笔记：（Lecture 4）神经网络基础","slug":"cs231n-lecture-4","date":"2018-05-03T08:52:26.000Z","updated":"2018-05-16T13:59:45.204Z","comments":false,"path":"2018/05/03/cs231n-lecture-4/","link":"","permalink":"http://blog.a-stack.com/2018/05/03/cs231n-lecture-4/","excerpt":"","text":"摘要： 计划花一个月的时间刷一遍斯坦福的机器视觉课程cs231n，并做笔记记录每天学习到的内容。 概述cs231n是斯坦福在深度学习和机器视觉领域的入门经典课程，相关资源如下： 课程主页： http://cs231n.stanford.edu/ 课程Notes：http://cs231n.github.io/ Event Type Date Description Course Materials Lecture 4 Thursday April 12 Introduction to Neural Networks BackpropagationMulti-layer PerceptronsThe neural viewpoint [slides] [backprop notes][linear backprop example][derivatives notes] (optional) [Efficient BackProp] (optional)related: [1], [2], [3] (optional) Discussion Section Friday April 13 Backpropagation [slides] 神经网络 更多内容参见123 这课程的大多数内容也综合了Lecture 6和Lecutre 7的内容，所以那两节课不单独写博客了。 一个人的神经系统大概有860亿个神经元，形成$10^{14}-10^{15}$ 个神经元链接； Coarse model. It’s important to stress that this model of a biological neuron is very coarse: For example, there are many different types of neurons, each with different properties. The dendrites in biological neurons perform complex nonlinear computations. The synapses are not just a single weight, they’re a complex non-linear dynamical system. The exact timing of the output spikes in many systems is known to be important, suggesting that the rate code approximation may not hold. Due to all these and many other simplifications, be prepared to hear groaning sounds from anyone with some neuroscience background if you draw analogies between Neural Networks and real brains. See this review (pdf), or more recently this review if you are interested. 激活函数 Sigmoid： $\\sigma(x) = 1 / (1 + e^{-x})$ 1)梯度饱和；2)函数形式不是中心对称的（这个特性会导致后面网络层的输入也不是零中心的，进而影响梯度下降的运作）； Tanh: $\\tanh(x) = 2 \\sigma(2x) -1$ 1)梯度饱和；2）中心对称[-1,1]； ReLU: $f(x) = \\max(0, x)$ 1)线性非饱和的形式使得其收敛速度是Tanh的6倍； 2）计算代价低；3）难训练，容易陷入”死区”，需要严格配置合适的学习率； Leaky ReLU： $f(x) = \\mathbb{1}(x &lt; 0) (\\alpha x) + \\mathbb{1}(x&gt;=0) (x)$ 另一种泛化形势为参数ReLU 另一种ELU Maxout: $\\max(w_1^Tx+b_1, w_2^Tx + b_2)$ ReLU和Leaky ReLU是其特例； 关于网络规模大小 一般浅层神经网络训练过程中容易陷入局部极小值，很难训练； 所以及时过拟合，也尽量不采用过小的神经网络（可以采用其它手段处理过拟合问题） 数据预处理 均值处理（中心化）：X -= np.mean(X, axis = 0) 归一化：X /= np.std(X, axis = 0) 使用归一化需要注意： 只有在不同变量的大小对于输出同等重要的情况下采用归一化；图像中由于所有变量都已经在区间[0,255]范围之中，所以没有必要采用归一化 PCA和白化 1234567891011# Assume input data matrix X of size [N x D]X -= np.mean(X, axis = 0) # zero-center the data (important)cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrixU，S, V = np.linalg.svd(cov)Xrot = np.dot(X, U) # decorrelate the data# 1. PCAXrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]#2 . whiten the data:# divide by the eigenvalues (which are square roots of the singular values)Xwhite = Xrot / np.sqrt(S + 1e-5) U为正交特征向量，我们可以只选择top N重要的特征向量来进行降维处理； PCA： 下图所示，数据中心对齐，以特征向量方向旋转对齐； 白化操作：如果输入数据为多元高斯分布，白化结果为高斯零均值独立协方差矩阵 以CIFAR-10为例，处理之后效果见下图，第2张为取3072个特征中前144个特征： 所有预处理过程仅作用到训练数据中，并记录相关参数，验证数据和测试数据采用训练数据的结果进行预处理 参数初始化 W = 0.01* np.random.randn(D,H) w = np.random.randn(n) / sqrt(2.0/n) randn 产生零均值单位方差高斯分布w参数不能全部初始化为零（对称效应导致网络激活不更新），bias参数一般初始化为零第二个公式为了解决输出随n的增加而比例增加的问题 \\begin{align} \\text{Var}(s) &= \\text{Var}(\\sum_i^n w_ix_i) \\\\ &= \\sum_i^n \\text{Var}(w_ix_i) \\\\ &= \\sum_i^n [E(w_i)]^2\\text{Var}(x_i) + E[(x_i)]^2\\text{Var}(w_i) + \\text{Var}(x_i)\\text{Var}(w_i) \\\\ &= \\sum_i^n \\text{Var}(x_i)\\text{Var}(w_i) \\\\ &= \\left( n \\text{Var}(w) \\right) \\text{Var}(x) \\end{align}正则化 L2正则化对于峰值权重增加比较大的惩罚，对平滑的权重矩阵更加友好； L1正则化的一个效果是使得输入参数稀疏化，从而过滤输入中的噪声数据； 最大基准门限：$\\Vert \\vec{w} \\Vert_2 &lt; c$ Dropout: 在预测阶段不要使用 inverted dropout 一般 $p=0.5$ 12345678910111213141516171819202122232425\"\"\" Inverted Dropout: Recommended implementation example.We drop and scale at train time and don't do anything at test time.\"\"\"p = 0.5 # probability of keeping a unit active. higher = less dropoutdef train_step(X): # forward pass for example 3-layer neural network H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = (np.random.rand(*H1.shape) &lt; p) / p # first dropout mask. Notice /p! H1 *= U1 # drop! H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = (np.random.rand(*H2.shape) &lt; p) / p # second dropout mask. Notice /p! H2 *= U2 # drop! out = np.dot(W3, H2) + b3 # backward pass: compute gradients... (not shown) # perform parameter update... (not shown) def predict(X): # ensembled forward pass H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary H2 = np.maximum(0, np.dot(W2, H1) + b2) out = np.dot(W3, H2) + b3 代价函数（data loss） 分类问题 SVM代价函数(合页损失函数) L_i = \\sum_{j\\neq y_i} \\max(0, f_j - f_{y_i} + 1) 交叉熵损失函数 L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) 当分类数目特别大时，可采用层次Softmax 多属性分类 如果一个分类目标对应多个正确的结果，比如对一张图片分类，可能同时有多个正确标签。可以对每个属性构建一个二元分类器： L_i = \\sum_j \\max(0, 1 - y_{ij} f_j)​ 其中，j为某个分类，$y_{ij}$ 为+1或-1，表示第i个样本是否含有第j个属性；$f_i$ 为正代表预测正确； ​ 第二种方法是对每个属性应用逻辑回归分类器： L_i = \\sum_j y_{ij} \\log(\\sigma(f_j)) + (1 - y_{ij}) \\log(1 - \\sigma(f_j))​ $\\partial{Li} / \\partial{f_j} = y{ij} - \\sigma(f_j)$。 回归问题 L2: $L_i = \\Vert f - y_i \\Vert_2^2$ ​ L1: $L_i = \\Vert f - y_i \\Vert_1 = \\sum_j \\mid f_j - (y_i)_j \\mid$ ​ 训练3梯度校验 \\frac{df(x)}{dx} = \\frac{f(x + h) - f(x - h)}{2h} \\hspace{0.1in} h可以设置为1e-5 可以使用如下值来判断校验结果： \\frac{\\mid f'_a - f'_n \\mid}{\\max(\\mid f'_a \\mid, \\mid f'_n \\mid)} relative error &gt; 1e-2 usually means the gradient is probably wrong 1e-2 &gt; relative error &gt; 1e-4 should make you feel uncomfortable 1e-4 &gt; relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high. 1e-7 and less you should be happy. 网络层数越多，相对误差值越大 进行梯度校验，关闭dropout、数据放大(或使用random seed) Sanity Checks Loss函数的初始化输出值是否合理； 增加正则化参数，loss是否增加 使用小数据集测试是否能够达到数据过拟合（零Loss值） 注意关闭正则化 训练过程监测 代价函数 训练/验证集准确率 更新参数比例 A rough heuristic is that this ratio should be somewhere around 1e-3. If it is lower than this then the learning rate might be too low. If it is higher then the learning rate is likely too high. ​ 123456# assume parameter vector W and its gradient vector dWparam_scale = np.linalg.norm(W.ravel())update = -learning_rate*dW # simple SGD updateupdate_scale = np.linalg.norm(update.ravel())W += update # the actual updateprint update_scale / param_scale # want ~1e-3 超参优化 learning_rate = 10 ** uniform(-6, 1) random serach &gt; grid search 模型组合 Same model, different initializations. Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization. Top models discovered during cross-validation. Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesn’t require additional retraining of models after cross-validation Different checkpoints of a single model. If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap. Running average of parameters during training. Related to the last point, a cheap way of almost always getting an extra percent or two of performance is to maintain a second copy of the network’s weights in memory that maintains an exponentially decaying sum of previous weights during training. This way you’re averaging the state of the network over last several iterations. You will find that this “smoothed” version of the weights over last few steps almost always achieves better validation error. The rough intuition to have in mind is that the objective is bowl-shaped and your network is jumping around the mode, so the average has a higher chance of being somewhere nearer the mode. 反向传播 Cache forward pass variables. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them. Gradients add up at forks. The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add. 实现 &lt;代码&gt; 配合Assignment 1： Implement a Neural Network整理一个从头训练2层全联通网络的例子。 网络结构（只有一个隐层的全联同神经网络）,其中输入训练样本为X (N x D), 第一个隐层由H个节点组成，输出层由C个节点组成： 12input - fully connected layer - ReLU - fully connected layer - softmax（N,D） H C 各参数的大小如下： input: X (N x D) W1 (H x D) ; b1 (H,) W2 (C x H); b2 (C,) 参数初始化 12345678H = 100C = 10N,D = X.shapeW1 = std * np.random.randn(input_size, hidden_size)b1 = np.zeros(hidden_size)W2 = std * np.random.randn(hidden_size, output_size)b2 = np.zeros(output_size) 前向传播，计算得分函数和代价函数 hiddenLayer = ReLU(X*W1 + b1) \\\\ ReLU(x) = \\max(0, x) scores = hidden\\_layer * W2 + b212hidden_layer = np.maximum(0, np.dot(X, W1) + b1) # N x Hsocres = np.dot(hidden_layer, W2) + b2 # N x C 代价函数使用交叉熵损失函数： L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}123456exp_scores = np.exp(scores)probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)correct_logprobs = -np.log(probs[range(N), y])loss = np.sum(correct_logprobs) / Nloss += 0.5*reg*(np.sum(W1*W1) + np.sum(W2*W2)) 反向传播，计算梯度 梯度的计算利用反向传播，从后往前逐步推导： 首先计算dscores \\begin{equation} \\frac{\\partial L}{\\partial scores} = \\left\\{ \\begin{array}{lr} -1 + \\frac{e^{\\hat f_{y_i}}}{\\sum_j e^{f_j}}, & j=i\\\\ \\frac{e^{\\hat f_{y_i}}}{\\sum_j e^{f_j}}, & j \\neq i \\end{array} \\right. \\end{equation}123dscores = probsdscores[range(N), y] -=1dscores /= N dW2 = H^T*dscores + 2\\lambda*W2 \\\\ db2 = dscores 12dW2 = np.dot(h1.T, dscores) + 2*reg * W2db2 = np.sum(dscores, axis=0) 接着计算隐层的偏导,其中激活函数由于使用ReLU，在$x&lt;0$时，导数为0 12345dh1 = np.dot(dscores, W2.T)dh1[h1 &lt;= 0] = 0dW1 = np.dot(X.T, dh1) + 2*reg * W1db1 = np.sum(dh1, axis=0) 其它 学习率的设计一般在[1e-3, 1e-5],10**uniform(-3,-6) 参考 1. cs231n notes: Neural Network &#8617; 2. cs231n notes: Setting up the Data and Loss &#8617; 3. cs231n notes: Learning and Evaluation &#8617;","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/"},{"name":"课程笔记","slug":"读书笔记/课程笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/课程笔记/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"},{"name":"笔记","slug":"笔记","permalink":"http://blog.a-stack.com/tags/笔记/"}]},{"title":"cs231n课程笔记:（Lecture 3）线性分类器和优化方法","slug":"cs231n-lecture-3","date":"2018-05-03T08:51:26.000Z","updated":"2018-05-16T13:59:35.315Z","comments":false,"path":"2018/05/03/cs231n-lecture-3/","link":"","permalink":"http://blog.a-stack.com/2018/05/03/cs231n-lecture-3/","excerpt":"摘要： 计划花一个月的时间刷一遍斯坦福的机器视觉课程cs231n，并做笔记记录每天学习到的内容。","text":"摘要： 计划花一个月的时间刷一遍斯坦福的机器视觉课程cs231n，并做笔记记录每天学习到的内容。 概述cs231n是斯坦福在深度学习和机器视觉领域的入门经典课程，相关资源如下： 课程主页： http://cs231n.stanford.edu/ 课程Notes：http://cs231n.github.io/ Event Type Date Description Course Materials Lecture 3 Tuesday April 10 Loss Functions and Optimization Linear classification IIHigher-level representations, image featuresOptimization, stochastic gradient descent [slides] [linear classification notes][optimization notes] 线性分类器 2 Interpretation of linear classifiers as template matching. Another interpretation for the weights WW is that each row of WW corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that “fits” best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class (although we will learn it, and it does not necessarily have to be one of the images in the training set), and we use the (negative) inner product as the distance instead of the L1 or L2 distance. 线性分类器本质上可以理解为一种模板匹配算法，匹配与分类模板最相近的模板，但由于线性分类器的简单性质，导致每个类最终只能学习到一个模板，如果目标类别有较大的差异，就需要把这些差异平均化。（这也是神经网络层次特性的优越性） 代价函数多分类SVM代价函数5SVM的目标函数定义： \\begin{equation} \\min_{ {w},\\xi_n}\\frac{1}{2} {w}^T {w}+C\\sum_{n=1}^N\\xi_n \\\\ s.t. {w}^T {x}_ny_n\\ge1-\\xi_n \\ \\forall n \\\\ \\xi_n \\ge 0 \\ \\forall n \\end{equation}为计算方便，其中偏移量已经被包含在权重之中。 上述问题的优化问题对等如下问题(L1-SVM)： \\min_{\\bold{w}}\\frac{1}{2}\\bold{w}^T\\bold{w}+C\\sum_{n=1}^N \\max(1-\\bold{w}^T \\bold{x_n}y_n, 0)和L2-SVM： \\min_{\\bold{w}}\\frac{1}{2}\\bold{w}^T\\bold{w}+C\\sum_{n=1}^N \\max(1-\\bold{w}^T \\bold{x_n}y_n, 0)^2对于第i个样本，多分类SVM代价函数定义为： L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta) 合页损失函数（hinge loss），一般我们令$\\Delta =1$ 其中$s_{y_i}$ 为预测值 $j \\neq y_i$ 的意思是只有错误的分类才叠加损失，正确的分类不对损失产生影响 带正则化的损失函数 L = \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2 Note that biases do not have the same effect since, unlike the weights, they do not control the strength of influence of an input dimension. Therefore, it is common to only regularize the weights $W$ but not the biases $b$. $\\Delta=1.0$ 即可，由于权重可以比例调节，所以实际上$\\Delta$ 等于1和100意义是一样的； 与SVM线性分类器关系 SVM线性分类器是多分类的一个特例， L_i = C \\max(0, 1 - y_i w^Tx_i) + R(W)[引申阅读]SVM4 单独摘成一篇博文吧，要想彻底搞清楚SVM内容实在是有点多 Softmax 分类器在Softmax中使用交叉熵(cross-entropy loss)损失函数， L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}其中函数$f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$ 为Softmax函数,给出了分类结果的概率分布描述。 梯度计算公式： \\begin{equation} dW= \\left\\{ \\begin{array}{lr} (-1 + \\frac{e^{\\hat f_{y_i}}}{\\sum_j e^{f_j}})x_i, & j=i\\\\ \\frac{e^{\\hat f_{y_i}}}{\\sum_j e^{f_j}}x_i, & j \\neq i \\end{array} \\right. \\end{equation}向量化实现： 1234567891011121314151617score = X.dot(W) # N x CN = X.shape[0]correct_score = score[np.arange(N),y].reshape(N,1)exp_sum = np.sum(np.exp(score),axis=1).reshape(N,1)loss += np.sum(np.log(exp_sum) - correct_score)loss /= Nloss += reg * 0.5 * np.sum(W*W)margin = np.exp(score)/exp_summargin[np.arange(N),y] -= 1dW = X.T.dot(margin)dW /=NdW += reg*W 交叉熵函数： 真实分类的分布$p$ 和估计分布$q$ 的交叉熵定义为： H(p,q) = - \\sum_x p(x) \\log q(x) Softmax 最小化交叉熵或最大化极大似然函数来求解问题；SVM通过寻找最大界面距离来优化目标。 优化算法3 梯度数值解（梯度校验） \\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h}$[f(x+h) - f(x-h)] / 2 h$ 是一种更好的实现。 123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): \"\"\" a naive implementation of numerical gradient of f at x - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at \"\"\" fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope it.iternext() # step to next dimension return grad 当适应SVM代价函数： L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right]代价函数关于权重的偏导为： \\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) \\right) x_i \\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) x_i 参考 2. CS231n Notes:linear classify &#8617; 3. CS231n Notes:Optimization &#8617; 4. CS229 Lecture Notes: SVM &#8617; 5. Deep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax. &#8617;","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/"},{"name":"课程笔记","slug":"读书笔记/课程笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/课程笔记/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"},{"name":"笔记","slug":"笔记","permalink":"http://blog.a-stack.com/tags/笔记/"}]},{"title":"cs231n课程笔记:（Lecture 2）图像分类","slug":"cs231n-lecture-2","date":"2018-05-03T08:50:26.000Z","updated":"2018-05-16T13:59:22.970Z","comments":true,"path":"2018/05/03/cs231n-lecture-2/","link":"","permalink":"http://blog.a-stack.com/2018/05/03/cs231n-lecture-2/","excerpt":"","text":"摘要： 计划花一个月的时间刷一遍斯坦福的机器视觉课程cs231n，并做笔记记录每天学习到的内容。 概述cs231n是斯坦福在深度学习和机器视觉领域的入门经典课程，相关资源如下： 课程主页： http://cs231n.stanford.edu/ 课程Notes：http://cs231n.github.io/ Event Type Date Description Course Materials Lecture 2 Thursday April 5 Image Classification The data-driven approach K-nearest neighbor Linear classification I [slides] [python/numpy tutorial][image classification notes][linear classification notes] Python/Numpy Tutorial Note that unlike many languages, Python does not have unary increment (x++) or decrement (x--) operators. 字符串操作 12345678s = \"hello\"print(s.capitalize()) # Capitalize a string; prints \"Hello\"print(s.upper()) # Convert a string to uppercase; prints \"HELLO\"print(s.rjust(7)) # Right-justify a string, padding with spaces; prints \" hello\"print(s.center(7)) # Center a string, padding with spaces; prints \" hello \"print(s.replace('l', '(ell)')) # Replace all instances of one substring with another; # prints \"he(ell)(ell)o\"print(' world '.strip()) # Strip leading and trailing whitespace; prints \"world\" List操作 123nums = [0, 1, 2, 3, 4]even_squares = [x ** 2 for x in nums if x % 2 == 0]print(even_squares) # Prints \"[0, 4, 16]\" 字典操作 123d = &#123;'person': 2, 'cat': 4, 'spider': 8&#125;for animal, legs in d.items(): print('A %s has %d legs' % (animal, legs)) 12nums = [0, 1, 2, 3, 4]even_num_to_square = &#123;x: x ** 2 for x in nums if x % 2 == 0&#125; Numpy 更多内容参见：NumPy Reference 123456# 数值乘print(x * y)print(np.multiply(x, y))# 矩阵乘print(v.dot(w))print(np.dot(v, w)) 12345x = np.array([[1,2],[3,4]])print(np.sum(x)) # Compute sum of all elements; prints \"10\"print(np.sum(x, axis=0)) # Compute sum of each column; prints \"[4 6]\"print(np.sum(x, axis=1)) # Compute sum of each row; prints \"[3 7]\" 12#转置x.T SciPy 更多内容参见: SciPy Tutorial Image Operations1234567891011121314151617181920# Image Operationsfrom scipy.misc import imread, imsave, imresize# Read an JPEG image into a numpy arrayimg = imread('assets/cat.jpg')print(img.dtype, img.shape) # Prints \"uint8 (400, 248, 3)\"# We can tint the image by scaling each of the color channels# by a different scalar constant. The image has shape (400, 248, 3);# we multiply it by the array [1, 0.95, 0.9] of shape (3,);# numpy broadcasting means that this leaves the red channel unchanged,# and multiplies the green and blue channels by 0.95 and 0.9# respectively.img_tinted = img * [1, 0.95, 0.9]# Resize the tinted image to be 300 by 300 pixels.img_tinted = imresize(img_tinted, (300, 300))# Write the tinted image back to diskimsave('assets/cat_tinted.jpg', img_tinted) Distance between points123456789101112131415161718import numpy as npfrom scipy.spatial.distance import pdist, squareform# Create the following array where each row is a point in 2D space:# [[0 1]# [1 0]# [2 0]]x = np.array([[0, 1], [1, 0], [2, 0]])print(x)# Compute the Euclidean distance between all rows of x.# d[i, j] is the Euclidean distance between x[i, :] and x[j, :],# and d is the following array:# [[ 0. 1.41421356 2.23606798]# [ 1.41421356 0. 1. ]# [ 2.23606798 1. 0. ]]d = squareform(pdist(x, 'euclidean'))print(d) 图像分类 Notes1 The task in Image Classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image. Images are 3-dimensional arrays of integers from 0 to 255, of size Width x Height x 3. The 3 represents the three color channels Red, Green, Blue. 面临的挑战： 不同视角. A single instance of an object can be oriented in many ways with respect to the camera. 尺寸变换. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image). 变形. Many objects of interest are not rigid bodies and can be deformed in extreme ways. 遮挡. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible. 不同光照条件. The effects of illumination are drastic on the pixel level. 背景干扰. The objects of interest may blend into their environment, making them hard to identify. 类内差异. The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance. Semantic Gap: 语义鸿沟，对于人类而言，图像分类十分简答，但对于计算机系统从像素中提取特征特别难。 数据驱动的方式来实现图像分类 KNN Demo 传统特征提取方法很难有质的提升和泛化的性能，尤其直接根据特征进行分类，不如利用大量标记数据训练一个模型，然后利用该模型对未知数据进行预测。 近邻优化算法处理图像分类 在训练阶段，NN什么都不做，知识把训练数据加载到内存中；$O(1)$ 预测阶段，将未知数据与所有训练数据进行比较，取最近距离的图像所在的类别作为预测类别；$O(N)$ 缺点： 计算量大(每预测一个图片都需要与所有训练数据计算一遍距离)，准确率低（~40%准确率在 CIFAR-10）； 图像的三维特征和边缘特性，导致仅从像素力度上很难进行比对，维度越好，NN算法就越无能为力 参考 1. CS231n Notes:Classification &#8617; 2. CS231n Notes:linear classify &#8617; 3. A Few Useful Things to Know About Machine Learning &#8617; 4. CS229 Lecture Notes: SVM &#8617; 5. Deep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax. &#8617;","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/"},{"name":"课程笔记","slug":"读书笔记/课程笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/课程笔记/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"},{"name":"笔记","slug":"笔记","permalink":"http://blog.a-stack.com/tags/笔记/"}]},{"title":"机器学习圣经","slug":"Holy-Bible-of-Machine-Learning","date":"2018-04-30T04:31:21.000Z","updated":"2018-05-15T09:14:33.416Z","comments":true,"path":"2018/04/30/Holy-Bible-of-Machine-Learning/","link":"","permalink":"http://blog.a-stack.com/2018/04/30/Holy-Bible-of-Machine-Learning/","excerpt":"摘要： 机器学习中充满很多难以理解的技巧和方法，姑且利用摄影中无忌77条那样的摄影圣经来称之为机器学习圣经吧。","text":"摘要： 机器学习中充满很多难以理解的技巧和方法，姑且利用摄影中无忌77条那样的摄影圣经来称之为机器学习圣经吧。 @[toc] 这篇文章的内容主要来自Pedro Domingos的论文《A Few Useful Things to Know About Machine Learning》1 ，作为一篇读书笔记。同时Pedro也是畅销书《The Master Algorithm》（中文译为《终极算法:机器学习和人工智能如何重塑世界》）的作者。 机器学习中充满很多难以理解的技巧和方法，Pedro称之为black art，姑且利用摄影中无忌77条那样的摄影圣经来称之为机器学习圣经吧。 机器学习圣经1 《The Master Algorithm》的作者 学习=表示+评价+优化 (Learning = Representation + Evaluation + optimization) 泛化 模型的泛化性能是机器学习算法追求的最终目标。所有设计良好的训练数据、验证数据和测试数据尤为关键。在早期的机器学习系统中，由于学习器学到的模型表示十分有限，所以训练集和测试集的误差不是十分显现。 只有数据还不够 ​除了数据，我们还需要知识（归纳+推理）和假设（一致性、同分布、独立性），来实现模型的泛化特征。 过拟合的多个方面 bias和variance Bias is a learner’s tendency to consistently learn the same wrong thing. Variance is the tendency to learn random things irrespective of the real signal. 维度灾难 随着维度增加，数据之间差异会变小，泛化将指数级的变难。 理论上的保证 归纳与演绎； 边界保证，给定一个足够大的训练集，告诉你在很大的概率上你的学习器会返回一个成功泛化的假设，还是无法找到一个保持正确的假设。 渐进保证是，给定无穷数据，学习器将保证输出正确的分类器。 机器学习中理论保证的主要作用并不是在实践中作为决策的标准，最多是在算法设计时给些提示。 特征工程 特征工程之所以比学习还要复杂，是因为它是和领域相关的，机器学习算法是通用的。 更多的数据绝对比一个更好的算法有效 问题的本质其实是时间、计算资源和数据，哪个是瓶颈的问题。本质的瓶颈是人力！ 没有免费午餐定律告诉我们，只要数据充分，简单的算法也可以实现和复杂算法等效的效果。 模型组合 与尝试N多模型，选择其中最好的相比，组合多个不同的模型（model ensembles）效果往往更好。常见组合有bagging，boosting，stacking。模型组合与贝叶斯模型平均的区别：在贝叶斯模型平均方法中，对新样例的预测是对假设空间中的所有分类器的预测取平均得到的，每个分类器会根据它解释训练数据的能力和我们对它的先验信任度而有不同的权重。模型组合修改了假设空间；贝叶斯模型平均给假设空间添加固定的权重。 Jensen’s Inequality，简森不等式提供了理论依据 简单 $\\neq$ 准确 著名的奥坎姆剃刀（occam’s razor）原理称：若无必要，勿增实体（entities should not be multi-plied beyond necessity）。但并非总是简单的模型可以获得更好的泛化性能，比如模型组合是在增加模型复杂度换取更好的准确率。模型参数的数量和过拟合之间并无直接的联系。奥坎姆最初的意思是说，开始的时候选择简单的假设，可以修正它，直到效果理想。但是不要一开始从复杂的做起，而不是说要找简单作为最终的学习器。 可表示 $\\neq$ 可学习 神经网络的万有逼近原理描述了只需要一个隐层，神经网络可以近似表示所有的函数形式。但一个函数可以被表示并不代表着它可以被很好的学习到。即使假设空间中真实存在这个函数，由于假设空间的范围之大，在有限数据、时间和计算资源的情况下，一般学习算法只能覆盖其中一个子集，在这个子集空间中是否有目标函数，往往跟你采用的表示方法有关。 这告诉我们，采用不同的表示方法，我们可能需要花费不同的代价（使用更少的数据）来学习到目标函数。很多学习器的工作机制正式将简单的基函数进行线性组合，来降低算法复杂度。 相关性 $\\neq$ 因果性 利用机器学习模型进行预测的前提是数据之间的因果性体现而非变量的相关性。​机器学习是基于观测数据，而观测数据中是否含有可预测变量是不可控的，这点跟基于实验数据的结论不同。因此，如果你能尽量多的获得实验数据，那充分准备足够的实验数据。一般我们认为，数据的相关性是潜在因果性的标志，但因果性是否真的存在没有坚实的依据。 参考 1. A Few Useful Things to Know About Machine Learning &#8617;","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.a-stack.com/categories/机器学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"技巧","slug":"技巧","permalink":"http://blog.a-stack.com/tags/技巧/"},{"name":"机器学习","slug":"机器学习","permalink":"http://blog.a-stack.com/tags/机器学习/"}]},{"title":"Everything-About-SVM","slug":"Everything-About-SVM","date":"2018-04-29T13:13:40.000Z","updated":"2018-05-15T09:14:33.431Z","comments":false,"path":"2018/04/29/Everything-About-SVM/","link":"","permalink":"http://blog.a-stack.com/2018/04/29/Everything-About-SVM/","excerpt":"摘要：","text":"摘要： @[toc] 支持向量机SVM，是一种二类分类模型，是定义在特征空间上的间隔最大化化线性分类器，可形式化为求解凸二次规划问题，也等价于正则化的合页损失函数最小化问题。主要涉及如下关键知识点需要后续进一步整理： 线性可分的支持向量机 线性支持向量机 非线性支持向量机 间隔最大化问题 硬间隔最大化 软间隔最大化 核技巧、核函数、核方法 Hinge Loss 快速学习算法——SMO（最小最优化算法） 多分类问题 感知机利用误分类最小的策略，求得分离超平面，这时解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时解是唯一的。 参考 李航，《最统计学习方法》 CS229 Lecture Notes: SVM 用一张图理解SVM的脉络","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.a-stack.com/categories/机器学习/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"},{"name":"支持向量机","slug":"支持向量机","permalink":"http://blog.a-stack.com/tags/支持向量机/"}]},{"title":"《机器学习》课程实验回顾","slug":"machinelearning-labs","date":"2018-04-26T05:56:28.000Z","updated":"2018-05-15T09:14:33.479Z","comments":false,"path":"2018/04/26/machinelearning-labs/","link":"","permalink":"http://blog.a-stack.com/2018/04/26/machinelearning-labs/","excerpt":"","text":"摘要： Cousera上机器学习课程提供了八个实验作业，使用MATLAB进行动手实验，熟悉相关技术，本文对实验中的关键内容进行总结归纳，方便以后及时查找。 概要 可以使用Matlab Online进行代码测试，地址; 相关代码已经传输至Github： https://github.com/ddebby/machine_learning_cousera.git Lab1: 线性回归1. 简单线性回归在第一个实验中提供了单变量线性回归的数据用来预测快餐车的盈利情况，其中输入为城市中的人口信息，输出为该城市快餐车的盈利情况，数据分布情况详见下图： 代价函数和梯度更新实现计算代价函数 computeCost.m 123456789101112131415161718function J = computeCost(X, y, theta)%COMPUTECOST Compute cost for linear regression% J = COMPUTECOST(X, y, theta) computes the cost of using theta as the% parameter for linear regression to fit the data points in X and y% Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost of a particular choice of theta% You should set J to the cost.J = 1/(2*m) * (X*theta - y)'*(X*theta -y)% =========================================================================end 梯度更新 gradientDescent.m 123456789101112131415161718192021222324252627function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)%GRADIENTDESCENT Performs gradient descent to learn theta% theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by % taking num_iters gradient steps with learning rate alpha% Initialize some useful valuesm = length(y); % number of training examplesJ_history = zeros(num_iters, 1);for iter = 1:num_iters % ====================== YOUR CODE HERE ====================== % Instructions: Perform a single gradient step on the parameter vector % theta. % % Hint: While debugging, it can be useful to print out the values % of the cost function (computeCost) and gradient here. % theta = theta - alpha/m * X'*(X*theta - y); % ============================================================ % Save the cost J in every iteration J_history(iter) = computeCost(X, y, theta);endend 实现并测试： 123456789101112131415161718192021222324252627282930313233data = load('ex1data1.txt');X = data(:, 1); y = data(:, 2);m = length(y); % number of training examplesX = [ones(m, 1), data(:,1)]; % Add a column of ones to xtheta = zeros(2, 1); % initialize fitting parameters% Some gradient descent settingsiterations = 1500;alpha = 0.01;% compute and display initial costJ = computeCost(X, y, theta);% further testing of the cost functionJ = computeCost(X, y, [-1 ; 2]);% run gradient descenttheta = gradientDescent(X, y, theta, alpha, iterations);% Plot the linear fithold on; % keep previous plot visibleplot(X(:,2), X*theta, '-')legend('Training data', 'Linear regression')hold off % don't overlay any more plots on this figure% Predict values for population sizes of 35,000 and 70,000predict1 = [1, 3.5] *theta;fprintf('For population = 35,000, we predict a profit of %f\\n',... predict1*10000);predict2 = [1, 7] * theta;fprintf('For population = 70,000, we predict a profit of %f\\n',... predict2*10000); $J(\\theta)$ 的可视化123456789101112131415161718192021222324252627282930313233%% ============= Part 4: Visualizing J(theta_0, theta_1) =============fprintf('Visualizing J(theta_0, theta_1) ...\\n')% Grid over which we will calculate Jtheta0_vals = linspace(-10, 10, 100);theta1_vals = linspace(-1, 4, 100);% initialize J_vals to a matrix of 0'sJ_vals = zeros(length(theta0_vals), length(theta1_vals));% Fill out J_valsfor i = 1:length(theta0_vals) for j = 1:length(theta1_vals) t = [theta0_vals(i); theta1_vals(j)]; J_vals(i,j) = computeCost(X, y, t); endend% Because of the way meshgrids work in the surf command, we need to% transpose J_vals before calling surf, or else the axes will be flippedJ_vals = J_vals';% Surface plotfigure;surf(theta0_vals, theta1_vals, J_vals)xlabel('\\theta_0'); ylabel('\\theta_1');% Contour plotfigure;% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))xlabel('\\theta_0'); ylabel('\\theta_1');hold on;plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2); 2. 多元线性回归利用多元线性回归预测房价，输入的特征包括房子大小、房间数目，输出为房子价格。 2.1 特征归一化featureNormalize.m 123456789101112131415161718192021function [X_norm, mu, sigma] = featureNormalize(X)%FEATURENORMALIZE Normalizes the features in X % FEATURENORMALIZE(X) returns a normalized version of X where% the mean value of each feature is 0 and the standard deviation% is 1. This is often a good preprocessing step to do when% working with learning algorithms.% You need to set these values correctlyX_norm = X;mu = zeros(1, size(X, 2));sigma = zeros(1, size(X, 2));% ====================== YOUR CODE HERE ======================% mu = mean(X,1);sigma = std(X,1);X_norm = (X - mu)./sigma;% ============================================================end The bsxfun is helpful for applying a function (limited to two arguments) in an element-wise fashion to rows of a matrix using a vector of source values. This is useful for feature normalization. An example you can enter at the octave command line: 1234567891011Z=[1 1 1; 2 2 2;];v=[1 1 1];bsxfun(@minus,Z,v);ans = 0 0 0 1 1 1 In this case, the corresponding elements of v are subtracted from each row of Z. The minus(a,b) function is equivalent to computing (a-b). 代价函数和梯度更新与简单线性回归相同； 2.2 计算 $\\theta$ 的数值解normalEqn.m 12345678910111213141516171819function [theta] = normalEqn(X, y)%NORMALEQN Computes the closed-form solution to linear regression % NORMALEQN(X,y) computes the closed-form solution to linear % regression using the normal equations.theta = zeros(size(X, 2), 1);% ====================== YOUR CODE HERE ======================% Instructions: Complete the code to compute the closed form solution% to linear regression and put the result in theta.%% ---------------------- Sample Solution ----------------------theta = pinv(X'*X)*X'*y;% -------------------------------------------------------------% ============================================================end Lab 2 - Lab 5: 留作后续更新 … Lab 6: SVM本实验利用SVM实现非线性分类器，核函数选择高斯核函数，高斯核函数的定义如下： K_{gaussian}(x^{(i)},x^{(j)}) = exp(-\\frac{||x^{(i)}-x^{(j)}||^2}{2\\sigma^2})1234567891011121314%% ========== Part 5: Training SVM with RBF Kernel (Dataset 2) ==========% After you have implemented the kernel, we can now use it to train the % SVM classifier.% load('ex6data2.mat');% SVM ParametersC = 1; sigma = 0.1;% We set the tolerance and max_passes lower here so that the code will run% faster. However, in practice, you will want to run the training to% convergence.model= svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma)); visualizeBoundary(X, y, model); 由于目前scikit-learn库对各种SVM算法都有比较好的封装，所以也不在进一步深究matlab的实现了。 Lab 7.1: K-Means本实验使用K-Means进行图像压缩，K-Means算法的基本实现： 1234567891011% Initialize centroidscentroids = kMeansInitCentroids(X, K);for iter = 1:iterations % Cluster assignment step: Assign each data point to the % closest centroid. idx(i) corresponds to cˆ(i), the index % of the centroid assigned to example i idx = findClosestCentroids(X, centroids); % Move centroid step: Compute means based on centroid % assignments centroids = computeMeans(X, idx, K);end 在循环中关键实现两个步骤：1）重新划分聚类；2）计算新的均值及中心点 寻找最近的聚类中心点，并聚类 c^{(i)}:=j \\ that \\ minimizes \\ ||x^{(i)} - \\mu_j ||^2 1234567891011121314151617181920212223242526272829function idx = findClosestCentroids(X, centroids)%FINDCLOSESTCENTROIDS computes the centroid memberships for every example% idx = FINDCLOSESTCENTROIDS (X, centroids) returns the closest centroids% in idx for a dataset X where each row is a single example. idx = m x 1 % vector of centroid assignments (i.e. each entry in range [1..K])%% Set KK = size(centroids, 1);% You need to return the following variables correctly.idx = zeros(size(X,1), 1);% Instructions: Go over every example, find its closest centroid, and store% the index inside idx at the appropriate location.% Concretely, idx(i) should contain the index of the centroid% closest to example i. Hence, it should be a value in the % range 1..K%% Note: You can use a for-loop over the examples to compute this.%for i=1:size(X,1) for j = 1: size(centroids,1) dis(j) = sum((centroids(j, :) - X(i, :)) .^ 2, 2); end [t,idx(i)] = min(dis);end end 更新聚类中心点位置 \\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}123456789101112131415161718192021function centroids = computeCentroids(X, idx, K)%COMPUTECENTROIDS returns the new centroids by computing the means of the %data points assigned to each centroid.% centroids = COMPUTECENTROIDS(X, idx, K) returns the new centroids by % computing the means of the data points assigned to each centroid. It is% given a dataset X where each row is a single data point, a vector% idx of centroid assignments (i.e. each entry in range [1..K]) for each% example, and K, the number of centroids. You should return a matrix% centroids, where each row of centroids is the mean of the data points% assigned to it.%% Useful variables[m n] = size(X);% You need to return the following variables correctly.centroids = zeros(K, n);for i = 1:K centroids(i,:) = mean(X(find(idx==i),:));end 3.初始起点的随机选取 1234% Randomly reorder the indices of examples randidx = randperm(size(X, 1)); % Take the first K examples as centroids centroids = X(randidx(1:K), :); 使用K-Means压缩图片 我们将一张24bit的照片，压缩为4-bit（16个色彩）；对于每个像素点，将选择最近的类簇进行颜色表示。通过压缩将128x128x24=393,216bits的图像压缩为16x24 + 128x128x4=65,920bits（其中需要额外每种颜色需要一个24bit空间存储各个颜色字典）。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061%% ============= Part 4: K-Means Clustering on Pixels ===============% In this exercise, you will use K-Means to compress an image. To do this,% you will first run K-Means on the colors of the pixels in the image and% then you will map each pixel onto its closest centroid.%% Load an image of a birdA = double(imread('bird_small.png'));% If imread does not work for you, you can try instead% load ('bird_small.mat');A = A / 255; % Divide by 255 so that all values are in the range 0 - 1% Size of the imageimg_size = size(A);% Reshape the image into an Nx3 matrix where N = number of pixels.% Each row will contain the Red, Green and Blue pixel values% This gives us our dataset matrix X that we will use K-Means on.X = reshape(A, img_size(1) * img_size(2), 3);% Run your K-Means algorithm on this data% You should try different values of K and max_iters hereK = 16; max_iters = 10;% When using K-Means, it is important the initialize the centroids% randomly. % You should complete the code in kMeansInitCentroids.m before proceedinginitial_centroids = kMeansInitCentroids(X, K);% Run K-Means[centroids, idx] = runkMeans(X, initial_centroids, max_iters);%% ================= Part 5: Image Compression ======================% In this part of the exercise, you will use the clusters of K-Means to% compress an image. To do this, we first find the closest clusters for% each example. After that, we % Find closest cluster membersidx = findClosestCentroids(X, centroids);% Essentially, now we have represented the image X as in terms of the% indices in idx. % We can now recover the image from the indices (idx) by mapping each pixel% (specified by its index in idx) to the centroid valueX_recovered = centroids(idx,:);% Reshape the recovered image into proper dimensionsX_recovered = reshape(X_recovered, img_size(1), img_size(2), 3);% Display the original image subplot(1, 2, 1);imagesc(A); title('Original');% Display compressed image side by sidesubplot(1, 2, 2);imagesc(X_recovered)title(sprintf('Compressed, with %d colors.', K)); Lab 7.2： PCA本实验利用PCA实现数据降维及可视化。 数据归一化 计算协方差和奇异值分解 \\Sigma =\\frac{1}{m}X^TX [U, S, V] = svd(\\Sigma) 降维 1Z = X * U(:,1:K); 数据恢复 1X_rec = Z * U(:,1:K)'; 数据可视化的降维显示12345678% Use PCA to project this cloud to 2D for visualization% X (16383,3)% Subtract the mean to use PCA[X_norm, mu, sigma] = featureNormalize(X);% PCA and project the data to 2D[U, S] = pca(X_norm);Z = projectData(X_norm, U, 2); Lab 8.1: 异常检测本实验搭建一个异常检测系统，用来检测服务器的异常信息，输入为每台服务器的每分钟吞吐(mb/s)和响应延时（ms），数据提供了m=307组样本数据。期望通过非监督学习的手段来检测异常。 为了对数据异常进行检测，首先需要将原始数据拟合到一个分布模型里，我们选择使用多元高斯模型进行数据拟合: p(x;\\mu,\\Sigma) = \\dfrac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} exp(-1/2(x-\\mu)^T\\Sigma^{-1}(x-\\mu)) 首先需要对参数$\\mu$ ,$\\sigma$ 进行估计，参见如下代码： 12345mu = mean(X, 1);sigma2 = var(X, 1);p = multivariateGaussian(X, mu, sigma2);% Visualize the fitvisualizeFit(X, mu, sigma2); 拟合高斯分布的轮廓图如下： 多元高斯分布函数multivariateGaussian()的定义如下： 12345678910111213141516171819202122function p = multivariateGaussian(X, mu, Sigma2)%MULTIVARIATEGAUSSIAN Computes the probability density function of the%multivariate gaussian distribution.% p = MULTIVARIATEGAUSSIAN(X, mu, Sigma2) Computes the probability % density function of the examples X under the multivariate gaussian % distribution with parameters mu and Sigma2. If Sigma2 is a matrix, it is% treated as the covariance matrix. If Sigma2 is a vector, it is treated% as the \\sigma^2 values of the variances in each dimension (a diagonal% covariance matrix)%k = length(mu);if (size(Sigma2, 2) == 1) || (size(Sigma2, 1) == 1) Sigma2 = diag(Sigma2);endX = bsxfun(@minus, X, mu(:)');p = (2 * pi) ^ (- k / 2) * det(Sigma2) ^ (-0.5) * ... exp(-0.5 * sum(bsxfun(@times, X * pinv(Sigma2), X), 2));end 阈值$\\epsilon$ 的选择，通过计算每个阈值的F1得分来评价验证集数据，选择最优得分的阈值： 123456789101112131415161718192021222324252627282930313233343536373839function [bestEpsilon bestF1] = selectThreshold(yval, pval)%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting%outliers% [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best% threshold to use for selecting outliers based on the results from a% validation set (pval) and the ground truth (yval).bestEpsilon = 0;bestF1 = 0;F1 = 0;stepsize = (max(pval) - min(pval)) / 1000;for epsilon = min(pval):stepsize:max(pval) % Instructions: Compute the F1 score of choosing epsilon as the % threshold and place the value in F1. The code at the % end of the loop will compare the F1 score for this % choice of epsilon and set it to be the best epsilon if % it is better than the current choice of epsilon. % % Note: You can use predictions = (pval &lt; epsilon) to get a binary vector % of 0's and 1's of the outlier predictions cvPredictions = (pval &lt; epsilon); fp = sum((cvPredictions == 1)&amp; (yval == 0)); tp = sum((cvPredictions == 1)&amp;(yval == 1)); fn = sum((cvPredictions == 0)&amp;(yval == 1)); prec = tp/(tp + fp); rec = tp/(tp + fn); F1 = 2*prec*rec/(prec + rec); if F1 &gt; bestF1 bestF1 = F1; bestEpsilon = epsilon; endendendpval = multivariateGaussian(Xval, mu, sigma2);[epsilon F1] = selectThreshold(yval, pval); 在更复杂场景下的应用，示例提供了个输入为11维特征的数据，对其异常数据进行预测，代码片段如下： 123456789101112131415% Apply the same steps to the larger dataset[mu sigma2] = estimateGaussian(X);% Training set p = multivariateGaussian(X, mu, sigma2);% Cross-validation setpval = multivariateGaussian(Xval, mu, sigma2);% Find the best threshold[epsilon F1] = selectThreshold(yval, pval);fprintf('Best epsilon found using cross-validation: %e\\n', epsilon);fprintf('Best F1 on Cross Validation Set: %f\\n', F1);fprintf('# Outliers found: %d\\n\\n', sum(p &lt; epsilon)); ​ Lab 8.2：推荐系统本实验将实现协同过滤算法，并应用到电影推荐之中。数据集来自943个用户的1682部电影的评分数据，每个评分范围为1-5，可以存在多个未评分数据。 $X$ 为num_movies x num_features，是电影的特征矩阵 $Y$ 为num_movies x num_user矩阵，描述了每个评分值$y^{(i,j)}$ $R$ 为与$Y$ 同维度的二值矩阵，$R(i,j)=1$ 代表用户$j$ 对电影$i$ 进行了评分 代价函数的定义 J(x,\\theta) = \\dfrac{1}{2} \\displaystyle \\sum_{(i,j):r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \\dfrac{\\lambda}{2}\\sum_{i=1}^{n_m} \\sum_{k=1}^{n} (x_k^{(i)})^2 + \\dfrac{\\lambda}{2}\\sum_{j=1}^{n_u} \\sum_{k=1}^{n} (\\theta_k^{(j)})^212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function [J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, ... num_features, lambda)%COFICOSTFUNC Collaborative filtering cost function% [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...% num_features, lambda) returns the cost and gradient for the% collaborative filtering problem.%% Unfold the U and W matrices from paramsX = reshape(params(1:num_movies*num_features), num_movies, num_features);Theta = reshape(params(num_movies*num_features+1:end), ... num_users, num_features); % You need to return the following values correctlyJ = 0;X_grad = zeros(size(X));Theta_grad = zeros(size(Theta));% Instructions: Compute the cost function and gradient for collaborative% filtering. %% Notes: X - num_movies x num_features matrix of movie features% Theta - num_users x num_features matrix of user features% Y - num_movies x num_users matrix of user ratings of movies% R - num_movies x num_users matrix, where R(i, j) = 1 if the % i-th movie was rated by the j-th user%% You should set the following variables correctly:%% X_grad - num_movies x num_features matrix, containing the % partial derivatives w.r.t. to each element of X% Theta_grad - num_users x num_features matrix, containing the % partial derivatives w.r.t. to each element of Theta%J = 1/2 * sum(sum(((X * Theta' - Y).*R).^2));X_grad = ((X * Theta' - Y).*R) * Theta;Theta_grad = ((X*Theta' -Y).*R)' * X;% Regular versionJ = J + lambda/2 * sum(sum(Theta .^2)) + lambda/2 * sum(sum(X.^2));X_grad = X_grad + lambda * X;Theta_grad = Theta_grad + lambda * Theta;grad = [X_grad(:); Theta_grad(:)];end% Evaluate cost functionJ = cofiCostFunc([X(:) ; Theta(:)], Y, R, num_users, num_movies, num_features, 1.5); 发起预测，在数据库中加入自己的数据，随便对一部分电影进行打分： 1234567891011121314151617181920212223242526272829%% ============== Part 6: Entering ratings for a new user ===============% Before we will train the collaborative filtering model, we will first% add ratings that correspond to a new user that we just observed. This% part of the code will also allow you to put in your own ratings for the% movies in our dataset!%movieList = loadMovieList();% Initialize my ratingsmy_ratings = zeros(1682, 1);% Check the file movie_idx.txt for id of each movie in our dataset% For example, Toy Story (1995) has ID 1, so to rate it \"4\", you can setmy_ratings(1) = 4;% Or suppose did not enjoy Silence of the Lambs (1991), you can setmy_ratings(98) = 2;% We have selected a few movies we liked / did not like and the ratings we% gave are as follows:my_ratings(7) = 3;my_ratings(12)= 5;my_ratings(54) = 4;my_ratings(64)= 5;my_ratings(66)= 3;my_ratings(69) = 5;my_ratings(183) = 4;my_ratings(226) = 5;my_ratings(355)= 5; 我们拥有了一份新的用户对电影打分的数据： 123456789101112New user ratings:Rated 4 for Toy Story (1995)Rated 3 for Twelve Monkeys (1995)Rated 5 for Usual Suspects, The (1995)Rated 4 for Outbreak (1995)Rated 5 for Shawshank Redemption, The (1994)Rated 3 for While You Were Sleeping (1995)Rated 5 for Forrest Gump (1994)Rated 2 for Silence of the Lambs, The (1991)Rated 4 for Alien (1979)Rated 5 for Die Hard 2 (1990)Rated 5 for Sphere (1998) 训练算法 1234567891011121314151617181920212223242526272829303132333435363738394041424344%% ================== Part 7: Learning Movie Ratings ====================% Now, you will train the collaborative filtering model on a movie rating % dataset of 1682 movies and 943 users%% Load dataload('ex8_movies.mat');% Add our own ratings to the data matrixY = [my_ratings Y];R = [(my_ratings ~= 0) R];% Normalize Ratings[Ynorm, Ymean] = normalizeRatings(Y, R);% Useful Valuesnum_users = size(Y, 2);num_movies = size(Y, 1);num_features = 10;% Set Initial Parameters (Theta, X)X = randn(num_movies, num_features);Theta = randn(num_users, num_features);initial_parameters = [X(:); Theta(:)];% Set options for fmincgoptions = optimset('GradObj', 'on', 'MaxIter', 100);% Set Regularizationlambda = 10;theta = fmincg (@(t)(cofiCostFunc(t, Ynorm, R, num_users, num_movies, ... num_features, lambda)), ... initial_parameters, options);% Unfold the returned theta back into U and WX = reshape(theta(1:num_movies*num_features), num_movies, num_features);Theta = reshape(theta(num_movies*num_features+1:end), ... num_users, num_features);fprintf('Recommender system learning completed.\\n');fprintf('\\nProgram paused. Press enter to continue.\\n');pause; 其中，函数normalizeRatings 用于对数据进行归一化，减去均值；函数fmincg为MATLAB自带的优化函数； 利用算法进行推荐 1234567891011121314151617%% ================== Part 8: Recommendation for you ====================% After training the model, you can now make recommendations by computing% the predictions matrix.%p = X * Theta';my_predictions = p(:,1) + Ymean;movieList = loadMovieList();[r, ix] = sort(my_predictions, 'descend');fprintf('\\nTop recommendations for you:\\n');for i=1:10 j = ix(i); fprintf('Predicting rating %.1f for movie %s\\n', my_predictions(j), ... movieList&#123;j&#125;);end 输出结果： 1234567891011Top recommendations for you:Predicting rating 5.0 for movie Someone Else's America (1995)Predicting rating 5.0 for movie Aiqing wansui (1994)Predicting rating 5.0 for movie Great Day in Harlem, A (1994)Predicting rating 5.0 for movie Prefontaine (1997)Predicting rating 5.0 for movie They Made Me a Criminal (1939)Predicting rating 5.0 for movie Entertaining Angels: The Dorothy Day Story (1996)Predicting rating 5.0 for movie Saint of Fort Washington, The (1993)Predicting rating 5.0 for movie Santa with Muscles (1996)Predicting rating 5.0 for movie Star Kid (1997)Predicting rating 5.0 for movie Marlene Dietrich: Shadow and Light (1996) 参考 ​","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.a-stack.com/tags/机器学习/"},{"name":"公开课","slug":"公开课","permalink":"http://blog.a-stack.com/tags/公开课/"},{"name":"实验","slug":"实验","permalink":"http://blog.a-stack.com/tags/实验/"},{"name":"线性回归","slug":"线性回归","permalink":"http://blog.a-stack.com/tags/线性回归/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://blog.a-stack.com/tags/逻辑回归/"}]},{"title":"Machine Learning Notes","slug":"machinelearning-notes","date":"2018-04-26T05:56:13.000Z","updated":"2018-05-15T09:14:33.448Z","comments":false,"path":"2018/04/26/machinelearning-notes/","link":"","permalink":"http://blog.a-stack.com/2018/04/26/machinelearning-notes/","excerpt":"","text":"摘要： 最近花了两周时间刷完了吴恩达在Cousera上关于机器学习的经典课程, 这已经是近两个月来刷完的第十个Cousera课程了。虽然课程是多年前开设的，但相关机器学习理论和方法内容介绍仍然具备很强的时效性，是机器学习入门的必选课程。 @[toc] 最近花了两周时间刷完了吴恩达在Cousera上关于机器学习的经典课程, 这已经是近两个月来刷完的第十个Cousera课程了。虽然课程是多年前开设的，但相关机器学习理论和方法内容介绍仍然具备很强的时效性，是机器学习入门的必选课程。为了巩固对课程内容和课后实验的理解，将在未来一周实践写几篇总结性笔记，作为对课程内容的回顾和总结。 Lecture 1： 机器学习的定义 Tom Mitchell provides a modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” 监督学习的一个主要特点，是在拿到数据的那一刻，我们已经知道正确的输出目标范围； 监督学习可以分为分类问题和回归问题； 鸡尾酒会问题及鸡尾酒会算法：Wiki, 属于非聚类的非监督问题 Lecture 2： 线性回归（多属性）变量定义$x_j^{(i)}$ : 第$i^{th}$训练样本的第$j$个特征向量； $x^{(i)}$ : 第$i^{th}$训练样本的特征向量； $m$ ： 训练样本数目； $n$ ： 特征数目 多元线性规划模型假设函数 \\begin{equation} h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n = \\theta ^T x \\tag{1.a} \\end{equation} 以上表达式中对$\\theta$ 和 $x$ 进行了维度变换，$x_0=1,for(i\\in1,…,m)$ $\\theta .shape = (n+1, 1)$ $x.shape = (m,n+1)$ 代价函数 J(\\theta) = \\dfrac {1}{2m} \\displaystyle \\sum_{i=1}^m \\left (h_\\theta (x^{(i)}) - y^{(i)} \\right)^2 = \\dfrac {1}{2m} (X\\theta - \\vec{y})^{T} (X\\theta - \\vec{y})梯度 \\theta := \\theta - \\frac{\\alpha}{m} X^{T} (X\\theta - \\vec{y})特征正则化 Feature scaling 和 mean normalization实现; 归一化的目的，是为了收敛域对其，更容易收敛，梯度下降更容易找到最佳方向； 吴恩达打了个很好的比喻，两个参数范围不一致，梯度下降是一个椭圆； x_i := \\dfrac{x_i - \\mu_i}{s_i}参数分析 \\theta = (X^T X)^{-1}X^T y Gradient Descent Normal Equation Need to choose alpha No need to choose alpha Needs many iterations No need to iterate $O(kn^2)$ $O(n^3)$ ,计算复杂度来自$X^TX$ 的计算 Works well when n is large Slow if n is very large Lecture 3：逻辑回归 逻辑回归名称的由来：逻辑回归来源于Logistic 分布，虽然名字带回归，但解决的是分类问题。而其密度函数曲线正对应着Sigmoid函数的曲线形状。 假设函数 \\begin{align*} h_\\theta (x) = g ( \\theta^T x ) \\\\ z = \\theta^T x \\\\g(z) = \\dfrac{1}{1 + e^{-z}}\\end{align*}假设函数输出的分类目标的条件概率： \\begin{align*}& h_\\theta(x) = P(y=1 | x ; \\theta) = 1 - P(y=0 | x ; \\theta) \\newline& P(y = 0 | x;\\theta) + P(y = 1 | x ; \\theta) = 1\\end{align*}代价函数 \\begin{align*}& J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_\\theta(x^{(i)}),y^{(i)})=-\\frac{1}{m}(y^Tlog(h)+(1-y)^T(1-h)) \\newline & \\mathrm{Cost}(h_\\theta(x),y) = -\\log(h_\\theta(x)) \\; & \\text{if y = 1} \\newline & \\mathrm{Cost}(h_\\theta(x),y) = -\\log(1-h_\\theta(x)) \\; & \\text{if y = 0}\\end{align*}梯度 \\theta := \\theta - \\frac{\\alpha}{m} X^{T} (g(X \\theta ) - \\vec{y}) \\sigma(x)'=\\sigma(x)(1 - \\sigma(x)) 一些梯度下降算法之外的选择： 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：共轭梯度（Conjugate Gradient），局部优化法(Broyden fletcher goldfarb shann,BFGS)和有限内存局部优化法(LBFGS)fminunc是 matlab和octave 中都带的一个最小值优化函数，使用时我们需要提供代价函数和每个参数的求导 多分类： One-vs-all每个分类目标对应一个分类器，计算分类过程中，取预测值最大的分类器输出作为最终分类结果： prediction = \\max_i( h_\\theta ^{(i)}(x) )正则化正则化的线性回归 \\begin{align*} & \\text{Repeat}\\ \\lbrace \\newline & \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline & \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] &\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2...n\\rbrace\\newline & \\rbrace \\end{align*}添加正则化的数值解 \\begin{align*}& \\theta = \\left( X^TX + \\lambda \\cdot L \\right)^{-1} X^Ty \\newline& \\text{where}\\ \\ L = \\begin{bmatrix} 0 & & & & \\newline & 1 & & & \\newline & & 1 & & \\newline & & & \\ddots & \\newline & & & & 1 \\newline\\end{bmatrix}\\end{align*}正则化的逻辑回归 J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\large[ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)}))\\large] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2 \\begin{array}& \\text{Repeat}\\ \\lbrace \\newline& \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline& \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] &\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2...n\\rbrace\\newline& \\rbrace\\end{array}Lecture 4-5: 神经网络 大脑学习的原理举例： 人类失去眼睛，可以学习通过超声来判断物体和方位，具备看的能力； If networkhas $s_j$ units in layer j and $s_j +1$ units in layer j+1, then Θ(j) willbe of dimension sj+1×(sj+1). 多分类 \\begin{align*}\\begin{bmatrix}x_0 \\newline x_1 \\newline x_2 \\newline\\cdots \\newline x_n\\end{bmatrix} \\rightarrow\\begin{bmatrix}a_0^{(2)} \\newline a_1^{(2)} \\newline a_2^{(2)} \\newline\\cdots\\end{bmatrix} \\rightarrow\\begin{bmatrix}a_0^{(3)} \\newline a_1^{(3)} \\newline a_2^{(3)} \\newline\\cdots\\end{bmatrix} \\rightarrow \\cdots \\rightarrow\\begin{bmatrix}h_\\Theta(x)_1 \\newline h_\\Theta(x)_2 \\newline h_\\Theta(x)_3 \\newline h_\\Theta(x)_4 \\newline\\end{bmatrix} \\rightarrow\\end{align*}代价函数 \\begin{gather*}\\large J(\\Theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[y^{(i)}_k \\log ((h_\\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\\log (1 - (h_\\Theta(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} ( \\Theta_{j,i}^{(l)})^2\\end{gather*}反向传播 参见： Backpropagation-in-Neural-Network 梯度校验 \\dfrac{\\partial}{\\partial\\Theta_j}J(\\Theta) \\approx \\dfrac{J(\\Theta_1, \\dots, \\Theta_j + \\epsilon, \\dots, \\Theta_n) - J(\\Theta_1, \\dots, \\Theta_j - \\epsilon, \\dots, \\Theta_n)}{2\\epsilon}Lecture 6：采用机器学习的建议bias 与 variance 高偏差==欠拟合 高方差==过拟合 正则化与偏差/方差： 大的$\\lambda$ :高偏差（欠拟合）； 小的$\\lambda$ :高方差（过拟合）； 选择合适的正则化参数 学习曲线（Learning Curves） 错误率随样本数目变化的曲线； 如果算法过拟合，增大样本数有助于提升算法性能 Lecture 7： SVM z = \\theta^Tx \\text{cost}_0(z) = \\max(0, k(1+z)) \\text{cost}_1(z) = \\max(0, k(1-z)) 更多内容查看WiKi： Hingle loss 代价函数 J(\\theta) = C\\sum_{i=1}^m y^{(i)} \\ \\text{cost}_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) \\ \\text{cost}_0(\\theta^Tx^{(i)}) + \\dfrac{1}{2}\\sum_{j=1}^n \\Theta^2_j C = \\frac{1}{\\lambda} 因为SVM的优化目标为最大化边界举例，所有SVM也叫做Large Margin Classifier 核函数实现SVM非线性分类的基础，将低维空间点投射到高维空间，定义近似函数： 高斯核函数(主要参数为 $\\sigma$) f_i = similarity(x, l^{(i)}) = \\exp(-\\dfrac{\\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\\sigma^2})参数 $C=\\frac{1}{\\lambda}$ 如果$C$ 过大，高方差、低偏差； 如果$C$ 过小，高偏差、低方差； $\\sigma^2$ 如果$\\sigma^2$ 过大，特征更加平缓，导致高偏差，低方差； 如果$\\sigma^2$ 过小，特征分布更集中，导致高方差，低偏差； Lecture 8: K-Means 和 PCAK-Means 算法实现(Cluster Assignment+Move Centroid)： 123456Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)Repeat: for i = 1 to m: c(i):= index (from 1 to K) of cluster centroid closest to x(i) for k = 1 to K: mu(k):= average (mean) of points assigned to cluster k c^{(i)} = argmin_k\\ ||x^{(i)} - \\mu_k||^2优化目标 J(c^{(i)},\\dots,c^{(m)},\\mu_1,\\dots,\\mu_K) = \\dfrac{1}{m}\\sum_{i=1}^m ||x^{(i)} - \\mu_{c^{(i)}}||^2注意事项 K个初始对象的选择应为随机选取k个训练样本； 为了尽量避免陷入局部极小值，需要进行多次随机初始化求解，选择代价函数最小的解； K的选择，可以采用elbow method： Choose K at the point where the cost function starts to flatten out. PCA降维的目的： 1. 数据压缩；2. 可视化； The goal of PCA is to reduce the average of all the distances of every feature to the projection line. PCA与线性回归不同之处在于，计算的各点到分割线的最短距离； 算法流程 数据预处理：归一化； 计算协方差矩阵 \\Sigma = \\dfrac{1}{m}\\sum^m_{i=1}(x^{(i)})(x^{(i)})^T 计算$\\Sigma$ 的特征矩阵 1[U,S,V] = svd(Sigma); 取$U$ 的前k列计算$z$ z^{(i)} = Ureduce^T \\cdot x^{(i)} 恢复 x_{approx}^{(1)} = U_{reduce} \\cdot z^{(1)}k值的选择 \\dfrac{\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)}||^2} \\leq 0.01 描述了丢失特征的比例 可以通过SVD分解之后的S值来做同样的事情： \\dfrac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} \\geq 0.99Lecture 9： 应用1-异常检测 x(i)= features of user i’s activities Model p(x) from the data. Identify unusual users by checking which have p(x)&lt;ϵ. 高斯分布 $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$ p(x;\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{(2\\pi)}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} p(x) = p(x_1;\\mu_1,\\sigma_1^2)p(x_2;\\mu_2,\\sigma^2_2)\\cdots p(x_n;\\mu_n,\\sigma^2_n)= \\displaystyle \\prod^n_{j=1} p(x_j;\\mu_j,\\sigma_j^2)算法流程 计算均值和方差 计算概率分布函数 p(x) = \\displaystyle \\prod^n_{j=1} p(x_j;\\mu_j,\\sigma_j^2) = \\prod\\limits^n_{j=1} \\dfrac{1}{\\sqrt{2\\pi}\\sigma_j}exp(-\\dfrac{(x_j - \\mu_j)^2}{2\\sigma^2_j}) 利用标记数据确定ϵ 数据预处理为了满足高斯分布，需要对原始数据进行转换： log(x) log(x+1) log(x+c) for some constant $\\sqrt x$ $x^{1/3}$ 多元高斯分布 p(x;\\mu,\\Sigma) = \\dfrac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} exp(-1/2(x-\\mu)^T\\Sigma^{-1}(x-\\mu)) 传统高斯模型乘积是多元多元高斯的一种特例 $\\Sigma$ 为协方差矩阵，如何假设$A$ 为非奇异矩阵，进行特征变换：$y=Ax$,则$\\Sigma = AA^T$ 多元高斯分布 vs 传统高斯模型 多元 更能捕获特征之间的关联性； 多元 计算更耗时； 多元的前提，样本数 &gt; 特征数；m&gt;10n Lecture 10: 应用2-推荐系统问题抽象以电影推荐为例，定义如下变量： $n_\\mu$ = 用户数目 $n_m$ = 电影数目 $r(i,j)=1$ = 1,如果用户j对电影i评分 $y(i,j)=rating_score$ 基于内容的推荐 min_{\\theta^{(1)},\\dots,\\theta^{(n_u)}} = \\dfrac{1}{2}\\displaystyle \\sum_{j=1}^{n_u} \\sum_{i:r(i,j)=1} ((\\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \\dfrac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n(\\theta_k^{(j)})^2协同过滤 既学习参数又学习特征表示 J(x,\\theta) = \\dfrac{1}{2} \\displaystyle \\sum_{(i,j):r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \\dfrac{\\lambda}{2}\\sum_{i=1}^{n_m} \\sum_{k=1}^{n} (x_k^{(i)})^2 + \\dfrac{\\lambda}{2}\\sum_{j=1}^{n_u} \\sum_{k=1}^{n} (\\theta_k^{(j)})^2参考 SVM：http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf ​","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.a-stack.com/categories/机器学习/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"机器学习","slug":"机器学习","permalink":"http://blog.a-stack.com/tags/机器学习/"},{"name":"线性回归","slug":"线性回归","permalink":"http://blog.a-stack.com/tags/线性回归/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://blog.a-stack.com/tags/逻辑回归/"},{"name":"笔记","slug":"笔记","permalink":"http://blog.a-stack.com/tags/笔记/"},{"name":"神经网络","slug":"神经网络","permalink":"http://blog.a-stack.com/tags/神经网络/"}]},{"title":"机器学习三要素：模型,策略与算法","slug":"机器学习三要素-模型-策略与算法","date":"2018-04-21T15:12:04.000Z","updated":"2018-05-15T09:14:33.495Z","comments":false,"path":"2018/04/21/机器学习三要素-模型-策略与算法/","link":"","permalink":"http://blog.a-stack.com/2018/04/21/机器学习三要素-模型-策略与算法/","excerpt":"","text":"摘要： 李航在《统计学习方法》中将统计学习方法的三要素——模型、策略、算法总结为机器学习方法的提纲挈领。这篇博文主要总结归纳《统计学习方法》中对于这三要素的阐述。 @[toc] 最近在读李航的《统计学习方法》，作者从统计学习的方法来分析机器学习技术，分析很细致深刻，值得反思，未来将逐步总结相关内容，期待更多共鸣与思考。 什么是统计机器学习统计学习又叫统计机器学习，是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。 统计学习的对象是具备一定统计规律的数据，同时对数据有一个强假设：数据是独立同分布产生的。学习的模型属于某个函数的集合，称为假设空间(hypothesis space)，应用某个评价准则(evaluation criterion) 从假设空间中选取一个最优的模型，使它对已知训练数据和未知测试数据在给定评价准则下有最优的预测结果，这个模型的选取过程由算法实现。这样统计学习的方法由模型的假设空间、模型选择的准则以及模型的学习算法组成，称为统计机器学习的三要素。 概念定义 输入空间： 输入变量$X={x_1, x_2, …, x_N}$ 构成的空间； 输出空间：对于输入变量的真实输出$Y={y_1, y_2,…,y_N}$ 构成的空间； 特征空间：每个输入变量都可以由一组特征向量表示，这些特征向量构成了特征空间； 模型实际上都是定义在特征空间上的； 假设空间： 包含所有可能的映射关系集合 $F={ f|y=f_{\\theta}(X), \\theta \\in R^n}$ 输入到输出的映射的集合，表示为条件概率分布$\\hat P(X|Y)$ 或决策函数 $Y = \\hat f(X)$ 参数空间： 构成假设空间每个映射函数的参数所形成的空间； 监督学习的联合概率分布： 监督学习假设输入与输出的随机变量$X$ 和 $Y$ 遵循联合概率分布 $P(X,Y)$ , 机器学习的前提是假设该概率分布存在，但具体定义是未知的（如果知道的话也没必要通过机器学习来获取的了，直接可以通过条件函数来预测未知结果）。训练数据和测试数据被看作是依据联合概率分布$P(X,Y)$ 独立同分布产生的。这是机器学习数据具备一定统计规律的基本假设。 模型如下图所示，在机器学习中，模型描述的是一组从输入到输出的映射关系的函数表示。 在监督学习中，模型就是从假设空间中所要学习条件概率分布$\\hat P(X|Y)$ 或决策函数 $Y = \\hat f(X)$ 。 策略策略描述了学习的目标函数，通过什么样的准则使学习的方向最终逼近最终的优化目标从而能够从假设空间中选取最优的模型。一般采用损失函数或代价函数来对模型的好坏进行度量。损失函数是$f(X)$ 和 $Y$ 的非负实值函数，记作 $L(Y, f(X))$ 。 损失函数数值越小，模型越好，损失函数的期望为： R_{exp} (f) = E_p [L(Y,f(x))] = \\int_{x\\times y} L(y,f(x))P(x,y)dxdy这是理论上模型$f(X)$ 关于联合分布的平均意义下的损失，称为风险函数或期望损失。由于$P(X,Y)$ 未知，所以期望损失无法直接计算。因此引入另一个概念：经验风险（损失）： R_{emp} (f) = \\frac{1}{N} \\sum_{i=1}^NL(y_i,f(x_i))经验风险是模型关于训练样本的平均损失。根据大数定理，当样本容量N趋于无穷时，经验风险趋于期望风险。所以可以用经验风险估计期望风险，但由于现实世界中训练样本数目有限，所以需要对经验风险进行一定的矫正(如正则化)。 当样本容量很小时，经验风险最小化学习会产生过拟合现象，于是引入修正方案：结构风险最小化（SRM）： R_{emp} (f) = \\frac{1}{N} \\sum_{i=1}^NL(y_i,f(x_i)) + \\lambda J(f)结构风险是在经验风险基础上加上表示模型复杂度的正则化项（惩罚项）$J(f)$ ，它描述了模型的复杂度。 过拟合：如果在假设空间中存在“真”模型，过拟合可以理解为所选模型复杂度由于比真模型更高，包含了更多的参数，对训练数据预测比”真”模型更好，但对新数据的预测很差。（训练数据中掺杂着噪声，所以即使”真”模型也无法一定完全拟合训练数据）。 过拟合问题告诉我们，对模型好坏的评价不是完全靠在训练数据上的经验风险或结构风险来决定的，而是学习方法的泛化能力，这是期望损失本身要表达的意思。如果学到一个模型$\\hat f$,那么用这个模型对未知数据预测的误差即为泛化误差： R_{exp} (\\hat f) = E_p [L(Y,\\hat f(x))] = \\int_{x\\times y} L(y,\\hat f(x))P(x,y)dxdy泛化误差具备如下性质： 它是样本容量的函数，随着样本容量增加，泛化误差上界趋于0； 它是假设空间容量的函数，假设空间越大，模型就越难学，泛化误差上界就越大。 算法算法将机器学习问题转换为最优化问题进行求解。设计目标是寻找全局最优解并使得求解过程足够高效。 参考 ​《统计学习方法》","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.a-stack.com/categories/机器学习/"}],"tags":[{"name":"模型","slug":"模型","permalink":"http://blog.a-stack.com/tags/模型/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"机器学习","slug":"机器学习","permalink":"http://blog.a-stack.com/tags/机器学习/"},{"name":"策略","slug":"策略","permalink":"http://blog.a-stack.com/tags/策略/"}]},{"title":"Backpropagation in Neural Network","slug":"Backpropagation-in-Neural-Network","date":"2018-04-20T01:09:14.000Z","updated":"2018-07-07T05:54:29.862Z","comments":false,"path":"2018/04/20/Backpropagation-in-Neural-Network/","link":"","permalink":"http://blog.a-stack.com/2018/04/20/Backpropagation-in-Neural-Network/","excerpt":"","text":"摘要： 反向传播毋庸置疑是整个神经网络的精髓，正是由于它的提出标志着深度神经网络的训练在有限算力基础上成为可能，但反向传播本身的原理同样值得品读和思考。 本文主要总结神经网络中反向传播算法的推导流程并挖掘一些深层次的原理。反向传播算法在1970年就已经提出，直到1986年 David Rumelhart, Geoffrey Hinton, and Ronald Williams在一篇论文中对其实现的分析才得以普及。 1. 表达式的定义 $w_{jk}^l$ 代表第l−1层第k个神经元，与第l层第j个神经元之间的权重（注意j与k的顺序）； $b_j^l$ 代表第l层中第j个神经元的偏移； $a_j^l$ 代表第l层中第j个神经元的激活函数值； $L$ 代表神经网络的总层数； $J(W,b)$ 简写为$J$ 代表神经网络的代价函数； 假设我们有$m$ 个训练样本${(x^{(1)},y^{(1)}),…, (x^{(m)},y^{(m)})}$ ,对于每个训练样本$(x,y)$ 定义代价函数为： J(W,b;x,y) = \\frac{1}{2} ||h_{W,b}(x) - y||^2对于$m$个训练样本，总的代价函数为： J(W,b) = [\\frac{1}{m} \\sum_{i=1}^m J(W,b;x^{(i)},y^{(i)}] + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l -1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2根据神经网络层与层之间关系的定义，我们有如下表达式（向量形式）： z^l = w^l a^{l-1} + b^l, l=2,3,...,L a^l = \\sigma (z^l), l=2,3,...,L a^1=z^1=X其中 z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l利用梯度下降法进行目标优化使用的主要梯度更新公式： w^l := w^l - \\alpha \\frac{\\partial J}{\\partial w^l} b^l := b^l - \\alpha \\frac{\\partial J}{\\partial b^l}因为神经网络的复杂性导致$\\frac{\\partial J}{\\partial w^l}$ 无法直接计算或者计算代价太大（每个权重的计算都需要进行一次前向传播），反向传播的目的在于提供一种更加高效的手段，完成上述梯度的更新操作。 2. 反向传播的直观理解反向传播是用于理解改变网络中的任一权重如何影响网络代价函数的过程。假设神经网络中某个权重$w_{jk}^l$ 产生了轻微的扰动误差 $\\Delta w_{jk}^l$ ,扰动误差导致该神经元的输出产生误差 $\\Delta z_j^l$ ,这个扰动将使对应的神经元$a_j^l$ 产生一个扰动误差 $\\Delta a_j^l$ ，该误差将逐步向后层传递，直至达到输出层，并最终影响代价函数，生成一个代价误差 $\\Delta J = \\frac{\\partial J}{\\partial z_j^l}\\Delta z_j^l$ 。我们可以给出如下式子来为通过误差近似计算梯度提供方向： \\Delta J \\approx \\frac{\\partial J}{\\partial a^L_m} \\frac{\\partial a^L_m}{\\partial a^{L-1}_n} \\frac{\\partial a^{L-1}_n}{\\partial a^{L-2}_p} \\ldots \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\Delta w^l_{jk} \\frac{\\Delta J}{\\Delta z_{j}^l}=\\delta_j^l=\\frac{\\partial J}{\\partial z_j^l}其中，$\\delta_j^l$ 可以定义为$l$层第$j$个神经元上的误差。 我们先从计算最后一层误差$\\delta_j^L$ 开始， \\delta^L_j = \\frac{\\partial J}{\\partial a^L_j} \\sigma'(z^L_j)公式E1（向量形式）： \\delta^L = \\nabla_a J \\odot \\sigma'(z^L)证明： \\delta^L_j =\\frac{\\partial J}{\\partial z_j^L}=\\frac{\\partial J}{\\partial a_j^L}.\\frac{\\partial a_j^L}{\\partial z_j^L}= \\frac{\\partial J}{\\partial a^L_j} \\sigma'(z^L_j).现在把问题转变为如何利用反向传播由后往前逐步计算误差$\\delta_j^l$ ，为计算不同层之间误差之间的关系，利用链式法则给出如下推理过程： \\delta^l =\\frac{\\partial J}{\\partial z^l} = \\frac{\\partial J}{\\partial z^{l+1}}.\\frac{\\partial z^{l+1}}{\\partial z^l}=\\delta^{l+1}.(\\frac{\\partial z^{l+1}}{\\partial a^l}.\\frac{\\partial a^l}{\\partial z^l})= ((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l)即公式E2： \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l)有了以上误差的反向传播计算方法，我们可以利用计算的误差计算权重的梯度如下： 公式E3： \\frac{\\partial J}{\\partial b^l_j} =\\delta^l_j公式E4： \\frac{\\partial J}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j其中公式（4）也可以写成： \\frac{\\partial J}{\\partial w} = a_{\\rm in} \\delta_{\\rm out}证明： \\frac{\\partial J}{\\partial b^l_j} =\\frac{\\partial J}{\\partial z^l_j}.\\frac{\\partial z^l_j}{\\partial b^l_j}= \\delta^l_j \\frac{\\partial J}{\\partial w^l_{jk}} = \\frac{\\partial J}{\\partial z^l_{jk}}.\\frac{\\partial z_{jk}^l}{\\partial w^l_{jk}}=a^{l-1}_k \\delta^l_j另一种求解方式 \\dfrac{\\partial J}{\\partial w_{ij}^l} =\\frac{\\partial J}{\\partial a_j^l}.\\frac{\\partial a_j^l}{\\partial z_j^l}.\\frac{\\partial z_j^l}{w_{ij}^l}=\\frac{\\partial J}{\\partial a_j^l}.\\sigma^{'}(z_j^l)a_i^{l-1}如何我们令$\\delta_j^l =\\frac{\\partial J}{\\partial a_j^l}$ 也可以按照上述过程类似的方法推到出相关公式 3. 反向传播算法的计算流程 输入$x$，令$a^1=z^1=x$; 前向传播： 对于每层$l=2,3,…,L$计算$z^l$和$a^l$； 根据公式E1计算输出误差：$\\delta^L = \\nabla_a J \\odot \\sigma’(z^L)$ ; 反向传播：对于每层$l=L-1,L-2,…,2$ 利用公式E2计算误差：$\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma’(z^l)$ ； 利用公式E3和E4计算参数梯度； 更新权重 参考 CS231n讲义：Backpropagation, Intuitions Neural Networks and Deep Learning Machine Learning Cousera Course","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"基础知识","slug":"深度学习/基础知识","permalink":"http://blog.a-stack.com/categories/深度学习/基础知识/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"}]},{"title":"使用TensorFlow Object Detection API识别仪表表盘","slug":"TensorFlow-Object-Detection-API","date":"2018-04-14T13:16:41.000Z","updated":"2018-05-23T12:42:43.515Z","comments":false,"path":"2018/04/14/TensorFlow-Object-Detection-API/","link":"","permalink":"http://blog.a-stack.com/2018/04/14/TensorFlow-Object-Detection-API/","excerpt":"","text":"前面用到了Tensorflow的物体识别API做了一个检测仪表表盘的实践，记录实践过程中的技巧。 1. 概述本文主要介绍如何使用Tensorflow 物体识别API应用自己业务场景进行物体识别。将结合从事的一些实际经验，分享一个仪表表盘识别的案例。我们在一个利用机器视觉技术自动识别仪表表读数的项目中，需要首先识别各种不同类型表盘的显示屏位置。本案例分析将针对面板识别中采用的关键技术进行分析，详细阐述如何利用物体识别技术和已训练好的模型快速实现使用用户数据设计一个面向特定物体识别的深度神经网络。 目标： 从给定的水表图片中将关键的数字面板给扣取出来。 如下图所示，如果采用通用OCR技术对水表图片面板进行检测，将同时提取很多特征项，对实际的检测值造成比较大的干扰。 1.1 模型及算法选型在方案设计初期我们分别使用公有云服务、现有的成熟OCR软件、开源的OCR方案对目标对象进行了初步识别及分析。通过实测，现有方案无法满足我们的任务需求。为此希望能够利用深度学习在物体识别领域的成熟方案，构建一个面向水表图片面板识别的神经网络模型。 计算框架： Tensorflow (v1.4) 使用接口： Tensorflow Object Detection API1 算法模型： 根据预训练采用的数据集不同，可用的模型列表如下： COCO dataset Model name Speed (ms) COCO mAP2 Outputs ssd_mobilenet_v1_coco 30 21 Boxes ssd_inception_v2_coco 42 24 Boxes faster_rcnn_inception_v2_coco 58 28 Boxes faster_rcnn_resnet50_coco 89 30 Boxes faster_rcnn_resnet50_lowproposals_coco 64 Boxes rfcn_resnet101_coco 92 30 Boxes faster_rcnn_resnet101_coco 106 32 Boxes faster_rcnn_resnet101_lowproposals_coco 82 Boxes faster_rcnn_inception_resnet_v2_atrous_coco 620 37 Boxes faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco 241 Boxes faster_rcnn_nas 1833 43 Boxes faster_rcnn_nas_lowproposals_coco 540 Boxes Kitti dataset Model name Speed (ms) Pascal mAP@0.5 (ms) Outputs faster_rcnn_resnet101_kitti 79 87 Boxes Open Images dataset Model name Speed (ms) Open Images mAP@0.52 Outputs faster_rcnn_inception_resnet_v2_atrous_oid 727 37 Boxes faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid 347 Boxes 几种模型主要在精度和速度方面进行了取舍，如果需要一个高精度的模型可以选择faster R-CNN，如果希望速度快比如实时检测，则可以选择SSD模型。鉴于本方案中设计检测目标特征将为简单，可采用最轻量级的MobileSSD进行优化。 2. 实现流程2.1 数据准备数据准备的环节是除了训练过程之外最为耗时的环节，准备数据的质量和数量将直接决定了训练模型的好坏。图片数据最好在光线、角度、清晰度等方面能最大化的泛化实际的业务场景。由于在这个案例中我们只需要输出一个分类对象，而且输入对象被限定在水表图片上，所以整体涉及需要提取的特征参数空间不是很大，少量经过处理好的明显可供辨识的水表图片即可。目前可用水表图片攻击229张，我们采用80%用于训练，20%用于测试的方式进行划分。 如果分类较多，数据有限，可以选择从互联网上下载或者在开源数据集中获得所需的数据。 另外需要注意图片的大小，图片太大一方面影响训练过程的处理时间，另外大量图片载入内存将很容导致内存溢出，所以如果图像特征粒度不是特别精细可以采用低分辨率图片进行分析。 数据标记对于物体识别而言，数据标记过程是一个相对复杂的过程，目前除了人工标记没有太好的自动或半监督手段，幸好针对图片的标记已经有了几款很好用的工具： LabelImg 这是一个可以直接在图片上做注释框自动生成标记信息的软件，注释信息将被保存为PASCAL VOC 格式的XML文件（ImageNet的文件格式） FIAT (Fast Image Data Annotation Tool) 该工具生成csv格式的注释文件 ImageMagick 图片预处理工具 Use ImageMagick® to create, edit, compose, or convert bitmap images. It can read and write images in a variety of formats (over 200) including PNG, JPEG, GIF, HEIC, TIFF, DPX, EXR, WebP, Postscript, PDF, and SVG. Use ImageMagick to resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses and Bézier curves. 在本方案中我们使用工具LabelImage将229张水表图片进行了标注，同时生成了PASCAL格式的XML文件，名字为000001.jpg的图片注释格式如下： 1234567891011121314151617181920212223242526&lt;annotation&gt; &lt;folder&gt;imgs&lt;/folder&gt; &lt;filename&gt;000001.jpg&lt;/filename&gt; &lt;source&gt; &lt;database&gt;VOC&lt;/database&gt; &lt;annotation&gt;PASCAL VOC&lt;/annotation&gt; &lt;/source&gt; &lt;size&gt; &lt;width&gt;2448&lt;/width&gt; &lt;height&gt;3264&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; &lt;object&gt; &lt;name&gt;panel&lt;/name&gt; &lt;pose&gt;Frontal&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;739&lt;/xmin&gt; &lt;ymin&gt;430&lt;/ymin&gt; &lt;xmax&gt;1475&lt;/xmax&gt; &lt;ymax&gt;796&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt;&lt;/annotation&gt; 我们在根目录新建一个annotations的文件夹存储229张图片的XML描述文件，同时根目录images文件夹用于存储所有的训练数据和测试数据。 数据描述格式 在TensorFlow 物体检测API中使用 TFRecord file format格式对图像标记信息进行描述，所以我们无论采取下面的那种标记方式，最终需要生成TFRcord格式的文件格式。 TFRecord格式要去如下： For every example in your dataset, you should have the following information: An RGB image for the dataset encoded as jpeg or png. A list of bounding boxes for the image. Each bounding box should contain: A bounding box coordinates (with origin in top left corner) defined by 4floating point numbers [ymin, xmin, ymax, xmax]. Note that we store thenormalized coordinates (x / width, y / height) in the TFRecord dataset. The class of the object in the bounding box. TensorFlow针对主流的物体识别类数据集格式提供了转换工具，包括 PASCAL VOC dataset ， Oxford Pet dataset等； PASCAL VOC 1234567891011# From tensorflow/models/research/wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tartar -xvf VOCtrainval_11-May-2012.tarpython object_detection/dataset_tools/create_pascal_tf_record.py \\ --label_map_path=object_detection/data/pascal_label_map.pbtxt \\ --data_dir=VOCdevkit --year=VOC2012 --set=train \\ --output_path=pascal_train.recordpython object_detection/dataset_tools/create_pascal_tf_record.py \\ --label_map_path=object_detection/data/pascal_label_map.pbtxt \\ --data_dir=VOCdevkit --year=VOC2012 --set=val \\ --output_path=pascal_val.record Oxford-IIIT Pet 123456789# From tensorflow/models/research/wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gzwget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gztar -xvf annotations.tar.gztar -xvf images.tar.gzpython object_detection/dataset_tools/create_pet_tf_record.py \\ --label_map_path=object_detection/data/pet_label_map.pbtxt \\ --data_dir=`pwd` \\ --output_dir=`pwd` 如果你使用了自己的格式，可以参考TensorFlow官方文档完成格式转换； 在本方案中我们采用自己处理的方式来进行格式转换，使用文件xml_to_csv.py将所有图片的PASCAL格式文件转化为一个csv文件，然后进一步的利用TensorFLow工具generate_tfrecord.py生成TFRecord格式文件： 执行python xml_to_csv.py,该文件将在根目录的annotations的文件夹下所有的*xml文件，并生成screen_labels.csv文件； 使用如下代码随机生成训练数据和测试数据： 123456789101112import numpy as npimport pandas as pdnp.random.seed(1)full_labels = pd.read_csv('screen_labels.csv')gb = full_labels.groupby('filename')grouped_list = [gb.get_group(x) for x in gb.groups]train_index = np.random.choice(len(grouped_list), size=180, replace=False)test_index = np.setdiff1d(list(range(229)), train_index)train = pd.concat([grouped_list[i] for i in train_index])test = pd.concat([grouped_list[i] for i in test_index])train.to_csv('train_labels.csv', index=None)test.to_csv('test_labels.csv', index=None) 分别执行脚本将训练数据和测试数据转换为TFRecord： 1234567# From tensorflow/models/research/protoc object_detection/protos/*.proto --python_out=.# Create train data: python generate_tfrecord.py --csv_input=data/train_labels.csv --output_path=train.record # Create test data: python generate_tfrecord.py --csv_input=data/test_labels.csv --output_path=test.record 将train.record和test.record的文件存储至根目录的data文件夹下 至此数据准备基本结束，我们创建了如下目录结构 1234567891011121314151617181920212223.├── annotations│ ├── 000001.xml ...│ └── 000229.xml├── data│ ├── screen_labels.csv│ ├── test_labels.csv│ ├── test.record│ ├── train_labels.csv│ └── train.record├── generate_tfrecord.py├── images│ ├── 000001.jpg ...│ └── 000229.jpg├── __init__.py├── README.md├── split labels.ipynb├── test_generate_tfrecord.py├── test_xml_to_csv.py└── xml_to_csv.py​ 2.2 模型配置/迁移学习 完全从头训练一个用于物体检测的模型即使采用大量GPU资源至少也需要数周时间，为了加速模型初期迭代过程，我们选择了一个已经在COCO数据集针对其他多种物体识别场景预训练好的模型，通过重复使用该模型的多数参数来快速生成我们的模型。更多技术内容可以参照迁移学习的技术实现。 由于没有足够的资源从头训练一个模型，我们将采用迁移学习技术，利用一个已经训练好的模型进行迁移学习及训练。 从测试角度考虑，本测试方案选择了体积最小，速度最快的用于嵌入式设备的SSD模型：ssd_mobilenet_v1_coco 下载模型包，可以得到如下文件： 1234567891011.├── object-detection.pbtxt ├── ssd_mobilenet_v1_pets.config├── checkpoint├── frozen_inference_graph.pb├── model.ckpt.data-00000-of-00001├── model.ckpt.index├── model.ckpt.meta└── saved_model ├── saved_model.pb └── variables a graph proto (graph.pbtxt) a checkpoint (model.ckpt.data-00000-of-00001, model.ckpt.index, model.ckpt.meta) a frozen graph proto with weights baked into the graph as constants (frozen_inference_graph.pb) to be used for out of the box inference a config file (pipeline.config) which was used to generate the graph. These directly correspond to a config file in the samples/configs) directory but often with a modified score threshold. 我们下载并在根目录解压模型包ssd_mobilenet_v1_coco，同时创建一个training文件，存储如下文件： 123training/├── object-detection.pbtxt└── ssd_mobilenet_v1_pets.config 其中 object-detection.pbtxt是我们模型所有分类的标签，如下所示，如果有多个分类id从1开始递增，同时给每个标签一个唯一的名称 12345678item &#123; id: 1 name: 'panel'&#125;item&#123; id: 2 name: '其他分类'&#125; 配置物体识别训练流程文件 更多内容参考：https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md Tensorflow Object Detection API 使用 protobuf 文件来配置训练和检测的流程。通过配置Training Pipleline的参数配置可以决定训练参数的选择，我们将尽量多的利用已经训练好的参数进行训练。 一个配置文件由5部分组成： 123456789101112131415161718model &#123;(... Add model config here...)&#125;train_config : &#123;(... Add train_config here...)&#125;train_input_reader: &#123;(... Add train_input configuration here...)&#125;eval_config: &#123;&#125;eval_input_reader: &#123;(... Add eval_input configuration here...)&#125; The model configuration. This defines what type of model will be trained (ie. meta-architecture, feature extractor). The train_config, which decides what parameters should be used to train model parameters (ie. SGD parameters, input preprocessing and feature extractor initialization values). The eval_config, which determines what set of metrics will be reported for evaluation (currently we only support the PASCAL VOC metrics). The train_input_config, which defines what dataset the model should be trained on. The eval_input_config, which defines what dataset the model will be evaluated on. Typically this should be different than the training input dataset. ssd_mobilenet_v1_pets.config为模型配置文件，我们在样例(详见：object_detection/samples/configs 文件夹)上进行如下修改： 1234567891011121314151617181920212223242526 ssd &#123; num_classes: 1 #修改类别为实际类别值 box_coder &#123; faster_rcnn_box_coder &#123; y_scale: 10.0 x_scale: 10.0 height_scale: 5.0 width_scale: 5.0 &#125; &#125;fine_tune_checkpoint: \"ssd_mobilenet_v1_coco_2017_11_17/model.ckpt\" #指向模型文件中的checkpoint文件train_input_reader: &#123; tf_record_input_reader &#123; input_path: \"data/train.record\" #修改为上一个步骤生成的训练record路径 &#125; label_map_path: \"data/object-detection.pbtxt\" #修改为pbtxt文件路径，描述类别标签&#125;eval_input_reader: &#123; tf_record_input_reader &#123; input_path: \"data/test.record\" #修改为上一个步骤生成的测试数据record路径 &#125; label_map_path: \"data/object-detection.pbtxt\" #修改为pbtxt文件路径，描述类别标签 shuffle: false num_readers: 1&#125; train_config provides two fields to specify pre-existing checkpoints: fine_tune_checkpoint and from_detection_checkpoint. fine_tune_checkpoint should provide a path to the pre-existing checkpoint (ie:”/usr/home/username/checkpoint/model.ckpt-#####”). from_detection_checkpoint is a boolean value. If false, it assumes the checkpoint was from an object classification checkpoint. Note that starting from a detection checkpoint will usually result in a faster training job than a classification checkpoint. The list of provided checkpoints can be found here. 2.3 训练Tensorflow Object Detection API 安装 从GitHub下载Tensorflow Object Detection API 1git clone https://github.com/tensorflow/models.git 配置基本环境 12345678910Tensorflow Object Detection API depends on the following libraries:- Protobuf 2.6- Pillow 1.0- lxml- tf Slim (which is included in the \"tensorflow/models/research/\" checkout)- Tensorflowsudo apt-get install protobuf-compiler python-pil python-lxml或者sudo pip install pillowsudo pip install lxml 3.Protobuf 编译 Tensorflow通过Google的Protobufs来配置和训练模型，所以在开始使用之前需要对protobuf相关库进行编译。 12# From tensorflow/models/research/protoc object_detection/protos/*.proto --python_out=. Add Libraries to PYTHONPATH[重要] 在路径 tensorflow/models/research/ 下添加PYTHONPATH路径，实现全局引用 12# From tensorflow/models/research/export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim Note: This command needs to run from every new terminal you start. If you wish to avoid running this manually, you can add it as a new line to the end of your ~/.bashrc file. 启动训练过程 将在数据准备和模型配置阶段的文件复制到tensorflow object_detect文件夹下/models/research/object_detection,包括data/文件夹，image/文件夹，training/文件夹，ssd_mobilenet_v1_coco_2017_11_17/原始模型文件夹 执行如下代码启动训练过程： 1234567# From the tensorflow/models/research/ directorypython object_detection/train.py \\ --logtostderr \\ --pipeline_config_path=$&#123;PATH_TO_YOUR_PIPELINE_CONFIG&#125; \\ --train_dir=$&#123;PATH_TO_TRAIN_DIR&#125;-------------------------------------------------------------------------------------------- python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config 使用TensorBoard 跟踪训练过程训练过程会持续几个小时到十几个小时，可以通过tensorboard查看训练的情况 使用一台Azure的CPU虚拟机进行训练~4s进行一次迭代，正常模型有比较不错结果迭代次数大概在10K以上。 1tensorboard --logdir=/training 2.4 导出模型模型训练过程中，会每隔一段时间生成一个checkpoint，一个checkpoint至少包括三个文件： model.ckpt-${CHECKPOINT_NUMBER}.data-00000-of-00001 model.ckpt-${CHECKPOINT_NUMBER}.index model.ckpt-${CHECKPOINT_NUMBER}.meta 可以通过如下命令从checkpoints中提取模型： 123456789101112131415161718Example Usage:--------------python export_inference_graph \\ --input_type image_tensor \\ --pipeline_config_path path/to/ssd_inception_v2.config \\ --trained_checkpoint_prefix path/to/model.ckpt \\ --output_directory path/to/exported_model_directory -----The expected output would be in the directorypath/to/exported_model_directory (which is created if it does not exist)with contents: - graph.pbtxt - model.ckpt.data-00000-of-00001 - model.ckpt.info - model.ckpt.meta - frozen_inference_graph.pb + saved_model (a directory) 123456789101112131415# From tensorflow/models/research/export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slimpython export_inference_graph.py \\ --input_type image_tensor \\ --pipeline_config_path training/ssd_mobilenet_v1_pets.config \\ --trained_checkpoint_prefix training/model.ckpt-10116 \\ --output_directory water_meter_panel----------WARNING:tensorflow:From /home/gaoc/data/test/object_detect/models/research/object_detection/exporter.py:357: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.Instructions for updating:Please switch to tf.train.get_or_create_global_step2018-01-20 06:34:08.551446: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA2018-01-20 06:34:14.056881: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count &gt;= 8): 0Converted 199 variables to const ops. 如果Exportor.py文件报bug，请参考https://github.com/tensorflow/models/issues/2861 修复 运行以上代码，将在water_meter_panel文件夹下生成模型所需的相关文件： 1234567891011121314151617gaoc@DataScience:~/data/test/object_detect/models/research/object_detection$ ls -l water_meter_panel/total 44820-rw-r--r-- 1 gaoc root 77 Jan 20 06:34 checkpoint-rw-r--r-- 1 gaoc root 22636802 Jan 20 06:34 frozen_inference_graph.pb-rw-r--r-- 1 gaoc root 22174240 Jan 20 06:34 model.ckpt.data-00000-of-00001-rw-r--r-- 1 gaoc root 8873 Jan 20 06:34 model.ckpt.index-rw-r--r-- 1 gaoc root 1058139 Jan 20 06:34 model.ckpt.metadrwxr-xr-x 3 gaoc root 4096 Jan 20 06:34 saved_modelwater_meter_panel/├── checkpoint├── frozen_inference_graph.pb├── model.ckpt.data-00000-of-00001├── model.ckpt.index├── model.ckpt.meta└── saved_model ├── saved_model.pb └── variables 2.5 测试123456# From the tensorflow/models/research/ directorypython object_detection/eval.py \\ --logtostderr \\ --pipeline_config_path=$&#123;PATH_TO_YOUR_PIPELINE_CONFIG&#125; \\ --checkpoint_dir=$&#123;PATH_TO_TRAIN_DIR&#125; \\ --eval_dir=$&#123;PATH_TO_EVAL_DIR&#125; 物体识别领域的算法性能评价指标多数选择AP和mAP（mean average precision），多个类别物体检测中，每一个类别都可以根据recall和precision绘制一条曲线，AP就是该曲线下的面积，mAP是多个类别AP的平均值 12345678Average Precision (AP):AP% AP at IoU=.50:.05:.95 (primary challenge metric) APIoU=.50% AP at IoU=.50 (PASCAL VOC metric) APIoU=.75% AP at IoU=.75 (strict metric) AP Across Scales:APsmall% AP for small objects: area &lt; 322 APmedium% AP for medium objects: 322 &lt; area &lt; 962 APlarge% AP for large objects: area &gt; 962 Average Recall (AR):ARmax=1% AR given 1 detection per image ARmax=10% AR given 10 detections per image ARmax=100% AR given 100 detections per image AR Across Scales:ARsmall% AR for small objects: area &lt; 322 ARmedium% AR for medium objects: 322 &lt; area &lt; 962 ARlarge% AR for large objects: area &gt; 962 从性能测试结果来看该模型已经具备96%的检测精度了，具备投入生产环境所需的性能。 预测结果见下图，准确率达到了99%以上。 3. 结论 对于简单业务场景的物体识别类案例，可以通过迁移学习利用已有的成熟模型快速迭代生成新的特定模型； 这种迁移的另一个优势是对小数据样本具备很好的适应能力，可以解决前期数据不足和数据质量差的问题； 当然采用这种方式也同时存在一定的弊端，比如由于模型预训练参数较多，模型体积较大，模型的选取需要反复测试和优化。 3.1 经验总结 虽然Tensorflow Object Detection API提供了丰富的文档介绍相关工作流程，但由于技术、平台和软件版本本身更新较快，实践中还是或多或少会遇到不少问题，静下心来多翻翻Github的issues里一般都有别人的提问及解答；建议还是先根据文档跑通demo，熟悉相关工具和流程再将框架迁移到自己的数据集之上； 建议自己识别的项目文件单独建立一个数据准备文件夹进行数据准备和相关配置脚本的准备，不要跟Github克隆的Object Detection项目混在一起，不容易管理，也不利于重复利用； TFOD API提供的Tensorflow可视化相当完备，启动训练任务之后，一定要同步启动验证脚本，可以实时跟踪训练进程； 很容易疏忽的一个步骤是关于PYTHON PATH的处理，在执行相关API之前一定要记得EXPORT相关path，可以些一个bash文件，在执行命令的Terminal中source一下；常见错误如下： 1ImportError: No module named ’object_detection’ 充分利用GPU和CPU进行训练： 如果使用CPU训练，控制配置参数中num_examples为一个很小的值（5-10），这样将使用验证数据中的一部分进行验证而不是全部； 配置CUDA_VISIBLE_DEVICES环境变量，选择使用哪个GPU或CPU来分配内存资源： 12$ export CUDA_VISIBLE_DEVICES=\"0\"... 12$ export CUDA_VISIBLE_DEVICES=\"1\"... another scripts 配置为空使用CPU 尽量使用最新版的Tensorflow，在撰写本文时已经是1.7了，当时做实验用的是1.4，复现的时候发现1.4版本已经抛错了… 参考 1. The TensorFlow Object Detection API is an open source framework built on top of TensorFlow that makes it easy to construct, train and deploy object detection models. &#8617; 2. See MSCOCO evaluation protocol. &#8617; Image classification with a pre-trained deep neural network How to train your own Object Detector with TensorFlow’s Object Detector API https://github.com/datitran/raccoon_dataset","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"},{"name":"机器视觉","slug":"动手实践营/机器视觉","permalink":"http://blog.a-stack.com/categories/动手实践营/机器视觉/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"物体识别","slug":"物体识别","permalink":"http://blog.a-stack.com/tags/物体识别/"},{"name":"实践","slug":"实践","permalink":"http://blog.a-stack.com/tags/实践/"},{"name":"工具","slug":"工具","permalink":"http://blog.a-stack.com/tags/工具/"}]},{"title":"物体识别技术之Faster R-CNN","slug":"Object-Detection-Faster-RCNN","date":"2018-04-13T06:45:52.000Z","updated":"2018-05-23T12:43:12.780Z","comments":false,"path":"2018/04/13/Object-Detection-Faster-RCNN/","link":"","permalink":"http://blog.a-stack.com/2018/04/13/Object-Detection-Faster-RCNN/","excerpt":"","text":"摘要: 2015年提出的Faster R-CNN架构在基于机器视觉的物体识别领域占据重要的地位，从R-CNN到fast R-CNN再到faster R-CNN,乃至后续的Mask-R-CNN形成了一条完整的两步识别的物体识别技术生态。 概述物体识别技术一直是机器视觉中业务场景最丰富，关注度最高的一个类别。将花几期来分别对主流的物体识别技术如Faster RCNN，SSD，YOLO，Mask-RCNN进行整理和分析，并利用实践的方式进行强化。 在R-CNN,Fast R-CNN，Faster R-CNN中，物体识别被分为两个步骤实现（与SSD、YOLO的主要差异）：候选区域选择和基于深度网络的物体识别。 传统的物体识别算法传统的物体识别技术采用的滑动窗口+图像金字塔+分类器的算法，可以参见前述博文在基于机器视觉技术的品牌LOGO检测中做的实际测试，原理易于理解，但效率较低，很难达到实时处理的需求： 速度慢，效率低：需要利用滑动窗口遍历图像的不同位置； 受图像畸变影响严重：由于CNN的输入必须是固定大小的图像，所以限制了检测目标的长宽比例，比如这种方法不能同时检测矮胖对象和长瘦对象； 错误率高，没法识别图像的全局特征，每个窗口只能看到局部特征，所以检测精度也受到了比较大的影响。 物体识别精度的衡量指标 IoU(Intersection over Union) IoU = \\frac{Area\\ of\\ Overlap}{Area\\ of\\ Union} mAP(Mean Average Precision) 所有分类的IoU均值； 参考论文 R-CNN： Rich feature hierarchies for accurate object detection and semantic segmentation Faster R-CNN R-CNN 论文： Rich feature hierarchies for accurate object detection and semantic segmentation，2013，Girshick 问题：解决目标检测网络 R-CNN的实现包括如下图所示的4个主要步骤： 接受输入图像； 利用Selective Search算法从图像中抽取大约2000个候选区域； 对每个候选区域利用预训练的CNN进行特征抽取（迁移学习）； 对每个特征抽取区域利用线性SVM进行分类 论文的主要贡献： 使用Selective Search替代了特征金字塔和滑动窗口实现的兴趣区域选择，提升了效率； 利用预训练的神经网络进行特征提取替代了手工特征如HOG的特征提取方法，正是由于CNN学习的特征具备的鲁棒性大大提供了系统的泛化性能 仍然存在的问题： 识别慢，效率低； 不是一个端到端的解决方案 Selective Search算法 论文：Selective Search for Object Recognition，2012，J.R.R.Uijings 之前很多算法都是基于蛮力搜索(Exhaustive Search),对整张图片进行扫描，或者是采用动态窗口的方法，这种方法耗时严重，操作麻烦。J.R.R提出的选择性搜索的方法，在识别前期在整张图片中生成1~3K个proposal的方法，再对每个proposal进行处理。 Selective Search [4], one of the most popular methods, greedily merges superpixels based on engineered low-level features. 缺点：效率低，计算量大，使用1个CPU处理一张图片，需要2s1 Fast R-CNN问题提出：解决端到端训练的问题，提出了Region of Interest（ROI）Pooling 跟R-CNN中使用深度CNN的方式不同，Fast R-CNN中首先将CNN应用到整个图像中进行特征提取，利用一个固定窗口在抽取特征上滑动，分别进行分类预测和回归预测。Fast R-CNN的主要处理流程包括： 输入图像和标定的识别框信息； 利用深度卷积神经网络抽取图像特征； 利用ROI pooling获取ROI特征向量； 利用两个全联通层进行分类和回归预测 端到端的训练过程源于提出了多任务损失函数，将分类问题和回归问题整合在一起，打通了梯度的更新路径，下图描述了Fast R-CNN的训练和测试过程： 缺点：仍然没有摆脱Selective Search算法在推理阶段进行候选区域生成。 Faster R-CNN 论文：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, 2015, Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun 实现：Github上作者提供的Python实现 商业实现：Pinterests2 问题提出：在基于候选区域选择的CNN（region-based CNN）物体识别网络中，候选区域选择的效率成为了整个系统的瓶颈；Faster R-CNN中提出了Region Proposal Netwrok与物体识别网络共享网络参数（替代了Fast R-CNN中的Selective Search算法），降低了候选区域选择的时间代价。 基础网络（Base Network）：特征抽取（迁移学习），抽取的特征将同时应用于RPN和RoIP阶段 RPN：候选区域选择（利用了网络的Attention机制），用于发掘图像中潜在的可能存在物体的区域 RoIP：兴趣区域特征提取 R-CNN：分类预测和候选框回归 使用一块GPU，性能大概在7-10 FPS 基础网络基础网络的主要作用是利用迁移学习完成原始图像的特征抽取，在论文中使用了 在ImageNet预训练的ZF 或 VGG来完成这一任务。当然根据物体识别任务的不同应用场景可以在模型精度和推理时间上进行折中选择 MobileNet, ResNet-152， DenseNet。 文献中，Faster R-CNN的基础网络在使用VGG作为特征提取网络时，使用conv5/conv5_1层的输出特征； 目前ResNet在很多情况下已经替代了VGG16作为特征提取网络； 为了保证网络是全卷积神经网络架构，需要把全连接层剔除，保证可以输入任意维度的输入图像 Anchor Box 替代传统算法的特征金字塔或filter金字塔 一张图像中被识别目标形状大小各异，这也是在原始算法中加入特征金子塔来对原始图像进行多个维度特征变换的原因。 Anchor Box也是为了解决上述问题，我们可以不改变图像的形状，通过改变预测每个区域物体的“窗口”来框出不同大小的物体。首先在原始图像中均匀的选取一些Anchor Box中心点，然后在每个中心点上预制多个Anchor Box。 在Faster R-CNN是使用3个不同形状（1:1, 1:2,2:1）和3个不同大小(128x128,256x256,512x512，按照原图尺寸生成)进行组合共计3x3=9种不同的Anchor box。 使用VGG16做特征提取的情况下，一张输入图片总共可以刻画为512个窗口区域，生成512x(4+2)x9个输出参数。 由于直接预测bouding box难以实现，作者将问题转变为预测预测值与真实值之间的偏移，将问题转变为四个偏移值的预测问题。 Region Proposal Network(RPN)RPN的主要目的是对每个区域是否可能有物体进行打分，基于打分值决定是否进行下一步的分类任务。在基础网络抽取的特征图上使用一个3x3的滑动窗口（512个卷积核），每个滑动窗口的中心点位置为上述Achor Box的中心点区域，在每个滑动窗口区域，将得到两个1x1卷积网络输出，分别为2k的前景/背景预测（该区域是否存在可被预测物体，分类问题）以及4k的位置信息预测（回归问题），四个值分别是 $\\Delta{center{x}}$, $\\Delta{center{y}}$, $\\Delta{width}$, $\\Delta{height}$。 k是Anchor Box的数目 我们将从候选区域中选择打分较高的前N个进行下一轮分析，如果物体打分足够高，下一步将进行非极大抑制和区域选择，如果打分值很低将抛弃这些区域 目标和损失函数The RPN does two different type of predictions: the binary classification and the bounding box regression adjustment. For training, we take all the anchors and put them into two different categories. Those that overlap a ground-truth object with an Intersection over Union (IoU) bigger than 0.5 are considered “foreground” and those that don’t overlap any ground truth object or have less than 0.1 IoU with ground-truth objects are considered “background”. L({p_i}, {t_i}) = \\frac{1}{N_{cls}}\\sum_iL_{cls}(p_i,p_i^*)+\\lambda\\frac{1}{N_{reg}}\\sum_ip_i^*L_{reg}(t_i,t_i^*)其中分类的损失函数为： L_{cls}(p_i,p_i^*) = -log[p_i^*p_i+(1-p_i^*)(1-p_i)]$p_i$为第$i$个参考框是物体的预测概率值，$p_i^*$为实际值，如果anchor是物体的话该值为1，否则为0。 回归损失函数为： L_{reg}(t_i,t_i^*)=R(t_i-t_i^*)其中R为smooth L1平滑方程： smooth_{L_1}(x)=\\left\\{ \\begin{array}{lr} 0.5x^2 & if \\ |x|","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"机器视觉","slug":"深度学习/机器视觉","permalink":"http://blog.a-stack.com/categories/深度学习/机器视觉/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"物体识别","slug":"物体识别","permalink":"http://blog.a-stack.com/tags/物体识别/"},{"name":"文献","slug":"文献","permalink":"http://blog.a-stack.com/tags/文献/"}]},{"title":"利用迁移学习实现车辆识别","slug":"Vechicle-identification-by-stanford-car-dataset","date":"2018-04-11T06:44:58.000Z","updated":"2018-05-15T09:14:33.589Z","comments":false,"path":"2018/04/11/Vechicle-identification-by-stanford-car-dataset/","link":"","permalink":"http://blog.a-stack.com/2018/04/11/Vechicle-identification-by-stanford-car-dataset/","excerpt":"","text":"摘要： 利用迁移学习技术训练识别汽车厂商和款式的模型。 @[toc] 概述数据集： Stanford Cars Dataset 数据集组成：包含196种车辆的16,185张照片；其中训练集8144，测试集8041； 关键特征包括：车辆制造商、款式、生产日期（比如：2012 Tesla Model S）； 下载地址：https://ai.stanford.edu/~jkrause/cars/car_dataset.html； 相关论文：3D Object Representations for Fine-Grained Categorization，Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei 数据集特点： 存在明显的数据不平衡问题； 每个分类图像数目过少，无法达到准确预测分类目标的基准； 针对数据集的特点，利用迁移学习Fine-Tune来训练一个在ImageNet上预训练的模型是一个不错的方式，下面我们将从数据的准备开始一步步得完成模型的训练任务。 数据准备在这一部分中，我们将读入原始数据，进行基本的数据处理，然后对数据进行统一存储，一般面对大规模的数据集可以选用HDF5或MXNet的.LST格式进行存储。 通过这种方式进行存储可以解决每张图片读取都要产生一次IO带来的访问时延，同时可以利用存储系统连续读的方式，直接对大规模数据集进行切片操作。 配置信息为了配合后续处理流程方便，新建一个car.config的配置文件，用于对相关配置信息的存储： 1234567891011121314151617181920212223242526272829303132333435from os import path# define the base path to the cars datasetBASE_PATH = \"Path-to-car-dataset\"# based on the base path, derive the images path and meta file pathIMAGES_PATH = path.sep.join([BASE_PATH, \"car_ims\"])LABELS_PATH = path.sep.join([BASE_PATH, \"complete_dataset.csv\"])#define path for HDF5TRAIN_HDF5 = path.sep.join([MX_OUTPUT, \"hdf5/train.hdf5\"])VAL_HDF5 = path.sep.join([MX_OUTPUT, \"hdf5/val.hdf5\"])TEST_HDF5 = path.sep.join([MX_OUTPUT, \"hdf5/test.hdf5\"])#define path for storing Mean R G B dataDATASET_MEAN = path.sep.join([BASE_PATH, \"output/car_mean.json\"])# define the path to the output directory used for storing plots,# classification reports, etc.OUTPUT_PATH = \"output\"MODEL_PATH = path.sep.join([OUTPUT_PATH,\"inceptionv3_stanfordcar.hdf5\"])FIG_PATH = path.sep.join([OUTPUT_PATH,\"inceptionv3_stanfordcar.png\"])JSON_PATH = path.sep.join([OUTPUT_PATH,\"inceptionv3_stanfordcar.json\"])# define the path to the label encoderLABEL_ENCODER_PATH = path.sep.join([BASE_PATH, \"output/le.cpickle\"])# define the percentage of validation and testing images relative# to the number of training imagesNUM_CLASSES = 164NUM_VAL_IMAGES = 0.15NUM_TEST_IMAGES = 0.15# define the batch sizeBATCH_SIZE = 64 配置文件中包括HDF5文件的存放位置描述，原始数据和数据描述文件路径，RGB均值存储位置，训练过程中输出的图像和日志存储位置等信息。 数据概览 我们先将数据描述文件complete_dataset.csv导入，了解下数据格式： 1234import pandas as pd# loading image paths and labelsdf = pd.read_csv(config.LABELS_PATH)df.head() id Image FileName Make Model Vechicle Typle Year 0 car_ims/000090.jpg Acura RL Sedan 2012 1 car_ims/000091.jpg Acura RL Sedan 2012 2 car_ims/000092.jpg Acura RL Sedan 2012 3 car_ims/000093.jpg Acura RL Sedan 2012 遍历文件列表，将文件地址和样本标签分别进行存储，在本实验中值使用了制造商和款式两种特征，所以构成分类总共有164个： 1234567891011import osfrom sklearn.preprocessing import LabelEncodertrainPaths = []trainLabels = []for id,name in enumerate(df[\"Image Filename\"]): trainPaths.append(os.sep.join([config.IMAGES_PATH,name])) trainLabels.append(\"&#123;&#125;:&#123;&#125;\".format(df.iloc[id][\"Make\"], df.iloc[id][\"Model\"]))#Encoding labels to numle = LabelEncoder()trainLabels = le.fit_transform(trainLabels) 按照70%，15%，15%切分训练集、验证集和测试集： 1234567891011121314from sklearn.model_selection import train_test_splitnumVal = int(len(trainPaths)*0.15)numTest = int(len(trainPaths)*0.15)# perform sampling from the training set to construct a a validation setsplit = train_test_split(trainPaths, trainLabels, test_size=numVal, stratify=trainLabels)(trainPaths, valPaths, trainLabels, valLabels) = split# perform stratified sampling from the training set to construct a testing setsplit = train_test_split(trainPaths, trainLabels, test_size=numTest, stratify=trainLabels)(trainPaths, testPaths, trainLabels, testLabels) = split 初始化相关配置： 12345678910# initialize the lists of RGB channel averages(R, G, B) = ([], [], [])# construct a list pairing the training, validation, and testing# image paths along with their corresponding labels and output list# filesdatasets = [ (\"train\", trainPaths, trainLabels, config.TRAIN_HDF5), (\"val\", valPaths, valLabels, config.VAL_HDF5), (\"test\", testPaths, testLabels, config.TEST_HDF5)] 遍历数据集并存储至HDF5文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import HDF5DatasetWriterimport AspectAwarePreprocessorimport progressbar#resize images to (256,256,3)aap = AspectAwarePreprocessor(256,256)# loop over the dataset tuplesfor (dType, paths, labels, outputPath) in datasets: # create HDF5 writer print(\"[INFO] building &#123;&#125;...\".format(outputPath)) writer = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath) # initialize the progress bar widgets = [\"Building Dataset: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()] pbar = progressbar.ProgressBar(maxval=len(paths), widgets=widgets).start() # loop over the image paths for (i, (path, label)) in enumerate(zip(paths, labels)): # load the image from disk try: image = cv2.imread(path) image = aap.preprocess(image) #print(image.shape) # if we are building the training dataset, then compute the # mean of each channel in the image, then update the respective lists if dType == \"train\": (b, g, r) = cv2.mean(image)[:3] R.append(r) G.append(g) B.append(b) # add the image and label to the HDF5 dataset writer.add([image], [label]) pbar.update(i) except: print(path) print(label) break # close the HDF5 writer pbar.finish() writer.close() 我们首先统一将文件调整到(256,256,3)大小，再进行存储，所以在读取文件之后进行了简单的预处理。 将RGB均值存储至单独文件 12345678import pandas as pdimport json# construct a dictionary of averages, then serialize the means to a JSON fileprint(\"[INFO] serializing means...\")D = &#123;\"R\": np.mean(R), \"G\": np.mean(G), \"B\": np.mean(B)&#125;f = open(config.DATASET_MEAN, \"w\")f.write(json.dumps(D))f.close() 至此，我们完成了将图像文件分为三个类别并分别存到了三个HDF5文件之中。 关于HDF5文件存储相关内容，详见后续推出的预处理博客~~ 如果使用MXNET的list和rec来构建数据存储集合，在上述步骤3的基础上，按如下步骤： 构建数据集列表文件’.list’文件 1234567891011121314151617181920212223# construct a list pairing the training, validation, and testing# image paths along with their corresponding labels and output list# filesdatasets = [ (\"train\", trainPaths, trainLabels, config.TRAIN_MX_LIST), (\"val\", valPaths, valLabels, config.VAL_MX_LIST), (\"test\", testPaths, testLabels, config.TEST_MX_LIST)]# loop over the dataset tuplesfor (dType, paths, labels, outputPath) in datasets: # open the output file for writing print(\"[INFO] building &#123;&#125;...\".format(outputPath)) f = open(outputPath, \"w\") # loop over each of the individual images + labels for (i, (path, label)) in enumerate(zip(paths, labels)): # write the image index, label, and output path to file row = \"\\t\".join([str(i), str(label), path]) f.write(\"&#123;&#125;\\n\".format(row)) # close the output file f.close() 将Label名称序列化存储，便于后续调用： 123f = open(config.LABEL_ENCODER_PATH, \"wb\")f.write(pickle.dumps(le))f.close() 3.利用MXNet工具im2rec创建记录文件 12345$ /dsvm/tools/mxnet/bin/im2rec ./raid/datasets/cars/lists/train.lst \"\" ./raid/datasets/cars/rec/train.rec resize=256 encoding='.jpg' quality=100$ /dsvm/tools/mxnet/bin/im2rec ./raid/datasets/cars/lists/test.lst \"\" ./raid/datasets/cars/rec/test.rec resize=256 encoding='.jpg' quality=100$ /dsvm/tools/mxnet/bin/im2rec ./raid/datasets/cars/lists/val.lst \"\" ./raid/datasets/cars/rec/val.rec resize=256 encoding='.jpg' quality=100 训练迁移学习网络在迁移学习的模型选择上我们选择了基于Keras提供的InceptionV3,可通过Keras官方文档了解更多使用说明。下表列出了在keras中各模型的表现： 模型 大小 Top1准确率 Top5准确率 参数数目 深度 Xception 88MB 0.790 0.945 22,910,480 126 VGG16 528MB 0.715 0.901 138,357,544 23 VGG19 549MB 0.727 0.910 143,667,240 26 ResNet50 99MB 0.759 0.929 25,636,712 168 InceptionV3 92MB 0.788 0.944 23,851,784 159 IncetionResNetV2 215MB 0.804 0.953 55,873,736 572 MobileNet 17MB 0.665 0.871 4,253,864 88 数据读入输入读入过程主要包括几个关键任务：读取RGB均值文件，对原始数据进行预处理：图像扣取、数据增强、去通道均值、矩阵化等。 这块内容不是本篇重点，只能挖坑留给后续更新。 12345678910111213141516171819# construct the training image generator for data augmentationaug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")# load the RGB means for the training setmeans = json.loads(open(config.DATASET_MEAN).read())# initialize the image preprocessorssp = SimplePreprocessor(224, 224)pp = PatchPreprocessor(224, 224)mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])iap = ImageToArrayPreprocessor()# initialize the training and validation dataset generatorstrainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 64, aug=aug, preprocessors=[pp, mp, iap], classes=config.NUM_CLASSES)valGen = HDF5DatasetGenerator(config.VAL_HDF5, 64, preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES) 模型设计模型设计过程参考了Keras官方文档给出的演示，导入没有top的预训练InceptionV3模型， 12345678910111213141516171819202122232425262728from keras.applications.inception_v3 import InceptionV3from keras.preprocessing import imagefrom keras.models import Modelfrom keras.layers import Dense, GlobalAveragePooling2Dfrom keras import backend as K# load the Inception network, ensuring the head FC layer sets are left offbaseModel = InceptionV3(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))# initialize the new head of the network, a set of FC layers# followed by a softmax classifierx = baseModel.outputx = GlobalAveragePooling2D()(x)x = Dense(1024, activation='relu')(x)headModel = Dense(config.NUM_CLASSES, activation='softmax')(x)model = Model(inputs=baseModel.input, outputs=headModel)# loop over all layers in the base model and freeze them so they# will *not* be updated during the training processfor layer in baseModel.layers: layer.trainable = False# compile our model (this needs to be done after our setting our# layers to being non-trainableprint(\"[INFO] compiling model...\")opt = SGD(lr=0.005,momentum=0.9)model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]) 特别注意在迁移学习中，由于新添加增的初始权重是随机生成的，而前面大量网络参数并frozen之后不再发生变化，所以需要一个预测的过程来学习参数到一定水平，需要控制学习率在一个比较小的范围。 这个过程可能需要反复尝试试错。 训练过程优化训练过程参考了《Deep Learning for Computer Vison with Python》作者给出的Ctrl+C训练方法，可以随时保存训练现场，调整训练率继续进行训练。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import argparseimport jsonimport osimport logging# construct the argument parse and parse the argumentsap = argparse.ArgumentParser()ap.add_argument(\"-c\", \"--checkpoints\", required=True, help=\"path to output checkpoint directory\")ap.add_argument(\"-m\", \"--model\", type=str, help=\"path to *specific* model checkpoint to load\")ap.add_argument(\"-s\", \"--start-epoch\", type=int, default=0, help=\"epoch to restart training at\")args = vars(ap.parse_args())# set the logging level and output filelogging.basicConfig(level=logging.DEBUG, filename=\"training_&#123;&#125;.log\".format(args[\"start_epoch\"]), filemode=\"w\")if args[\"model\"] is None:# load the VGG16 network, ensuring the head FC layer sets are left off ... ## 实现上一步骤的预训练模型定义和模型预热 else: print(\"[INFO] loading &#123;&#125;...\".format(args[\"model\"])) model = load_model(args[\"model\"]) # update the learning rate print(\"[INFO] old learning rate: &#123;&#125;\".format(K.get_value(model.optimizer.lr))) K.set_value(model.optimizer.lr, 1e-3) print(\"[INFO] new learning rate: &#123;&#125;\".format(K.get_value(model.optimizer.lr))) # construct the set of callbackscallbacks = [ EpochCheckpoint(args[\"checkpoints\"], every=5, startAt=args[\"start_epoch\"]), TrainingMonitor(config.FIG_PATH, jsonPath=config.JSON_PATH, startAt=args[\"start_epoch\"])]# train the networkprint(\"[INFO] training network...\")model.fit_generator( trainGen.generator(), steps_per_epoch=trainGen.numImages // config.BATCH_SIZE, validation_data=valGen.generator(), validation_steps=valGen.numImages // config.BATCH_SIZE, epochs=100, max_queue_size=config.BATCH_SIZE * 2, callbacks=callbacks, verbose=1) 参考 deep learning for computer vision with python","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"},{"name":"开放数据集","slug":"开放数据集","permalink":"http://blog.a-stack.com/tags/开放数据集/"},{"name":"图像分类","slug":"图像分类","permalink":"http://blog.a-stack.com/tags/图像分类/"}]},{"title":"基于机器视觉技术的品牌LOGO检测","slug":"Deep-Learning-Lab-Logo-Detection","date":"2018-04-09T04:45:13.000Z","updated":"2018-05-15T14:45:46.067Z","comments":false,"path":"2018/04/09/Deep-Learning-Lab-Logo-Detection/","link":"","permalink":"http://blog.a-stack.com/2018/04/09/Deep-Learning-Lab-Logo-Detection/","excerpt":"","text":"利用Flickr LOGO数据集训练一个检测品牌LOGO的网络，对机器视觉的物体识别技术进行验证。 @[toc] 概述最近在做一个利用机器视觉技术进行超市物品检点的项目调研分析，需要先寻找一个可行的技术方案验证可行性，Flickr提供的LOGO数据集是一个很好的品牌LOGO识别例子，本文记录利用Flickr LOGO数据集训练一个物体识别的深度神经网络过程。 数据集Flickr LOGO数据集提供了三种不同类型的LOGO数据集集合，分别为Flickr Logos 27 dataset，Datasets: FlickrLogos-32以及Datasets: FlickrLogos-47。我们先来看一下每种数据集的组成及数据结构： Flickr Logos 27 dataset 训练集包含27个分类的810张标记照片，每个分类30张照片 分散集包含4207张logo图片 测试集有270张照片，每个分类5张照片，另外有135张分类外照片集 27个分类包括：Adidas, Apple, BMW, Citroen, Coca Cola, DHL, Fedex, Ferrari, Ford, Google, Heineken, HP, McDonalds, Mini, Nbc, Nike, Pepsi, Porsche, Puma, Red Bull, Sprite, Starbucks, Intel, Texaco, Unisef, Vodafone and Yahoo. 下载地址：下载 数据格式：下载文件夹中提供一个txt文件用于描述每个文件中LOGO的分类和位置信息 12345678910# FileName ClassName subset Coordinates（x1 y1 x2 y2）4763210295.jpg Adidas 1 91 288 125 306 4763210295.jpg Adidas 1 182 63 229 94 4763210295.jpg Adidas 1 192 291 225 306 4763210295.jpg Adidas 1 285 61 317 79 4763210295.jpg Adidas 1 285 298 324 329 4763210295.jpg Adidas 1 377 292 421 324 4763210295.jpg Adidas 1 383 55 416 76 1230939811.jpg Adidas 2 129 326 257 423 1230939811.jpg Adidas 2 137 336 243 395 Flickr Logos 32/47 dataset FlickrLogos-32 was designed for logo retrieval and multi-class logo detection and object recognition. However, the annotations for object detection were often incomplete,since only the most prominent logo instances were labelled. FlickrLogos-47 uses the same image corpus as FlickrLogos-32 but has been re-annotated specifically for the task of object detection and recognition. 2.1 Flickr Logos-32 32个分类包括： Adidas, Aldi, Apple, Becks, BMW,Carlsberg, Chimay, Coca-Cola, Corona, DHL, Erdinger, Esso,Fedex, Ferrari, Ford, Foster’s, Google, Guiness, Heineken, HP,Milka, Nvidia, Paulaner, Pepsi, Ritter Sport, Shell, Singha,Starbucks, Stella Artois, Texaco, Tsingtao and UPS 数据集被划分为P1,P2,P3三个子集： Partition Description Images #Images P1 (training set) Hand-picked images 10 per class 320 images P2 (validation set) Images showing at least a single logo under various views 30 per class + 3000 non-logo images 3960 images P3 (test set = query set) Images showing at least a single logo under various views 30 per class + 3000 non-logo images 3960 images / / / 8240 images 2.2 FlickrLogos-47 47个分类包括：Adidas (Symbol), Adidas (Text), Aldi,Apple, Becks (Symbol), Becks (Text), BMW, Carlsberg (Symbol),Carlsberg (Text), Chimay (Symbol), Chimay (Text), Coca-Cola,Corona (Symbol), Corona (Text), DHL, Erdinger (Symbol),Erdinger (Text), Esso (Symbol), Esso (Text), Fedex, Ferrari, Ford,Foster’s (Symbol), Foster’s (Text), Google, Guiness (Symbol),Guiness (Text), Heineken, HP, Milka (Symbol), Milka (Text), Nvidia (Symbol), Nvidia (Text), Paulaner (Symbol), Paulaner (Text), Pepsi (Symbol), Pepsi (Text), Ritter Sport, Shell, Singha (Symbol),Singha (Text), Starbucks, Stella Artois (Symbol), Stella Artois (Text), Texaco, Tsingtao (Symbol) Tsingtao (Text) and UPS. 小结 Flickr Logo数据集虽然类别数目众多，但具体到每个分类提供的样本数目有限，在数据预处理环节需要配合数据增强手段来扩充数据集的数目； 另外也可以仿照车牌识别的方法，将扣取的LOGO图像添加到不同背景噪声的图像中，生成多种训练数据； 由于LOGO图像包含图像特征有限，同时提供小样本数据，通过迁移学习的方案利用ImageNet训练好的模型进行迁移学习是一种很好的方式，本文将对这种方式进行讨论及实现； 三种数据集面向不同的功能也设计需求，从图像质量上来看Flickr-47质量相对较好，同时在32分类和47分类中提供了对图像语义分割的标定数据； 在概览过任务数据集之后，我们将按照深度学习业务处理流程，逐步进行数据的预处理、模型准备、训练和验证等工作。为简化问题处理难度，我们使用Flickr Logo -27来进行本次实验。 数据准备数据准备环节主要使用如下基本的工具和库文件： 1234567import numpy as npimport pandas as pdimport osimport matplotlib.pyplot as pltimport matplotlib.patches as patchesimport cv2import imutils 其中， numpy用来做基本的矩阵处理； pandas用于读取和分析数据描述文件； matplotlib用于辅助显示预处理结果； cv2是opencv的python封装，进行图像读取、图像分析等操作； imutils是一个很好用的图像处理库，可以满足基本的图像处理需求 首先我们先查看从flickr-27上下载的文件flickr_logos_27_dataset_training_set_annotation.txt来了解基本的图像数据信息和分类信息： 1234df=pd.read_csv(\"./flickr_logos_27_dataset/flickr_logos_27_dataset_training_set_annotation.txt\",sep=\" \", header=None)df.drop(df.columns[-1],axis=1, inplace=True)df.columns=[\"Name\",\"labels\",\"subset\",\"x1\",\"y1\",\"x2\",\"y2\"]df.head() 1234567output:&gt;&gt;&gt;&gt; Name labels subset x1 y1 x2 y20 144503924.jpg Adidas 1 38 12 234 1421 2451569770.jpg Adidas 1 242 208 413 3312 390321909.jpg Adidas 1 13 5 89 603 4761260517.jpg Adidas 1 43 122 358 3544 4763210295.jpg Adidas 1 83 63 130 93 在描述文件中总共提供了4536条记录，而实际提供的图像文件只有1000多张，这说明很多文件包括不止一个LOGO。 12len(df)&gt;&gt;&gt;: 4536 我们可以利用pandas对文件进行一个简单的shuffle处理，便于快速切分成训练集和测试集： 12# shuffle the datasetsdf = df.sample(frac=1).reset_index(drop=True) 为了快速查看描述文件提供的标记信息在图像中的显示效果，我们写一个函数来查看一下LOGO标记信息的效果： 123456789101112def show_image(id): fig = plt.figure() image = os.path.join(\"./flickr_logos_27_dataset/flickr_logos_27_dataset_images/\",df.loc[id][\"Name\"]) image = cv2.imread(image) plt.figure(8) plt.imshow(image) currentAxis=plt.gca() rect=patches.Rectangle((df[\"x1\"].iloc[id], df[\"y1\"].iloc[id]), df[\"x2\"].iloc[id]-df[\"x1\"].iloc[id], df[\"y2\"].iloc[id]-df[\"y1\"].iloc[id], linewidth=2,edgecolor='r',facecolor='none') currentAxis.add_patch(rect) 其中用到了plt.gca()和matplotlib的patches函数用于图像的叠加显示，当然也可以直接调用cv2.rectangle函数 随机查看一个标记在图像中的显示效果 123import randomid = random.randint(0,len(df))show_image(id) 下面需要写一个抠图程序，把所有LOGO从原始图像中扣取出来，形成训练用数据集，在保存图像之前进行图像简单的预处理和调整形状： 12345678910111213141516171819def crop_img(id): image = os.path.join(\"./flickr_logos_27_dataset/flickr_logos_27_dataset_images/\",df.loc[id][\"Name\"]) image = cv2.imread(image) crop_image = image[df[\"y1\"].iloc[id]:df[\"y2\"].iloc[id],df[\"x1\"].iloc[id]:df[\"x2\"].iloc[id]] return crop_image WIDTH = 64HEIGHT = 64for id, name in enumerate(df[\"Name\"]): cropped_image = crop_img(id) try: resized_image = cv2.resize(cropped_image, (WIDTH,HEIGHT),interpolation=cv2.INTER_CUBIC) except: print(id) continue image_name = str(id)+\"_\"+df.iloc[id][\"labels\"]+\".jpg\" cv2.imwrite(os.path.join(\"./flickr_logos_27_dataset/cropped/\",image_name),resized_image) 在图像扣取过程中，有几点需要注意： 由于描述问题提供的信息本身的问题，有一些异常数据需要剔除，比如有5条记录提供的x1=x2,或y1=y2，即在原始图像上没有进行标记； 图像缩放其实不应该采用这种傻瓜的压缩方式，应该尽量控制长宽比，保证不产生明显的形变； 处理完成之后，扣取图像将在cropped文件夹中以{id}_{label}.jpg的文件名存储。 图像扣取之后，通过人工核对，我们发现仍然存在一些明显有问题的图像，比如多张puma的图像，其实存在明显的标记问题，需要从数据集中剔除： 数据集切分我们将扣取数据读入进行简单预处理和数据切分： 12345678910data = []labels = []for img in os.listdir(\"./flickr_logos_27_dataset/cropped/\"): img_file = cv2.imread(os.path.join(\"./flickr_logos_27_dataset/cropped/\",img)) data.append(img_file) labels.append(img.split(\"_\")[1].split(\".\")[0])data = np.stack(data)labels = np.stack(labels)data = data/255 将标签数据转变成OneHot矩阵： 123from sklearn.preprocessing import LabelBinarizerle = LabelBinarizer()labels = le.fit_transform(labels) 切分数据集 1X,testX,y,testy = train_test_split(data, labels,test_size=0.1,stratify=labels,random_state=42 ) 数据增强数据增强是图像处理中经常采用的一种数据处理方式，由于涉及内容较多，在本篇实战中不单独展开，仅把利用Keras数据增强工具ImageDataGenerator的方法提供一下： 12345678from keras.preprocessing.image import ImageDataGenerator # construct the training image generator for data augmentationaug = ImageDataGenerator(rotation_range=18, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")gen_flow=aug.flow(X, y,batch_size=64,seed=0)validation=aug.flow(testX,testy,batch_size=32,seed=0) 模型定义根据前文对数据的分析，我们分别采取两种方式设计网络模型：从头训练一个深度卷积神经网络和利用迁移学习Fine-Tune一个满足需求的网络模型。 从头训练一个网络模型由于问题本质是一个物体识别任务，所以在实现上应该包括图像分类和定位的回归两个子任务，我们可以简化问题通过一个滑动窗口来对输入图像进行扫描，然后针对每个扫描窗口进行图像分类。 当然实际过程中，问题要远比这复杂，很难选择合适的滑动窗口大小适用现实图像的需求，所以在主流的物体识别模型中一般都采用多种不同大小的Anchor box来回归图像的位置。 由于LOGO每张图像包含特征有限，我们在本次实验中利用LeNet的架构，设计了一个简单的卷积网络模型如下图所示： 模型主体利用三个CONV =&gt; RELU =&gt; POOL结构来抽取图像特征，最后利用全联通网络+Softmax分类器来获得最终27类分类结果。 12345678910111213141516171819202122232425262728293031323334from keras.models import Sequentialfrom keras.layers.convolutional import Conv2Dfrom keras.layers.convolutional import MaxPooling2Dfrom keras.layers.core import Activationfrom keras.layers.core import Flattenfrom keras.layers.core import Densefrom keras import backend as Kmodel = Sequential()inputShape = (HEIGHT, WIDTH, 3) # first set of CONV =&gt; RELU =&gt; POOL layersmodel.add(Conv2D(16, (3, 3), padding=\"same\",input_shape=inputShape))model.add(Activation(\"relu\"))model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))# second set of CONV =&gt; RELU =&gt; POOL layersmodel.add(Conv2D(32, (3, 3), padding=\"same\"))model.add(Activation(\"relu\"))model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))# third set of CONV =&gt; RELU =&gt; POOL layersmodel.add(Conv2D(64, (3, 3), padding=\"same\"))model.add(Activation(\"relu\"))model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))# first (and only) set of FC =&gt; RELU layersmodel.add(Flatten())model.add(Dense(500))model.add(Activation(\"relu\"))model.add(Dropout(0.25))# softmax classifiermodel.add(Dense(len(CLASSNAME)))model.add(Activation(\"softmax\")) 定义目标优化函数： 123from keras.optimizers import Adam,SGD,RMSpropopt = RMSprop(lr=0.001, rho=0.9)model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"]) 迭代训练100个epoch: 1234567history=model.fit_generator( gen_flow, steps_per_epoch=len(X) // 32, validation_data=aug.flow(testX,testy,batch_size=32,seed=0), validation_steps=len(testX) // 32, epochs=100, verbose=1) 100轮之后，验证集达到了99.89%的准确率，基本满足了要求，训练过程中训练数据和验证数据的准确率及Loss变化详见下图： 测试1234567891011plt.figure(figsize = (15,40))for i,test_img in enumerate(os.listdir(\"./test\")): img = cv2.imread(os.path.join(\"./test\",test_img)) img = cv2.resize(img, (WIDTH,HEIGHT),interpolation=cv2.INTER_CUBIC) img = np.expand_dims(img,axis=0) result = model.predict(img) result = le.inverse_transform(result) plt.subplot(8,4, i+1) img = cv2.cvtColor(img[0], cv2.COLOR_BGR2RGB) plt.imshow(img) plt.title('pred:' + str(result[0])) 设计滑动窗口和特征金字塔其中滑动窗口用来遍历图像，特征金字塔用于实现图像的多尺度变换，保证多种不同大小的LOGO都可以被准确识别。 123456789101112131415161718192021222324def sliding_window(image, step, ws): # slide a window across the image for y in range(0, image.shape[0] - ws[1], step): for x in range(0, image.shape[1] - ws[0], step): # yield the current window yield (x, y, image[y:y + ws[1], x:x + ws[0]])def image_pyramid(image, scale=1.5, minSize=(64, 64)): # yield the original image yield image # keep looping over the image pyramid while True: # compute the dimensions of the next image in the pyramid w = int(image.shape[1] / scale) image = imutils.resize(image, width=w) # if the resized image does not meet the supplied minimum # size, then stop constructing the pyramid if image.shape[0] &lt; minSize[1] or image.shape[1] &lt; minSize[0]: break # yield the next image in the pyramid yield image 特征金字塔： 为了检测不同尺度的目标，依次将原图按比例缩放并送入网络。缺点是需要多次resize图像，繁琐耗时。 我们定义了输入图像的尺寸为(150,150)，滑动窗口大小与我们前面训练的分类网络的输入一致为(64,64),特征金字塔的缩小比例为1.5倍，这样将在原始图像基础上进行两次缩放；另外定义了滑动窗口的步长为16。 12345678# initialize variables used for the object detection procedureINPUT_SIZE = (150, 150)PYR_SCALE = 1.5WIN_STEP = 16ROI_SIZE = (64, 64)labels = &#123;&#125;CLASS_NAMES = list(lb.classes_) 为简化后续分析，定义一个预测函数，用于返回图像中预测准确率超过minProb窗口及对象分类： 123456789101112131415def logo_prediction(model, batchROIs, batchLocs, labels, minProb=0.5,dims=(64, 64)): preds = model.predict(batchROIs) for i in range(0,len(preds)): prob = np.max(preds[i]) if prob &gt; 0.5: index = np.argmax(preds[i]) label = CLASS_NAMES[int(index)] # grab the coordinates of the sliding window for # the prediction and construct the bounding box (pX, pY) = batchLocs[i] box = (pX, pY, pX + dims[0], pY + dims[1]) L = labels.get(label, []) L.append((box,prob)) labels[label] = L return labels 我们将遍历每个特征金字塔和每个滑动窗口，对识别结果进行预测： 1234567891011121314151617181920212223242526272829303132333435img_file = \"./test/2.jpg\"orig = cv2.imread(img_file)# resize the input image to be a squareresized = cv2.resize(orig, INPUT_SIZE, interpolation=cv2.INTER_CUBIC)# initialize the batch ROIs and (x, y)-coordinatesbatchROIs = NonebatchLocs = []# loop over the image pyramidfor image in image_pyramid(resized, scale=PYR_SCALE,minSize=ROI_SIZE): # loop over the sliding window locations for (x, y, roi) in sliding_window(resized, WIN_STEP, ROI_SIZE): # take the ROI and pre-process it so we can later classify the # region with Keras #roi = img_to_array(roi) roi = roi/255 roi = np.expand_dims(roi, axis=0) # roi = imagenet_utils.preprocess_input(roi) # if the batch is None, initialize it if batchROIs is None: batchROIs = roi # otherwise, add the ROI to the bottom of the batch else: batchROIs = np.vstack([batchROIs, roi]) # add the (x, y)-coordinates of the sliding window to the batch batchLocs.append((x, y)) # classify the batch, then reset the batch ROIs and # (x, y)-coordinates model.predict(batchROIs) labels = logo_prediction(model, batchROIs, batchLocs,labels, minProb=0.9) 当进行到这步骤才突然发现训练分类中缺了一个很重要的背景分类，将导致在背景上很多信息的预测会出问题，后续等整些背景图片再重新训练网络，😭 最后一步是预测结果的极大值抑制和显示： 1234567891011121314151617181920212223242526from imutils.object_detection import non_max_suppression# loop over the labels for each of detected objects in the imagefor k in labels.keys(): # clone the input image so we can draw on it clone = resized.copy() # loop over all bounding boxes for the label and draw them on the image for (box, prob) in labels[k]: (xA, yA, xB, yB) = box cv2.rectangle(clone, (xA, yA), (xB, yB), (0, 255, 0), 2) # grab the bounding boxes and associated probabilities for each # detection, then apply non-maxima suppression to suppress # weaker, overlapping detections boxes = np.array([p[0] for p in labels[k]]) proba = np.array([p[1] for p in labels[k]]) boxes = non_max_suppression(boxes, proba) # loop over the bounding boxes again, this time only drawing the # ones that were *not* suppressed for (xA, yA, xB, yB) in boxes: cv2.rectangle(clone, (xA, yA), (xB, yB), (0, 0, 255), 2) # show the output image print(\"[INFO] &#123;&#125;: &#123;&#125;\".format(k, len(boxes))) plt.imshow(clone) 极大值抑制是物体识别中很重要的一个环节，相关概念以后在慢慢整理 利用迁移学习优化一个物体识别网络模型上述方法虽然简单容易理解，但存在很大的计算效率问题，每张图片需要进行多次特征提取和多次运算，对计算效率造成很大影响。目前主流的物体识别算法往往都可以应用于实时视频流的分析，显然使用上述方法是不合适的。我们将在后面探讨利用现有的物体识别网络通过迁移学习解决我们的目标识别问题。 由于本篇内容太多，利用迁移学习实现的方法，将单独作为一篇，此处留待插入链接。 本文涉及代码详见Github 训练一个二分类网络检查货架上是否有百事可乐 参考Github实现一个物品检测原型：训练一个二分类分类器 在数据准备阶段与上述过程唯一不同是label的设置，如下： 123456789101112131415import os, cv2import numpy as npdata = []labels = []HEIGHT = 64WIDTH = 64for img in os.listdir(\"./flickr_logos_27_dataset/cropped/\"): img_file = cv2.imread(os.path.join(\"./flickr_logos_27_dataset/cropped/\",img)) data.append(img_file) label = img.split(\"_\")[1].split(\".\")[0] if label != \"Pepsi\": label = \"Nop\" labels.append(label)data = np.stack(data)labels = np.stack(labels) 由于是二分类问题，所以只需要最后一层使用sigmoid函数构建分类器即可，label的序列话方面使用LabelEncoder转换为0或者1即可： 1234from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelEncoderlb = LabelEncoder()y = lb.fit_transform(labels) 数据增强与前文类似，不再赘言。在网络结构上，只需要修改最后为sigmoid函数输出，优化目标使用binary_crossentropy： 12345...model.add(Activation(\"sigmoid\"))...model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]) 由于只有两个分类，所以模型很容易收敛，最后准确率也接近100%。 最后利用一个滑动窗口不停的扫描图像并利用cv2展示结果即可： 12345678910111213141516171819202122for (x, y, window) in sliding_window(img, stepSize=32, windowSize=(winW, winH)): # if the window does not meet our desired window size, ignore it if window.shape[0] != winH or window.shape[1] != winW: continue crop_img=crop_image(sample_path,x, y, x + winW, y + winH) crop_img=imresize(crop_img,(64,64)) crop_img = crop_img/255 prediction=model.predict(crop_img.reshape(1,64,64,3)) if prediction == 1: pred = 'Pepsi' else: pred=' ' clone = img.copy() cv2.putText(clone, pred, (10, 30),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2) cv2.rectangle(clone, (x, y), (x + winW, y + winH), (0, 255, 0), 2) clone = cv2.cvtColor(clone,cv2.COLOR_BGR2RGB) cv2.imshow(\"Window\", clone) cv2.waitKey(1) time.sleep(0.5) ​ Logo检测的应用及分析DeepSense.ai给出了一种Logo检测的分析方法，通过分析视频中不同品牌的logo呈现，统计了不同品牌在同一个视频中Logo出现的时间、出现的方式、呈现的效果等，最终提供给客户一个Logo Visubility Report。 方案的主要流程如下图所示： 生成的分析报告参见下图： 针对的分析视频如下： 参考 Flickr Logos 27 dataset Datasets: FlickrLogos-32 / FlickrLogos-47 ​","categories":[{"name":"动手实践营","slug":"动手实践营","permalink":"http://blog.a-stack.com/categories/动手实践营/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"},{"name":"物体识别","slug":"物体识别","permalink":"http://blog.a-stack.com/tags/物体识别/"},{"name":"开放数据集","slug":"开放数据集","permalink":"http://blog.a-stack.com/tags/开放数据集/"}]},{"title":"深度学习基础之优化算法","slug":"深度学习基础之优化算法","date":"2018-04-01T07:35:01.000Z","updated":"2018-07-02T15:47:11.000Z","comments":true,"path":"2018/04/01/深度学习基础之优化算法/","link":"","permalink":"http://blog.a-stack.com/2018/04/01/深度学习基础之优化算法/","excerpt":"合适优化算法的选择有助于提升训练效率和收敛的速度，本文对常用的优化算法进行总结。在这章中将归纳SGD、RMSprop、Adam等的基础原理。优化算法的优劣来自于对相关算法的熟悉程度。","text":"合适优化算法的选择有助于提升训练效率和收敛的速度，本文对常用的优化算法进行总结。在这章中将归纳SGD、RMSprop、Adam等的基础原理。优化算法的优劣来自于对相关算法的熟悉程度。 深度学习基础篇将从几个不同的层面来总结在过去一段时间对于深度学习关键技术的理解，通过知识体系的归纳了解知识体系的不足，提升对核心技术点的认识。所有系列文章将在未来一段时间内容随着掌握了解的深入迭代更新。目前主要希望对如下几个领域进行归纳汇总： 问题定义 目标及评估 数据准备与预处理 激活函数的归纳及总结 优化算法的归纳及总结 正则化与泛化性能 模型压缩 数据扩充 引言深度学习模型训练的优化算法可以通过寻找降低代价函数$J(\\theta)$的一组参数$\\theta$来实现优化目标。其中代价函数可以定义为训练集上的评价损失： J(\\theta)=E_{(x,y)\\sim \\hat{p}_{data}} L(f(\\boldsymbol{x}; \\boldsymbol{\\theta}), y)其中，$\\hat p_{data}$ 为经验分布，$L$是每个样本的损失函数。由于分布只是真实数据分布的一部分，该式叫做经验风险最小化。我们并不直接最优化风险，而是最优化经验风险，希望也能够很大地降低风险。一方面由于经验风险最小化很容易过拟合训练数据，另一方面深度学习中很多损失函数没有有效的导数形式，所以一般使用代理损失函数来近似优化目标。 好的优化算法在深度学习中主要起到如下作用： 提升收敛速度，降低训练时间； 使网络模型表现更加全面，而不单依靠学习率； 获得更好的模型性能。 深度学习优化算法的挑战 非凸优化众多的局部极小值； 梯度为零的点——鞍点；多类随机函数表现出以下性质：低维空间中，局部极小值很普遍。在更高维空间中，局部极小值很罕见，而鞍点则很常见； 梯度消失与梯度爆炸；（梯度截断解决爆炸问题） 许多现有研究方法在求解具有困难全局结构的问题时，旨在寻求良好的初始点， 而不是开发非局部范围更新的算法。 优化算法从经典的SGD算法开始 W = W-lr * dW其中， W 为权重矩阵 lr 为学习率 dW 为权重W的梯度 在经典SGD中，学习率lr是固定的，为了实现学习率的自适应，提出了多种算法来应对不同的场景需求。 mini-batch和梯度下降在每一轮（epoch）中，使用1个mini-batch的样本来计算梯度下降方向；使用mini-batch的梯度近似全部样本的梯度；是Batch梯度下降和随机梯度下降的折中。 每次只使用单个样本的优化算法有时被称为随机（stochastic）或者在线（online）算法。术语 “在线’’ 通常是指从连续产生样本的数据流中抽取样本的情况，而 不是从一个固定大小的训练集中遍历多次采样的情况。 使用mini-batch的另一个主要原因是为了利用向量化的并行计算来提升处理效率。 理论上，从mini-batch中获得梯度的统计估计源于训练集的冗余。mini-batch的选择主要由如下几个因素决定： 更大的批量会计算更精确的梯度估计，但是汇报确实小于线性的； 如果批量处理中的所有样本可以并行地处理（通常确是如此），那么内存消耗和批量大小会正比； 在某些硬件上使用特定大小的数组时，运行时间会更少。尤其是在使用GPU时， 通常使用 2 的幂数作为批量大小可以获得更少的运行时间； 可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果 (Wilson and Martinez, 2003)。 其中mini-batch的大小一般设置为2的幂指数，比如64，128，256，…,需要根据CPU/GPU内存大小进行设置 Momentum 目的：使用指数移动平均平滑梯度下降的抖动,一般$\\beta=0.9$, 适用于处理高曲率、小但一致的梯度或带有噪声的梯度； 可取的参数有[0.5, 0.9, 0.95, 0.99] v_{t} = \\beta v_{t-1} - lr*dW W_t = W_{t-1} + v_{t}Nesterov型动量随机下降法 在上述动量的基础上加上了对当前梯度的矫正。Nesterov对凸函数在收敛性证明上有更强的理论保证 w_{ahead} = w_{t-1} +\\beta v_{t-1} v_{t} = \\beta v_{t-1} - \\alpha dw_{ahead} W += v_{t} 123v_prev = v # back this upv = mu * v - learning_rate * dx # velocity update stays the samex += -mu * v_prev + (1 + mu) * v # position update changes form 高级优化算法（自适应学习率算法） 自适应学习率的设计 更多内容参见：An overview of gradient descent optimization algorithms Adagrad 引入新变量cache来衡量每个min-batch中哪些参数更新频繁，给与更高的学习率。 缺点： cache是个累加值，当训练比较多轮的大型网络，在后期学不到太多东西。 cache += {dW}^2 W += -\\frac{lr}{\\sqrt{cache}+\\epsilon} * dWAdadelta Adagrad的扩展，修正了学习率单调下降的问题。值使用近期几轮的dW来进行平滑累积。 RMSprop Geoffery Hinton在其Cousera课程中提供的一种方法，采用指数权重滑动平均修正Adagrad的问题。一般$\\rho=0.9$。 RMSprop性能比Adagrad和Adaelta都优秀，收敛速度也优于SGD，是目前用的比较多的优化算法。使用频次仅次于SGD。 如下图，RMSpror的效果：减缓垂直方向的大小，增大水平方向的大小 cache = \\rho * cache + (1-\\rho) * {dW}^2 W += -\\frac{lr}{\\sqrt{cache}+\\epsilon} * dWAdam Adapitive Moment Estimation,2014年提出。将dW和$dW^2$的更新都采用了滑动平均。 一般地，$\\beta_1=0.9, \\beta_2=0.999$； 经验证，Adam在多个场景下效果优于RMSprop； Adam可以认为RMSprop+动量，对应的Nadam算法=RMSprop + Nesterov加速 m = \\beta_1 * m + (1-\\beta_1)*dW v = \\beta_2 * v + (1-\\beta_2)*dW^2 m =\\frac{m}{1-\\beta^t_1} v =\\frac{v}{1-\\beta^t_2} W += -\\frac{lr}{\\sqrt{v}+\\epsilon} * m1234567891011m = config['beta1'] * config['m'] + (1-config['beta1']) * dwv = config['beta2'] * config['v'] + (1-config['beta2']) * np.square(dw)t = config['t'] + 1_m = m / (1-config['beta1']**t)_v = v / (1-config['beta2']**t)next_w = w - config['learning_rate']/(np.sqrt(_v) + config['epsilon']) * _mconfig['m'] = mconfig['v'] = vconfig['t'] = t 学习率衰减 lr = \\frac{1}{1+decay\\_rate*epoch\\_num}*lr_0 学习率的设置原则： 开始时学习率不宜过大，0.01或0.001作为起点。通过观察收敛速度调整学习率； 学习率衰减可以采用：1）上式中随训练轮数衰减；2）指数衰减；3）分数衰减 $lr=\\frac{lr_0}{1+kt}$ keras中提供了decay参数来调节学习率的变化情况： 1opt = SGD(lr=0.01, decay=0.01 / 40, momentum=0.9, nesterov=True) 使用公式： \\alpha_{e+1} = \\alpha_e \\times 1 /(1+\\gamma * e) 另一种学习率为阶梯学习率：ctrl + c Keras提供一个类：LearningrateScheduler来配置自定义的学习率函数 比如： \\alpha_{E+1} = \\alpha_1 \\times F^{(1+E)/D}1234567891011121314def step_decay(epoch): # initialize the base initial learning rate, drop factor, and epochs to drop every initAlpha = 0.01 factor = 0.25 dropEvery = 5 # compute learning rate for the current epoch alpha = initAlpha * (factor ** np.floor((1 + epoch) / dropEvery)) # return the learning rate return float(alpha) ##定义callbackcallbacks = [LearningRateScheduler(step_decay)] 当定义了学习率之后，SGD中声明的配置信息将被忽略 二阶近似方法 牛顿法、拟牛顿法、共轭梯度法、BFGS法 如何选择优化算法事实上各种优化算法没有本质的优劣，也没有完全的选择规则，更多的靠经验和试错。算法的选择更多的来自于每个人对算法的熟悉程度。最常用的三种：SGD、Adam和RMSprop必须研究透，掌握其本质。 目前很多主流的学术论文中还是使用SGD作为训练的优化算法，虽然自适应学习率算法可以提高收敛速度，但收敛速度不是算法的全部，尤其是SGD有可能带来更高的模型精度。 选择一族容易优化的模型比使用一个强大的优化算法更重要。 几种算法性能比较下面几张gif对比了SGD、Momentum、Nesterov、AdaGrad、AdaDelta、RMSProp等优化算法在不同情况下的特性。 Long Valley Beale‘s Function Saddle Point 参考 Keras文档 Deep Learning for Computer Vision with Python Deep Learning Specialization An overview of gradient descent optimization algorithms Deep Learning Book","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"基础知识","slug":"深度学习/基础知识","permalink":"http://blog.a-stack.com/categories/深度学习/基础知识/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"},{"name":"优化","slug":"优化","permalink":"http://blog.a-stack.com/tags/优化/"}]},{"title":"Transfer Learning Summary","slug":"Transfer-Learning-Summary","date":"2018-03-30T15:36:52.000Z","updated":"2018-05-15T09:14:33.636Z","comments":true,"path":"2018/03/30/Transfer-Learning-Summary/","link":"","permalink":"http://blog.a-stack.com/2018/03/30/Transfer-Learning-Summary/","excerpt":"","text":"本文对迁移学习在机器视觉中的实践技巧进行汇总整理 … … @toc Feature Extraction在这种迁移学习模式中，预训练模型将被当作特征提取器（feature extractor）,获得图像的特征表示(feature vactor)。获得特征向量之后，我们只需一个简单的分类器模型，如SVM、逻辑回归分类器、随机森林就可以完成目标分类器的设计。 VGG16倒数第二层(参数层)的输出维度为: 7x7x512 = 25,088 HDF5抽取特征的高效存储可以选用HDF5。Hierarchical Data Format(HDF)是一种针对大量数据进行组织和存储的文件格式。经历了20多年的发展，HDF格式的最新版本是HDF5，它包含了数据模型，库，和文件格式标准。以其便捷有效，移植性强，灵活可扩展的特点受到了广泛的关注和应用。很多大型机构的数据存储格式都采用了HDF5，比如NASA的地球观测系统，MATLAB的.m文件，流体细算软件CDF，都将HDF5作为标准数据格式。HDF5本身用C实现，可以使用python的库h5py对HDF5文件进行操作。可以像操作Numpy数组一样对大型数据进行操作，比如切片，按行读取 数据在HDF5中采取分层存储方式，很像文件系统管理方式，第一级叫做组，类似于container，每个组中可以创建新的组或数据集，每一个dataset包含两部分的数据，Metadata和Data。其中Metadata包含Data相关的信息，而Data则包含数据本身。 12345678910import h5pyp = \"./datasets/hdf5/features.hdf5\"db = h5py.File(p)list(db.keys())&gt;&gt;&gt; [u’features’, u’label_names’, u’labels’]db[\"features\"].shape&gt;&gt;&gt; (3000, 25088)list(db[\"label_name\"])&gt;&gt;&gt; ['cat', 'dogs', 'panda'] Fine-Tune 一般而言，fine-tune在样本数据足够的情况下训练效果优于特征抽取； 学习率要控制的尽量小 如何取层（Keras）123456789101112131415161718192021222324252627from keras.applications import VGG16model = VGG16(weights=\"imagenet\", include_top=False)model.layers&gt;&gt;&gt;[&lt;keras.engine.topology.InputLayer at 0x7ff06cc3b518&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cc3b7f0&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cc3b7b8&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7ff06cc3bd30&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cdb3b00&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cdb3748&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7ff06cbbb400&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cbcc630&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cb5dd68&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cb6d208&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7ff06cb7fdd8&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cb30fd0&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cb30e80&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cb52978&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7ff06caf6470&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06cb076a0&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06ca98dd8&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7ff06caaa278&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7ff06cab8e48&gt;]model.output&gt;&gt;&gt;&lt;tf.Tensor 'block5_pool/MaxPool:0' shape=(?, ?, ?, 512) dtype=float32&gt; 如何添加层（Keras）12345678910111213# load the VGG16 network, ensuring the head FC layer sets are left # offbaseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))# add new layersheadModel = baseModel.outputheadModel = Flatten(name=\"flatten\")(headModel)headModel = Dense(D_num, activation=\"relu\")(headModel)headModel = Dropout(0.5)(headModel)headModel = Dense(classes_num, activation=\"softmax\")(headModel)model = Model(inputs=baseModel.input, outputs=headModel)#freeze baseModel layersfor layer in baseModel.layers: layer.trainable = False Typically you’ll allow your own FC head to warmup for 10-30 epochs, depending on your dataset.使用RMSprop作为优化算法；​ 123456opt = RMSprop(lr=0.001)model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])# train the head of the network for a few epochs (all other # layers are frozen) -- this will allow the new FC layers to# start to become initialized with actual \"learned\" values # versus pure randomprint(\"[INFO] training head...\")model.fit_generator(aug.flow(trainX, trainY, batch_size=32), validation_data=(testX, testY), epochs=25, steps_per_epoch=len(trainX) // 32, verbose=1) 然后可以适当往前解冻一些层，重新训练，一般会解冻最后一层CONV，使用SGD（lr=0.001）作为优化算法； 123# now that the head FC layers have been trained/initialized, lets # unfreeze the final set of CONV layers and make them trainable for layer in baseModel.layers[15:]: layer.trainable = True 1234567# for the changes to the model to take affect we need to recompile # the model, this time using SGD with a *very* small learning rate print(\"[INFO] re-compiling model...\") opt = SGD(lr=0.001)model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])# train the model again, this time fine-tuning *both* the final set # of CONV layers along with our set of FC layers print(\"[INFO] fine-tuning model...\")model.fit_generator(aug.flow(trainX, trainY, batch_size=32), validation_data=(testX, testY), epochs=100, steps_per_epoch=len(trainX) // 32, verbose=1) 迁移学习的选择主要由样本数据量以及训练目标于原目标之间的相似程度决定。 数据规模 相似数据分布 不同数据分布 小样本数据集 特征提取：FC+分类器 特征提取：低层次的Conv+分类器 大样本数据集 Fine-Tune 从头训练新的网络模型","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/categories/算法/"},{"name":"迁移学习","slug":"算法/迁移学习","permalink":"http://blog.a-stack.com/categories/算法/迁移学习/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"迁移学习","slug":"迁移学习","permalink":"http://blog.a-stack.com/tags/迁移学习/"},{"name":"特征抽取","slug":"特征抽取","permalink":"http://blog.a-stack.com/tags/特征抽取/"}]},{"title":"神经网络之感知机（Perceptron）","slug":"神经网络之感知机","date":"2018-03-09T08:37:04.000Z","updated":"2018-05-15T09:14:33.671Z","comments":true,"path":"2018/03/09/神经网络之感知机/","link":"","permalink":"http://blog.a-stack.com/2018/03/09/神经网络之感知机/","excerpt":"感知机于1957年由Rosenblatt等人提出，是神经网络和支持向量机的基础。","text":"感知机于1957年由Rosenblatt等人提出，是神经网络和支持向量机的基础。 @[toc] 1. 人工神经网络现代神经网络概念由人工神经网络延伸而来，出发点在于模拟人类神经元的信息处理方式，而事实上现在采用的神经网络和人类的思维活动方式存在很大的差异。 人类大脑由大于100亿个神经元组成，每个神经元与周围1万一个其它神经元保持关联。 2.感知机（Perceptron）感知机可以被认为是人工神经网络的初级原型，是只有一个神经元一层的神经网络，但事实上感知机与支持向量机的关系比神经网络还要密切。感知机是个二分类的线性分类模型，输出实例为+1和-1的二值结果。感知机学习的过程旨在在假设空间中寻找一个分离超平面，将线性可分的目标数据分为两类。 2.1 感知机模型对于输入向量$x\\in X$ 和输出实例$y\\in {+1,-1}$ ,由输入空间到输出空间的如下函数称为感知机： f(x) = sign(w \\cdot x + b)其中，$w$ 和 $b$ 分别为权值向量和偏置，$sign$ 是符号函数，定义如下： sign(x)= \\left\\{ \\begin{array}{lr} +1, & x \\ge 0\\\\ -1, & x \\lt 0 \\end{array} \\right.其中线性方程$w \\cdot x + b =0$ 对应于特征空间中的一个超平面$S$, $w$ 为超平面的法向量，$b$ 为超平面的截距。这个超平面将特征空间划分为两部分，划分为正、负两个分类。所以，超平面S被称为分离超平面。 2.2 感知机模型的训练过程首先需要指出的是感知机解决的是线性分类问题，面向的数据集为线性可分数据集。给定一个数据集${ (x_1,y_1), (x_2,y_2), …, (x_N, y_N)}$ 。即对所有的正实例$y_i=+1$, 有 $w \\cdot x_i + b \\gt 0$ ; 同样对所有的负实例 $y_i = -1$, $w \\cdot x_i + b \\lt 0$ 。 目标损失函数的定义 对于线性可分数据集，感知机的训练目标为找到一个能够将训练数据正负实例完全分开的分离超平面。为满足这个目标，损失函数选择为误分类点到超平面S的距离总和。 对于任一误分类数据$(x_i, y_i)$， 利用超平面的性质，我们很容易得到： - y_i (w \\cdot x_i +b) > 0误分类点到超平面的距离如下： \\frac{1}{||w||} |w \\cdot x_i +b| =- \\frac{1}{||w||} y_i(w \\cdot x_i +b)如果不考虑$\\frac{1}{||w||}$ （因为它对目标损失函数的影响与参数$w$ 等效）我们可以定义感知机的目标损失函数为： L(w,b) = -\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)其中$M$ 为误分类点的集合。对于任何训练数据，如果该数据属于误分类点则损失函数为参数$w,b$ 的线性函数，如果数据为正确分类数据为0,损失函数$L(w,b)$ 是$w,b$的连续可导函数。 训练过程感知机的训练过程可以通过随机梯度下降法实现： 首先任意选取一个超平面$w_0, b_0$ 作为训练起点， 采用梯度下降法每次选取一个训练数据样本点更新参数$w,b$ 值，逐步逼近目标损失函数的最小值： dw = \\frac{\\partial L(w,b)}{\\partial w} = - \\sum_{x_i \\in M}y_ix_i db = \\frac{\\partial L(w,b)}{\\partial b} = -\\sum_{x_i \\in M}y_i w := w + \\alpha \\sum_{x_i \\in N} (\\hat y_i - y_i)x_i b := b - \\alpha \\sum_{x_i \\in N}(\\hat y_i -y_i) 在权重更新中，引入了一个小技巧，将对误分类数据M的更新扩展到所有的训练数据N。其中$\\hat y{i}$ 表示该点的估计值，如果为正确分类数据，由于$\\hat y{i}=y_i$， 所以两个权重都不会需要进行更新。 2.3 感知机的对偶形式 f(x) = sign(\\sum_{j-1}^N \\sigma_j y_jx_j \\cdot x + b) 暂时不对这部分过多讨论，以后结合SVM的对偶形式一并分析… 3. 感知机使用过程中注意点 感知机只能解决线性可分问题，比如对于XOR求解就无能为力； 感知机算法存在许多解，这些解既依赖初值的选择，也依赖迭代过程中误分类点的选择顺序； 为了得到唯一的分类超平面，需要对分离超平面增加约束条件，这正是线性支持向量机的实现原理； 4. 动手实现一个感知机模型1234567891011121314151617181920212223242526272829import numpy as npclass Perceptron: def __init__(self, N, alpha=0.1): self.W = np.random.randn(N + 1)/np.sqrt(N) # converge bias into W self.alpha = alpha def step(self, f_x): return 1 if f_x&gt;0 else -1 def fit(self, X, y, epochs=10): # insert a column into X X = np.c_[X, np.ones((X.shape[0]))] for epoch in np.arange(0, epochs): for (x, target) in zip(X,y): result = self.step(np.dot(x, self.W)) if result != target: error = result - target self.W += -self.alpha * error * x def predict(self, X, addBias=True): X = np.atleast_2d(X) if addBias: X = np.c_[X, np.ones((X.shape[0]))] return self.step(np.dot(X, self.W)) 为了计算方便，我们将bias参数整合进了w参数之中。 使用和测试感知机分类器： 12345678910## Test it on OR dataX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])y = np.array([[-1], [1], [1], [1]])p = Perceptron(X.shape[1], alpha=0.1)p.fit(X,y,epochs=20)for (x,target) in zip(X,y): pred = p.predict(x) print(\"[INFO] data=&#123;&#125;, ground-truth=&#123;&#125;, pred=&#123;&#125;\".format(x, target[0], pred)) ​ 参考 《统计学习方法》——李航","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"基础知识","slug":"深度学习/基础知识","permalink":"http://blog.a-stack.com/categories/深度学习/基础知识/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"感知机","slug":"感知机","permalink":"http://blog.a-stack.com/tags/感知机/"}]},{"title":"读书笔记：《Practical Python and OpenCV》","slug":"读书笔记：《Practical-Python-and-OpenCV》","date":"2018-03-05T08:52:33.000Z","updated":"2018-05-15T09:14:33.694Z","comments":true,"path":"2018/03/05/读书笔记：《Practical-Python-and-OpenCV》/","link":"","permalink":"http://blog.a-stack.com/2018/03/05/读书笔记：《Practical-Python-and-OpenCV》/","excerpt":"","text":"《Practical Python and OpenCV》读书札记。 1. Load，Display and Save an Image123456789import cv2# Load the image and show some basic information on itimage = cv2.imread(\"Path/to/Image\")# Show the image and wait for a keypresscv2.imshow(\"Image\", image)cv2.waitKey(0)# Save the image -- OpenCV handles converting filetypes# automaticallycv2.imwrite(\"newimage.jpg\", image) 2. cv2图像上添加线条及形状 cv2.line(image, start_point, stop_point, color, thickness) cv2.rectangle(image,top_left, bottom_right, color, thickness) thickness 为负数，填充形状 12345678910111213141516171819202122# Initialize our canvas as a 300x300 with 3 channels,# Red, Green, and Blue, with a black backgroundcanvas = np.zeros((300, 300, 3), dtype = \"uint8\")# Draw a green line from the top-left corner of our canvas# to the bottom-rightgreen = (0, 255, 0)cv2.line(canvas, (0, 0), (300, 300), green)# Now, draw a 3 pixel thick red line from the top-right# corner to the bottom-leftred = (0, 0, 255)cv2.line(canvas, (300, 0), (0, 300), red, 3)# Draw a green 50x50 pixel square, starting at 10x10 and# ending at 60x60cv2.rectangle(canvas, (10, 10), (60, 60), green)# Draw another rectangle, this time we'll make it red and# 5 pixels thickcv2.rectangle(canvas, (50, 200), (200, 225), red, 5)# Let's draw one last rectangle: blue and filled inblue = (255, 0, 0)cv2.rectangle(canvas, (200, 50), (225, 125), blue, -1)cv2.imshow(\"Canvas\", canvas)cv2.waitKey(0) 添加圆形 cv2.circle(image, (centerX, centerY), r, color,thickness) 12345678910# Reset our canvas and draw a white circle at the center# of the canvas with increasing radii - from 25 pixels to# 150 pixelscanvas = np.zeros((300, 300, 3), dtype = \"uint8\")(centerX, centerY) = (canvas.shape[1] // 2, canvas.shape[0] // 2)white = (255, 255, 255)for r in range(0, 175, 25): cv2.circle(canvas, (centerX, centerY), r, white)cv2.imshow(\"Canvas\", canvas)cv2.waitKey(0) 123456789101112131415# Let's go crazy and draw 25 random circlesfor i in range(0, 25): # randomly generate a radius size between 5 and 200, # generate a random color, and then pick a random # point on our canvas where the circle will be drawn radius = np.random.randint(5, high = 200) color = np.random.randint(0, high = 256, size = (3,)).tolist() pt = np.random.randint(0, high = 300, size = (2,)) # draw our random circle cv2.circle(canvas, tuple(pt), radius, color, -1)# Show our masterpiececv2.imshow(\"Canvas\", canvas)cv2.waitKey(0) 3. Image Processing 平移 cv2.warpAffine() imutil 1234567def translate(image, x, y): # Define the translation matrix and perform the translation M = np.float32([[1, 0, x], [0, 1, y]]) shifted = cv2.warpAffine(image, M, (image.shape[1], image.shape[0])) # Return the translated image return shifted 旋转 cv2.getRotationMatrix2D 123456789101112131415def rotate(image, angle, center = None, scale = 1.0): # Grab the dimensions of the image (h, w) = image.shape[:2] # If the center is None, initialize it as the center of # the image if center is None: center = (w / 2, h / 2) # Perform the rotation M = cv2.getRotationMatrix2D(center, angle, scale) rotated = cv2.warpAffine(image, M, (w, h)) # Return the rotated image return rotated 缩放 cv2.resize 1234567891011121314151617181920212223242526def resize(image, width = None, height = None, inter = cv2.INTER_AREA): # initialize the dimensions of the image to be resized and grab the image size dim = None (h, w) = image.shape[:2] # if both the width and height are None, then return the original image if width is None and height is None: return image # check to see if the width is None if width is None: # calculate the ratio of the height and construct the dimensions r = height / float(h) dim = (int(w * r), height) # otherwise, the height is None else: # calculate the ratio of the width and construct the dimensions r = width / float(w) dim = (width, int(h * r)) # resize the image resized = cv2.resize(image, dim, interpolation = inter) # return the resized image return resized 反转 cv2.flip(image, num) num=1, 水平反转；num=0，垂直翻转；num为负，对角反转 bitwise 12bitwiseAnd = cv2.bitwise_and(rectangle, circle) cv2.imshow(\"AND\", bitwiseAnd) cv2.waitKey(0) MASKING 12345mask = np.zeros(image.shape[:2], dtype = \"uint8\") cv2.circle(mask, (cX, cY), 100, 255, -1) masked = cv2.bitwise_and(image, image, mask = mask) cv2.imshow(\"Mask\", mask) cv2.imshow(\"Mask Applied to Image\", masked) cv2.waitKey(0) 色彩空间变换 123456789# Load the image and show itimage = cv2.imread(args[\"image\"])# Convert the image to grayscalegray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)# Convert the image to the HSV (Hue, Saturation, Value)# color spaceshsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)# Convert the image to the L*a*b* color spaceslab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB) 平滑/模糊（blurring） 12345678# Averaging Blurringcv2.blur(image, (3, 3))# Gaussian Blurringcv2.GaussianBlur(image, (3, 3), 0)# Median Blurringcv2.medianBlur(image, 3)# Bilateral Blurringcv2.bilateralFilter(image, 5, 21, 21) Traditionally, the median blur method has been most effective when removing salt-and-pepper noise. Threshold 1234(T, thresh) = cv2.threshold(blurred, 155, 255, cv2.THRESH_BINARY) cv2.imshow(\"Threshold Binary\", thresh) (T, threshInv) = cv2.threshold(blurred, 155, 255, cv2. THRESH_BINARY_INV)cv2.bitwise_and(image, image, mask = threshInv) 边缘检测 1234image = cv2.imread(img)image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)image = cv2.GaussianBlur(image, (5,5), 0)canny = cv2.Canny(image, 30 ,150) 轮廓检测 12345678image = cv2.imread(img)image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)image = cv2.GaussianBlur(image, (11,11), 0)canny = cv2.Canny(image, 30 ,150)(_, cnts, _) = cv2.findContours(canny.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)print(\"I count &#123;&#125; coins in this image\".format(len(cnts)))coins = image.copy()cv2.drawContours(coins, cnts, -1, (0, 255, 255), 2) 从图像中将对象扣取 1234567891011121314151617# Now, let's loop over each contourfor (i, c) in enumerate(cnts): # We can compute the 'bounding box' for each contour, which is the rectangle that encloses the contour (x, y, w, h) = cv2.boundingRect(c) # Now that we have the contour, let's extract it using array slices print(\"Coin #&#123;&#125;\".format(i + 1)) coin = image[y:y + h, x:x + w] cv2.imshow(\"Coin\", coin) # Just for fun, let's construct a mask for the coin by finding The minumum enclosing circle of the contour mask = np.zeros(image.shape[:2], dtype = \"uint8\") ((centerX, centerY), radius) = cv2.minEnclosingCircle(c) cv2.circle(mask, (int(centerX), int(centerY)), int(radius), 255, -1) mask = mask[y:y + h, x:x + w] cv2.imshow(\"Masked Coin\", cv2.bitwise_and(coin, coin, mask = mask)) cv2.waitKey(0) ​ Tips 为解决python 2.7和python 3中print函数不兼容的问题，可通过导入如下命令解决在python 2.7环境运行python 3中print函数不兼容的问题： 1from __future__ import print_function OpenCV存储RGB信息采用的逆向存储方式，即为BGR; matplotlib.plot中为RGB np.random Method Desription rand(d0, d1, …, dn) Random values in a given shape. randn(d0, d1, …, dn) Return a sample (or samples) from the “standard normal” distribution. randint(low[, high, size, dtype]) Return random integers from low (inclusive) to high (exclusive). random_integers(low[, high, size]) Random integers of type np.int between low and high, inclusive. random_sample([size]) Return random floats in the half-open interval [0.0, 1.0). random([size]) Return random floats in the half-open interval [0.0, 1.0).产生随机矩阵，如random.random([2,3])产生一个2x3维的随机数 ranf([size]) Return random floats in the half-open interval [0.0, 1.0). sample([size]) Return random floats in the half-open interval [0.0, 1.0). choice(a[, size, replace, p]) Generates a random sample from a given 1-D array bytes(length) Return random bytes.","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/"},{"name":"机器视觉","slug":"读书笔记/机器视觉","permalink":"http://blog.a-stack.com/categories/读书笔记/机器视觉/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/tags/读书笔记/"},{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"}]},{"title":"深度学习基础之正则化","slug":"深度学习基础之正则化","date":"2018-03-04T07:35:01.000Z","updated":"2018-05-15T09:14:33.716Z","comments":true,"path":"2018/03/04/深度学习基础之正则化/","link":"","permalink":"http://blog.a-stack.com/2018/03/04/深度学习基础之正则化/","excerpt":"明确任务目标和评价准则对于模型的设计及优化至关重要，本文将总结常用的相关方法和模型性能评价准则。为了简化，本文将主要针对回归问题和分类问题分别予以归纳，其它问题可采取类似的方法及手段。","text":"明确任务目标和评价准则对于模型的设计及优化至关重要，本文将总结常用的相关方法和模型性能评价准则。为了简化，本文将主要针对回归问题和分类问题分别予以归纳，其它问题可采取类似的方法及手段。 @toc 过拟合一直是深度学习算法训练过程中必须要面对的一个难题，而正则化是降低模型过拟合的有效手段，这篇笔记用于归纳正则化的关键技术。 深度学习基础篇将从几个不同的层面来总结在过去一段时间对于深度学习关键技术的理解，通过知识体系的归纳了解知识体系的不足，提升对核心技术点的认识。所有系列文章将在未来一段时间内容随着掌握了解的深入迭代更新。目前主要希望对如下几个领域进行归纳汇总： 问题定义 目标及评估 数据准备与预处理 激活函数的归纳及总结 优化算法的归纳及总结 正则化与泛化性能 模型压缩 数据扩充 从欠拟合和过拟合聊起 重读 Andrew Wu的课程 “泛化指的是一个假设模型能够应用到新样本的能力。 解决过拟合方法一：尽量减少选取变量的数量 具体而言，我们可以人工检查每一项变量，并以此来确定哪些变量更为重要，然后，保留那些更为重要的特征变量。至于，哪些变量应该舍弃，我们以后在讨论，这会涉及到模型选择算法，这种算法是可以自动选择采用哪些特征变量，自动舍弃不需要的变量。这类做法非常有效，但是其缺点是当你舍弃一部分特征变量时，你也舍弃了问题中的一些信息。例如，也许所有的特征变量对于预测房价都是有用的，我们实际上并不想舍弃一些信息或者说舍弃这些特征变量。 方法二：正则化 正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。 这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。 批量正则化（Batch Normalization） \\hat x_i = \\frac{x_i -\\mu_\\beta}{\\sqrt{\\sigma_\\beta^2 + \\epsilon}}其中， \\mu_\\beta = \\frac{1}{M}\\sum_{i=1}^m{x_i} \\quad \\sigma_\\beta^2=\\frac{1}{m}\\sum_{i=1}^m{(x_i-\\mu_\\beta)^2}数据增强(Data Augmentation)目的：通过轻微的改变样本数据，增强模型输入数据的泛化特性，希望让模型学到更加鲁棒的特征表达方式。 通过一定程度的降低训练精度，提升泛化性能。 Figure 2.1: Left: A sample of 250 data points that follow a normal distribution exactly. Right: Adding a small amount of random “jitter” to the distribution. This type of data augmentation can increase the generalizability of our networks. 如图所示，现实生活中的数据分布很难像标准正态分布一样完美，左图通过在每个维度上增加一定的抖动来提升系统的泛化能力，当前系统仍然近似符合正态分布。 常用的数据放大手段有： 仿射变换； 旋转； 缩放； 裁剪（Shearing）,如random crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换； 水平/垂直翻转； 平移； 色彩空间变换(Color Jittering): 图像亮度、饱和度、对比度变化； PCA Jittering: 首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering； 噪声处理，高斯噪声，模糊处理 Keras中的数据增强Keras中提供了keras.preprocessing.image.ImageDataGenerator用于对图片进行预处理。 123456789101112131415161718192021222324252627282930313233# import the necessary packagesfrom keras.preprocessing.image import ImageDataGenerator from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img import numpy as np import argparse# load the input image, convert it to a NumPy array, and then # reshape it to have an extra dimension print(\"[INFO] loading example image...\")image = load_img(args[\"image\"]) image = img_to_array(image)image = np.expand_dims(image, axis=0)# construct the image generator for data augmentation then# initialize the total number of images generated thus far aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\"nearest\")total = 0# construct the actual Python generator print(\"[INFO] generating images...\")imageGen = aug.flow(image, batch_size=1, save_to_dir=args[\"output\"], save_prefix=args[\"prefix\"], save_format=\"jpg\")# loop over examples from our image data augmentation generator for image in imageGen:# increment our counter total += 1 # if we have reached 10 examples, break from the loop if total == 10: break #Training TimeH = model.fit_generator(aug.flow(trainX, trainY, batch_size=32), validation_data=(testX, testY), steps_per_epoch=len(trainX) // 32, epochs=100, verbose=1) ImageDataGenerator的常用参数： rotation_range : 随机旋转角度，比如30°； width_shift_range和height_shift_range，控制宽度和高度的偏移比例，例子中0.1=10%的变动幅度； shear_range: zoom_range：[1-zoom_range, 1+zoom_range]范围‘ horizontal_flip: flow方法：flow(self, X, y, batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix=&#39;&#39;, save_format=&#39;png&#39;), 接收numpy数组和标签为参数，生成经过数据增强的batch数据，并在一个无限循环中不断的返回batch_size的数据 参考 Keras文档 deep learning for computer vision with python Machine Learning Lessons by Andrew","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"基础知识","slug":"深度学习/基础知识","permalink":"http://blog.a-stack.com/categories/深度学习/基础知识/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"正则化","slug":"正则化","permalink":"http://blog.a-stack.com/tags/正则化/"}]},{"title":"Deep Learning for Computer Vision","slug":"Deep-Learning-for-Computer-Vision","date":"2018-03-03T14:12:49.000Z","updated":"2018-05-23T12:44:00.535Z","comments":true,"path":"2018/03/03/Deep-Learning-for-Computer-Vision/","link":"","permalink":"http://blog.a-stack.com/2018/03/03/Deep-Learning-for-Computer-Vision/","excerpt":"","text":"本文记录深度学习书籍《Deep Learning for Computer Vision with Python》的读书笔记。 @toc 背景 深度学习拥有60多年历史，虽然曾经采用过不同的名称和不同的主导技术：“deep learning” has existed since the 1940s undergoing various name changes, including cybernetics, connectionism, and the most familiar, Artificial Neural Networks (ANNs). 神经网络的普适定律：Further research demonstrated that neural networks are universal approximators , capable of approximating any continuous function (but placing no guarantee on whether or not the network can actually learn the parameters required to represent a function). Classic machine learning algorithms for unsupervised learning include Principle Component Analysis (PCA) and k-means clustering. Specific to neural networks, we see Autoencoders, Self-Organizing Maps (SOMs), and Adaptive Resonance Theory applied to unsupervised learning. Popular choices for semisupervised learning include label spreading, label propagation, ladder networks, and co-learning/co-training. Image and Pixels Pixels are represented in two ways: Grayscale: Each pixel is a scalar value between 0 and 255.（0 for “Black” and 255 for “White”），0—&gt;255 dark —&gt; light Color: RGB color space, (R,G,B), Each Red, Green, and Blue channel can have values defined in the range [0,255] for a total of 256 “shades”, where 0 indicates no representation and 255 demonstrates full representation. Given that the pixel value only needs to be in the range [0,255], we normally use 8-bit unsigned integers to represent the intensity. Images as Numpy Arrays (height, width, depth) 表示 height 排第一的主要原因是由于矩阵表示形式中，一般把行放在前面，而图像中height大小表征了行的数目。 123456import cv2image = cv2.imread(\"example.png\") print(image.shape)cv2.imshow(\"Image\", image)cv2.waitKey(0)## Access an individual pixel value(b, g, r) = image[20, 100] # accesses pixel at x=100, y=20 取像素y在x前面，还是由于矩阵的表示形式； RGB顺序反的，这是由于OpenCV历史原因导致的表示形式差异: Because the BGR ordering was popular among camera manufacturers and other software developers at the time. Othersaspect ratio: the ratio of the width to the height of the image. 神经网络模型一般都是固定输入，比如32×32, 64×64, 224×224, 227×227, 256×256, and 299×299. 需要对不同大小的图像进行reshape操作，For some datasets you can simply ignore the aspect ratio and squish, distort, and compress your images prior to feeding them through your network. On other datasets, it’s advantageous to preprocess them further by resizing along the shortest dimension and then cropping the center. Image Classification图像分类和图像理解是当今技术视觉领域最火的课题。 定义图像分类： the task of assigning a label to an image from a predefined set ofcategories. 图像分类的过程是学习图片中的“underlying patterns” Semantic Gap： the difference between how a human perceives the contents of an image versus how an image can be represented in a way a computer can understand the process. 挑战 数据集(TODO) MNIST 目标： 完成0-9手写字符的识别 说明： NIST代表National Institute ofStandards and Technology， M代表Modified 深度学习的Hello World 包含60,000训练样本，10,000测试样本，每个样本为28x28的灰度图像 目前准确度： &gt;99% 获取地址： http://yann.lecun.com/exdb/mnist/ Fashion-MNIST 目标： 完成10种不同衣服的识别 说明： 根据MNIST设计的新的数据集，难度比MNIST略高 包含60,000训练样本，10,000测试样本，每个样本为28x28的灰度图像 目前准确度： &gt;95% 获取地址： https://github.com/zalandoresearch/fashion-mnist CIFAR-10 Animals： Dogs，Cat， Pandas Flowers-17 CALtECH-101 Tiny ImageNet 200 Adience ImageNet 表情识别(是否笑脸) 说明： 共计13165张灰度图片，每张图片大小为64x64 分为笑脸和非笑脸两类，其中笑脸3690张，非笑脸9475张（数据不平衡） 获取地址：https://github.com/hromi/SMILEsmileD 另外fer2013提供了更多表情的训练用数据集 性别和年龄数据集 IMDB-WIKI – 500k+ face images with age and gender labels 获取地址： https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/ Indoor CVPR Stanford Cars … 神经网络基础优化算法（TODO，整合到单独Note） Chapter 8 Regularization (TODO, 整合到单独Note) Chapter 9 chapter 10,激活函数，perception 为什么验证损失函数值有时候小于训练损失函数这可能是有几方面原因导致的，或多方面原因综合作用的结果，主要的原因包括： 训练集和验证集分布不均，导致训练集数据难度大，验证集简单数据分布比例大； 数据放大本身形成了一种规则化，降低了训练集的训练结果；（这本身是规则化的目标，降低在训练集的表现，提升泛化性能） 训练时间或轮数不够； 关于学习率 keras中提供了decay参数来调节学习率的变化情况： 1opt = SGD(lr=0.01, decay=0.01 / 40, momentum=0.9, nesterov=True) 使用公式： \\alpha_{e+1} = \\alpha_e \\times 1 /(1+\\gamma * e) 另一种学习率为阶梯学习率：ctrl + c Keras提供一个类：LearningrateScheduler来配置自定义的学习率函数 比如： \\alpha_{E+1} = \\alpha_1 \\times F^{(1+E)/D}1234567891011121314def step_decay(epoch): # initialize the base initial learning rate, drop factor, and epochs to drop every initAlpha = 0.01 factor = 0.25 dropEvery = 5 # compute learning rate for the current epoch alpha = initAlpha * (factor ** np.floor((1 + epoch) / dropEvery)) # return the learning rate return float(alpha) ##定义callbackcallbacks = [LearningRateScheduler(step_decay)] 当定义了学习率之后，SGD中声明的配置信息将被忽略 网络模型VGG 所有的卷积层使用同一种卷积核：3X3 堆积多个CONV=&gt;RELU层再进行一次POOL操作 MNISTResearchers tend to use the MNIST dataset as a benchmark to evaluate new classification algorithms. If their methods cannot obtain &gt; 95% classification accuracy, then there is either a flaw in (1) the logic of the algorithm or (2) the implementation itself. Case Study 使用OpenCV的Haar cascade 算法进行人脸检测，提取人脸的ROI(Region of intrest), 通过一个卷积神经网络进行表情识别； 可以结合Github开源的表情识别代码一起研究 路径处理 os.path.sep： 提取路径分隔符 数据不平衡的处理，可以考虑不同分类的权重，在训练时通过赋权调整平衡性，代码如下： 12345678# Handle data imbalance# account for skew in the labeled dataclassTotals = labels.sum(axis=0)classWeight = classTotals.max() / classTotals## When trainingH = model.fit(trainX, trainY, validation_data=(testX, testY), class_weight=classWeight, batch_size=64, epochs=15, verbose=1) 手写字的预处理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def preprocess(image, width, height): #grap the dimensions of the image, then initialize the padding values (h, w) = image.shape[:2] #if width greater than height, resize along the width if w &gt; h: image = imutils.resize(iamge, width=width) else: image = imutils.resize(image, height=height) #padding values for w and h to obtain the target dimensions padW = int((width - image.shape[1])/2.0) padH = int((height - image.shape[0])/2.0) #pad the image then apply one more resizing to handle any rounding issues image = cv2.copyMakeBorder(image, padH, padH, padW, padW, cv2.BORDER_REPLICATE) iamge = cv2.resize(image, (width, height)) return imageimage = cv2.imread(img)gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.copyMakeBorder(gray, 20,20,20,20, cv2.BORDER_REPLICATE)# threshold the image to reveal the digitsthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]#find contours in the image, keeping only the four largest onescnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if imutils.is_cv2() else cnts[1]cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:4]cnts = contours.sort_contours(cnts)[0]# initialize the output image as a \"grayscale\" image with 3# channels along with the output predictionsoutput = cv2.merge([gray] * 3)for c in cnts: # compute the bounding box for the contour then extract the # digit (x, y, w, h) = cv2.boundingRect(c) roi = gray[y - 5:y + h + 5, x - 5:x + w + 5] roi = preprocess(roi, 28, 28) roi = np.expand_dims(img_to_array(roi), axis=0) / 255.0 #pred = model.predict(roi).argmax(axis=1)[0] + 1 #predictions.append(str(pred)) # draw the prediction on the output image cv2.rectangle(output, (x - 2, y - 2),(x + w + 4, y + h + 4), (0, 255, 0), 1) #cv2.putText(output, str(pred), (x - 5, y - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 0), 2)# show the output image#print(\"[INFO] captcha: &#123;&#125;\".format(\"\".join(predictions)))plt.imshow(output)#cv2.waitKey() Useful Functions图像预处理及加载模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596'''File1: Preprocessor'''import cv2class SimplePreporcessor: def __init__(self, width, height, inter=cv2.INTER_AREA)： # store the target image width, height, and interpolation method used when resizing self.width = width self.height = height self.inter = inter def preprocess(self, image): # resize the image to a fixed size, ignoring the aspect ratio return cv2.resize(image, (self.width, self.height), interpolation = self.inter)'''Data Loader'''# import the necessary packages import numpy as np import cv2 import osclass SimpleDatasetLoader: def __init__(self, preprocessors=None): self.preprocessors = preprocessors # if the preprocessors are None, initialize them as an empty list if self.preprocessors is None: self.preprocessors = [] def load(self, imagePaths, verbose =-1): data = [] labels = [] for (i, imagePath) in enumerate(imagePaths): # load the image and extract the class label assuming # # that our path has the following format: # # /path/to/dataset/&#123;class&#125;/&#123;image&#125;.jpg image = cv2.imread(imagePath) label = imagePath.split(os.path.sep)[-2] # check to see if our preprocessors are not None if self.preprocessors is not None: for p in self.preprocessors: image = p.preprocess(image) data.append(image) labels.append(label) # show an update every ‘verbose‘ images if verbose &gt;0 and i &gt;0 and (i+1)%verbose == 0: print(\"[INFON] process &#123;&#125;/&#123;&#125;\").format(i+1,len(imagePaths)) # return a tuple of the data and labels return (np.array(data), np.array(labels))'''Main'''# import the necessary packagesfrom sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import classification_reportfrom imutils import paths import argparse# construct the argument parse and parse the arguments ap = argparse.ArgumentParser()ap.add_argument(\"-d\", \"--dataset\", required=True, help=\"path to input dataset\")ap.add_argument(\"-k\", \"--neighbors\", type=int, default=1, help=\"# of nearest neighbors for classification\") ap.add_argument(\"-j\", \"--jobs\", type=int, default=-1, help=\"# of jobs for k-NN distance (-1 uses all available cores)\")args = vars(ap.parse_args())print(\"[INFO] loading images ...\")imagePaths = list(paths.list_images(args[\"dataset\"]))# initialize the image preprocessor, load the dataset from disk,# and reshape the data matrixsp = SimplePreporcessor(32, 32)sdl = SimpleDatasetLoader(preprocessors=[sp])(data, labels) = sdl.load(imagePaths, verbose=500)#flatten for use in KNNdata = data.reshape((data.shape[0],32*32*3))print(\"[INFO] feature matrix: &#123;:.1f&#125;MB\").format(data.nbytes/(1024*1000.0))# encode the labels as integersle = LabelEncoder()labels = le.fit_transform(labels)(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)# train and evaluate a kNN classifier on raw pixel intensitiesprint(\"[INFO] evaluate kNN classifier ...\")model = KNeighborsClassifier(n_neighbors=args[\"neighbors\"]), n_jobs=args[\"jobs\"])model.fit(trainX, trainY)print(classification_report(testY, model.predict(testX),target_names==le.classes_)) sklearn.metrics.classification_reportsklearn中的classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息。主要参数: y_true：1维数组，或标签指示器数组/稀疏矩阵，目标值。 y_pred：1维数组，或标签指示器数组/稀疏矩阵，分类器返回的估计值。 labels：array，shape = [n_labels]，报表中包含的标签索引的可选列表。 target_names：字符串列表，与标签匹配的可选显示名称（相同顺序）。 sample_weight：类似于shape = [n_samples]的数组，可选项，样本权重。 digits：int，输出浮点值的位数． 12345from sklearn.metrics import classification_reporty_true = [0, 1, 2, 2, 2]y_pred = [0, 0, 2, 2, 1]target_names = ['class 0', 'class 1', 'class 2']print(classification_report(y_true, y_pred, target_names=target_names)) opencv 给图像添加描述123import cv2# draw the label with the highest score on the image as our # predictioncv2.putText(orig, \"Label: &#123;&#125;\".format(labels[np.argmax(scores)]), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2) Keras中的Checkpoint机制12345678910111213from keras.callbacks import ModelCheckpoint# construct the callback to save only the *best* model to disk# based on the validation lossfname = os.path.sep.join([args[\"weights\"], \"weights-&#123;epoch:03d&#125;-&#123;val_loss:.4f&#125;.hdf5\"])checkpoint = ModelCheckpoint(fname, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)callbacks = [checkpoint]print(\"[INFO] training network...\")H = model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=64, epochs=40, callbacks=callbacks, verbose=2) 参数： filename：字符串，保存模型的路径 monitor：需要监视的值 verbose：信息展示模式，0或1 save_best_only：当设置为True时，将只保存在验证集上性能最好的模型 mode：‘auto’，‘min’，‘max’之一，在save_best_only=True时决定性能最佳模型的评判准则，例如，当监测值为val_acc时，模式应为max，当检测值为val_loss时，模式应为min。在auto模式下，评价准则由被监测值的名字自动推断。 save_weights_only：若设置为True，则只保存模型权重，否则将保存整个模型（包括模型结构，配置信息等） period：CheckPoint之间的间隔的epoch数 可以monitor loss值也可以是val_acc，train_loss, train_acc; 更多内容参见：http://keras-cn.readthedocs.io/en/latest/other/callbacks/ EarlyStopping1keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto') 当监测值不再改善时，该回调函数将中止训练 参数 monitor：需要监视的量 patience：当early stop被激活（如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。 verbose：信息展示模式 mode：‘auto’，‘min’，‘max’之一，在min模式下，如果检测值停止下降则中止训练。在max模式下，当检测值不再上升则停止训练。 基于keras callback实现训练过程监控1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# import the necessary packagesfrom keras.callbacks import BaseLoggerimport matplotlib.pyplot as pltimport numpy as npimport jsonimport osclass TrainingMonitor(BaseLogger): def __init__(self, figPath, jsonPath=None, startAt=0): # store the output path for the figure, the path to the JSON serialized file, and the starting epoch super(TrainingMonitor, self).__init__() self.figPath = figPath self.jsonPath = jsonPath self.startAt = startAt def on_train_begin(self, logs=&#123;&#125;): # initialize the history dictionary self.H = &#123;&#125; # if the JSON history path exists, load the training history if self.jsonPath is not None: if os.path.exists(self.jsonPath): self.H = json.loads(open(self.jsonPath).read()) # check to see if a starting epoch was supplied if self.startAt &gt; 0: # loop over the entries in the history log and # trim any entries that are past the starting # epoch for k in self.H.keys(): self.H[k] = self.H[k][:self.startAt] def on_epoch_end(self, epoch, logs=&#123;&#125;): # loop over the logs and update the loss, accuracy, etc. # for the entire training process for (k, v) in logs.items(): l = self.H.get(k, []) l.append(v) self.H[k] = l # check to see if the training history should be serialized # to file if self.jsonPath is not None: f = open(self.jsonPath, \"w\") f.write(json.dumps(self.H)) f.close() # ensure at least two epochs have passed before plotting # (epoch starts at zero) if len(self.H[\"loss\"]) &gt; 1: #plot the training loss and accuracy N = np.arange(0, len(self.H[\"loss\"])) plt.style.use(\"ggplot\") plt.figure() plt.plot(N, self.H[\"loss\"], label=\"train_loss\") plt.plot(N, self.H[\"val_loss\"], label=\"val_loss\") plt.plot(N, self.H[\"acc\"], label=\"train_acc\") plt.plot(N, self.H[\"val_acc\"], label=\"val_acc\") plt.title(\"Training Loss and Accuracy [Epoch &#123;&#125;]\".format(len(self.H[\"loss\"]))) plt.xlabel(\"Epoch #\") plt.ylabel(\"Loss/Accuracy\") plt.legend() #save the figure plt.savefig(self.figPath) plt.close() figPath: The path to the output plot that we can use to visualize loss and accuracy over time. jsonPath: An optional path used to serialize the loss and accuracy values as a JSON file. This path is useful if you want to use the training history to create custom plots of your own. startAt: This is the starting epoch that training is resumed at when using ctrl + c training. 参考","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/"},{"name":"机器视觉","slug":"读书笔记/机器视觉","permalink":"http://blog.a-stack.com/categories/读书笔记/机器视觉/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/tags/读书笔记/"},{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"机器视觉","slug":"机器视觉","permalink":"http://blog.a-stack.com/tags/机器视觉/"}]},{"title":"深度学习基础之目标及评估","slug":"深度学习基础之目标及评估","date":"2018-03-02T07:35:01.000Z","updated":"2018-06-26T05:47:28.380Z","comments":true,"path":"2018/03/02/深度学习基础之目标及评估/","link":"","permalink":"http://blog.a-stack.com/2018/03/02/深度学习基础之目标及评估/","excerpt":"明确任务目标和评价准则对于模型的设计及优化至关重要，本文将总结常用的相关方法和模型性能评价准则。为了简化，本文将主要针对回归问题和分类问题分别予以归纳，其它问题可采取类似的方法及手段。","text":"明确任务目标和评价准则对于模型的设计及优化至关重要，本文将总结常用的相关方法和模型性能评价准则。为了简化，本文将主要针对回归问题和分类问题分别予以归纳，其它问题可采取类似的方法及手段。 @toc 深度学习基础篇将从几个不同的层面来总结在过去一段时间对于深度学习关键技术的理解，通过知识体系的归纳了解知识体系的不足，提升对核心技术点的认识。所有系列文章将在未来一段时间内容随着掌握了解的深入迭代更新。目前主要希望对如下几个领域进行归纳汇总： 问题定义 目标及评估 数据准备与预处理 激活函数的归纳及总结 优化算法的归纳及总结 正则化与泛化性能 模型压缩 数据扩充 概述机器学习中的所有算法都依赖于最小化或最大化某一个函数，我们称之为“目标函数”。最小化的这组函数被称为“损失函数”。损失函数是衡量预测模型预测期望结果表现的指标。寻找函数最小值的最常用方法是“梯度下降”。本文主要对相关的目标函数/损失函数的知识点进行归纳和总结。 没有一个损失函数可以适用于所有类型的数据。损失函数的选择取决于许多因素，包括是否有离群点，机器学习算法的选择，运行梯度下降的时间效率，是否易于找到函数的导数，以及预测结果的置信度。这个博客的目的是帮助你了解不同的损失函数。 损失函数可以大致分为两类：分类损失（ClassificationLoss）和回归损失（Regression Loss）。 分类问题基础知识分类阈值的概念：将预测结果划分到每个类别的判定阈值，比如二分类问题中取为0.5。 阳性与阴性如下图所示的混淆矩阵 混淆矩阵 (confusion matrix)一种 NxN 表格，用于总结分类模型的预测成效；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在二元分类问题中，N=2。 TP: 真正例，是指模型将正类别样本正确地预测为正类别。 FP：假正例，是指模型将负类别样本错误地预测为正类别， FN：假负例，是指模型将正类别样本错误地预测为负类别。 TN：真负例，是指模型将负类别样本正确地预测为负类别。 准确率准确率是一个用于评估分类模型的指标。通俗来说，准确率是指我们的模型预测正确的结果所占的比例。准确率的定义如下： Accuracy= \\frac{TP+TN}{TP+FP+TN+FN} 从识别结果看，其中识别正确的占比为多少 当使用分类不平衡的数据集（比如正类别标签和负类别标签的数量之间存在明显差异）时，单单准确率一项并不能反映全面情况。 精确率 Precision = \\frac{TP}{TP+FP} 在被识别为正类别的样本中，确实为正类别的比例是多少？ 召回率 Recall = \\frac{TP}{TP+FN} 从所有正确目标值看，有多少目标被正确的识别出来了 精确率和召回率是两个trade-off的指标。 F1指标 F1 = 2 \\frac{Recall*Precision}{Recall + Precision} =\\frac{TP}{TP+(FN+FP)/2}ROCROC 曲线（接收者操作特征曲线）是一种显示分类模型在所有分类阈值下的效果的图表。该曲线绘制了以下两个参数： 真正例率 假正例率 真正例率 (TPR) 是召回率的同义词，因此定义如下： $TPR = \\frac{TP} {TP + FN}$ 假正例率 (FPR) 的定义如下： $FPR = \\frac{FP} {FP + TN}$ ROC 曲线用于绘制采用不同分类阈值时的 TPR 与 FPR。降低分类阈值会导致将更多样本归为正类别，从而增加假正例和真正例的个数。下图显示了一个典型的 ROC 曲线。 曲线越靠近左上方，效果约好，代表正确分类比例越高； 四个关键坐标点： (0,0)： TP=FP=0，全部预测结果为反例； (1,1)： TN=FN=0, 全部预测结果为正例； (0,1): FP=FN=0, 全部正确分类样本； (1,0): TP=TN=0, 全部错误分类样本； 曲线下面积：ROC 曲线下面积曲线下面积表示“ROC 曲线下面积”。也就是说，曲线下面积测量的是从 (0,0) 到 (1,1) 之间整个 ROC 曲线以下的整个二维面积（参考积分学）。 曲线下面积对所有可能的分类阈值的效果进行综合衡量。曲线下面积的一种解读方式是看作模型将某个随机正类别样本排列在某个随机负类别样本之上的概率。以下面的样本为例，逻辑回归预测从左到右以升序排列。 曲线下面积表示随机正类别（绿色）样本位于随机负类别（红色）样本右侧的概率。 曲线下面积的取值范围为 0-1。预测结果 100% 错误的模型的曲线下面积为 0.0；而预测结果 100% 正确的模型的曲线下面积为 1.0。 曲线下面积因以下两个原因而比较实用： 曲线下面积的尺度不变。它测量预测的排名情况，而不是测量其绝对值。 曲线下面积的分类阈值不变。它测量模型预测的质量，而不考虑所选的分类阈值。 不过，这两个原因都有各自的局限性，这可能会导致曲线下面积在某些用例中不太实用： 并非总是希望尺度不变。 例如，有时我们非常需要被良好校准的概率输出，而曲线下面积无法告诉我们这一结果。 并非总是希望分类阈值不变。 在假负例与假正例的代价存在较大差异的情况下，尽量减少一种类型的分类错误可能至关重要。例如，在进行垃圾邮件检测时，您可能希望优先考虑尽量减少假正例（即使这会导致假负例大幅增加）。对于此类优化，曲线下面积并非一个实用的指标。 预测偏差预测偏差指的是这两个平均值之间的差值。即：预测偏差预测平均值数据集中相应标签的平均值 造成预测偏差的可能原因包括： 特征集不完整 数据集混乱 模型实现流水线中有错误？ 训练样本有偏差 正则化过强 回归损失函数均方误差（Mean Square Error，MSE）均方误差（MSE）是最常用的回归损失函数，也叫L2 Loss。MSE是目标变量与预测值之间距离平方之和。 MSE = \\frac{\\sum_{i=1}^n (\\hat y - y_{true})^2}{n}下面是一个MSE函数的图，其中真实目标值为100，预测值在-10,000至10,000之间。预测值（X轴）=100时，MSE损失（Y轴）达到其最小值。损失范围为0至∞。 平均绝对误差（Mean Absolute Error， L1 Loss）平均绝对误差（MAE）是另一种用于回归模型的损失函数，也叫做L1 Loss。MAE是目标变量和预测变量之间差异绝对值之和。因此，它在一组预测中衡量误差的平均大小，而不考虑误差的方向。（如果我们也考虑方向，那将被称为平均偏差（Mean Bias Error, MBE），它是残差或误差之和）。损失范围也是0到∞。 MAE = \\frac{\\sum_{i=1}^n |\\hat y - y_{true}|}{n} MSE vs MAE 简而言之， 使用平方误差更容易求解，但使用绝对误差对离群点更加鲁棒。 每当我们训练机器学习模型时，我们的目标就是找到最小化损失函数的点。当然，当预测值正好等于真实值时，这两个损失函数都达到最小值。下面让我们快速过一遍两个损失函数的Python代码。我们可以编写自己的函数或使用sklearn的内置度量函数： 1234567891011# true：真正的目标变量数组# pred：预测数组def mse(true, pred): return np.sum(((true – pred)**2)) def mae(true, pred): return np.sum(np.abs(true – pred))# 也可以在sklearn中使用from sklearn.metrics import mean_squared_errorfrom sklearn.metrics import mean_absolute_error 让我们来看看两个例子的MAE值和RMSE值（RMSE，Root Mean Square Error，均方根误差，它只是MSE的平方根，使其与MAE的数值范围相同）。在第一个例子中，预测值接近真实值，观测值之间误差的方差较小。第二个例子中，有一个异常观测值，误差很高。 我们从中观察到什么？我们该如何选择使用哪种损失函数？由于MSE对误差（e）进行平方操作（y - y_predicted = e），如果e&gt;1，误差的值会增加很多。如果我们的数据中有一个离群点，e的值将会很高，将会远远大于|e|。这将使得和以MAE为损失的模型相比，以MSE为损失的模型会赋予更高的权重给离群点。在上面的第二个例子中，以RMSE为损失的模型将被调整以最小化这个离群数据点，但是却是以牺牲其他正常数据点的预测效果为代价，这最终会降低模型的整体性能。 MAE损失适用于训练数据被离群点损坏的时候（即，在训练数据而非测试数据中，我们错误地获得了不切实际的过大正值或负值）。 直观来说，我们可以像这样考虑：对所有的观测数据，如果我们只给一个预测结果来最小化MSE，那么该预测值应该是所有目标值的均值。但是如果我们试图最小化MAE，那么这个预测就是所有目标值的中位数。我们知道中位数对于离群点比平均值更鲁棒，这使得MAE比MSE更加鲁棒。 使用MAE损失（特别是对于神经网络）的一个大问题是它的梯度始终是相同的，这意味着即使对于小的损失值，其梯度也是大的。这对模型的学习可不好。为了解决这个问题，我们可以使用随着接近最小值而减小的动态学习率。MSE在这种情况下的表现很好，即使采用固定的学习率也会收敛。MSE损失的梯度在损失值较高时会比较大，随着损失接近0时而下降，从而使其在训练结束时更加精确（参见下图）。 如何选择？如果离群点是会影响业务、而且是应该被检测到的异常值，那么我们应该使用MSE。另一方面，如果我们认为离群点仅仅代表数据损坏，那么我们应该选择MAE作为损失。 L1损失对异常值更加稳健，但其导数并不连续，因此求解效率很低。L2损失对异常值敏感，但给出了更稳定的闭式解（closedform solution）（通过将其导数设置为0）。 两种损失函数的问题：可能会出现这样的情况，即任何一种损失函数都不能给出理想的预测。例如，如果我们数据中90％的观测数据的真实目标值是150，其余10％的真实目标值在0-30之间。那么，一个以MAE为损失的模型可能对所有观测数据都预测为150，而忽略10％的离群情况，因为它会尝试去接近中值。同样地，以MSE为损失的模型会给出许多范围在0到30的预测，因为它被离群点弄糊涂了。这两种结果在许多业务中都是不可取的。 平滑的平均绝对误差函数（Huber Loss）Huber Loss对数据离群点的敏感度低于平方误差损失。它在0处也可导。基本上它是绝对误差，当误差很小时，误差是二次形式的。误差何时需要变成二次形式取决于一个超参数，(delta)，该超参数可以进行微调。当 𝛿 ~ 0时，Huber Loss接近MAE，当 𝛿 ~ ∞（很大的数）时，Huber Loss接近MSE。 delta的选择非常重要，因为它决定了你认为什么数据是离群点。大于delta的残差用L1最小化（对较大的离群点较不敏感），而小于delta的残差则可以“很合适地”用L2最小化。 为什么使用Huber Loss？使用MAE训练神经网络的一个大问题是经常会遇到很大的梯度，使用梯度下降时可能导致训练结束时错过最小值。对于MSE，梯度会随着损失接近最小值而降低，从而使其更加精确。 在这种情况下，HuberLoss可能会非常有用，因为它会使最小值附近弯曲，从而降低梯度。另外它比MSE对异常值更鲁棒。因此，它结合了MSE和MAE的优良特性。但是，HuberLoss的问题是我们可能需要迭代地训练超参数delta。 Log-Cosh LossLog-cosh是用于回归任务的另一种损失函数，它比L2更加平滑。Log-cosh是预测误差的双曲余弦的对数。 L(\\hat y,y_{true}) = \\sum^n_{i=1} log(cosh(y_{true}-\\hat y)) 优点： log(cosh(x))对于小的x来说，其大约等于 (x ** 2) / 2，而对于大的x来说，其大约等于 abs(x) -log(2)。这意味着logcosh的作用大部分与均方误差一样，但不会受到偶尔出现的极端不正确预测的强烈影响。它具有Huber Loss的所有优点，和Huber Loss不同之处在于，其处处二次可导。 为什么我们需要二阶导数？许多机器学习模型的实现（如XGBoost）使用牛顿方法来寻找最优解，这就是为什么需要二阶导数（Hessian）的原因。对于像XGBoost这样的机器学习框架，二阶可导函数更有利。 但Log-chshLoss并不完美。它仍然存在梯度和Hessian问题，对于误差很大的预测，其梯度和hessian是恒定的。因此会导致XGBoost中没有分裂。 Huber和Log-cosh损失函数的Python代码： 12345678910111213def sm_mae(true, pred, delta): \"\"\" true: array of true values pred: array of predicted values returns: smoothed mean absolute error loss \"\"\" loss = np.where(np.abs(true-pred) &lt; delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2)) return np.sum(loss)def logcosh(true, pred): loss = np.log(np.cosh(pred - true)) return np.sum(loss) Quantile Loss(分位数损失)在大多数真实预测问题中，我们通常想了解我们预测的不确定性。了解预测值的范围而不仅仅是单一的预测点可以显着改善许多业务问题的决策过程。 当我们有兴趣预测一个区间而不仅仅是预测一个点时，Quantile Loss函数就很有用。最小二乘回归的预测区间是基于这样一个假设：残差（y -y_hat）在独立变量的值之间具有不变的方差。我们不能相信线性回归模型，因为它违反了这一假设。当然，我们也不能仅仅认为这种情况一般使用非线性函数或基于树的模型就可以更好地建模，而简单地抛弃拟合线性回归模型作为基线的想法。这时，Quantile Loss就派上用场了。因为基于Quantile Loss的回归模型可以提供合理的预测区间，即使是对于具有非常数方差或非正态分布的残差亦是如此。 让我们看一个有效的例子，以更好地理解为什么基于QuantileLoss的回归模型对异方差数据表现良好。 Quantile 回归 vs 普通最小二乘（Ordinary Least Square, OLS）回归 橙线表示两种情况下的OLS估计 基于Quantile回归的目的是，在给定预测变量的某些值时，估计因变量的条件“分位数”。QuantileLoss实际上只是MAE的扩展形式（当分位数是第50个百分位时，Quantile Loss退化为MAE）。 QuantileLoss的思想是根据我们是打算给正误差还是负误差更多的值来选择分位数数值。损失函数根据所选quantile(γ)的值对高估和低估的预测值给予不同的惩罚值。举个例子，γ= 0.25的Quantile Loss函数给高估的预测值更多的惩罚，并试图使预测值略低于中位数。 γ 是给定的分位数，其值介于0和1之间。 我们也可以使用这个损失函数来计算神经网络或基于树的模型的预测区间。下图是sklearn实现的梯度提升树回归。 上图显示的是sklearn库的GradientBoostingRegression中的quantile loss函数计算的90％预测区间。上限的计算使用了γ = 0.95，下限则是使用了γ = 0.05。 几种算法的比较“Gradient boosting machines, a tutorial”中提供了一个很好的比较研究。为了演示上述所有的损失函数的性质，研究人员创造了一个人工数据集，数据集从sinc(x)函数中采样，其中加入了两种人造模拟噪声：高斯噪声分量和脉冲噪声分量。脉冲噪声项是用来展示结果的鲁棒效果的。以下是使用不同损失函数来拟合 BM（Gradient Boosting Machine, 梯度提升回归）的结果。 结论 以MAE为损失的模型预测较少受到脉冲噪声的影响，而以MSE为损失的模型的预测由于脉冲噪声造成的数据偏离而略有偏差。 以Huber Loss为损失函数的模型，其预测对所选的超参数不太敏感。 Quantile Loss对相应的置信水平给出了很好的估计。 Rank-1 和 Rank-5最初源自Imagenet中设计的一个分类准确率的评价准则。 Rank-1比较容易理解，就是精确匹配；Rank-5中，将所有的预测概率排序，只有目标的真实分类在Top 5预测之中就算该次分类正确预测了分类目标，主要针对现实中提供的分类照片不够理想：1）比如照片中同时存在多个目标对象；2）分类目标特别庞大，每个对象都存在多种分类目标(比如一只阿里斯加犬的照片，既可以被分类为狗，也可以被分类为阿拉斯加)，3）分类目标中没有任何明确与对象相关的内容（比如图像为一张小汽车照片，提供的分类为卡车、飞机、摩托车、货车、…）。 123456789101112131415161718192021222324252627def rank5_accuracy(preds, labels): # initialize the rank-1 and rank-5 accuracies rank1 = 0 rank5 = 0 # loop over the predictions and ground-truth labels for (p, gt) in zip(preds, labels): # sort the probabilities by their index in descending # order so that the more confident guesses are at the # front of the list p = np.argsort(p)[::-1] # check if the ground-truth label is in the top-5 # predictions if gt in p[:5]: rank5 += 1 # check to see if the ground-truth is the #1 prediction if gt == p[0]: rank1 += 1 # compute the final rank-1 and rank-5 accuracies rank1 /= float(len(preds)) rank5 /= float(len(preds)) # return a tuple of the rank-1 and rank-5 accuracies return (rank1, rank5) 参考 Keras文档 Google 机器学习速成课程 5 Regression Loss Functions All Machine Learners Should Know ​","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"基础知识","slug":"深度学习/基础知识","permalink":"http://blog.a-stack.com/categories/深度学习/基础知识/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"预处理","slug":"预处理","permalink":"http://blog.a-stack.com/tags/预处理/"}]},{"title":"深度学习基础之模型压缩","slug":"深度学习基础之模型压缩","date":"2018-03-01T08:02:01.000Z","updated":"2018-05-15T09:14:33.792Z","comments":true,"path":"2018/03/01/深度学习基础之模型压缩/","link":"","permalink":"http://blog.a-stack.com/2018/03/01/深度学习基础之模型压缩/","excerpt":"","text":"深度学习中模型压缩关键技术总结。 深度学习基础篇将从几个不同的层面来总结在过去一段时间对于深度学习关键技术的理解，通过知识体系的归纳了解知识体系的不足，提升对核心技术点的认识。所有系列文章将在未来一段时间内容随着掌握了解的深入迭代更新。目前主要希望对如下几个领域进行归纳汇总： 问题定义 目标及评估 数据准备与预处理 激活函数的归纳及总结 优化算法的归纳及总结 正则化与泛化性能 模型压缩 数据扩充 为了增加深度学习模型的容量，往往包含了大量的待训练参数和模型规模，以VGG-16为例，参数数目为1亿3千万，占用空间500MB，如果利用VGG-16进行一次图片识别任务，需要进行309亿次的浮点运算（FLOPs）。如此大的模型体积和对算力的依赖，为深度学习模型迁移到嵌入式设备或者利用CPU进行推理带来了很大的难度，为此通过合理的模型压缩技术在保证模型性能没有明显下降的情况下降低模型体积和参数数目成为一个热点的研究问题。 同时，研究发现深度神经网络面临严峻的过参数化问题，需要注意的是，这种冗余在模型训练阶段是十分必要的。因为深度神经网络面临的是一个极其复杂的非凸优化问题，对于现有的基于梯度下降的优化算法而言，这种参数上的冗余保证了网络能够收敛到一个比较好的最优值。因而在一定程度上，网络越深，参数越多，模型越复杂，其最终的效果也往往越好。 按照压缩过程对网络结构的破坏程度，我们将模型压缩技术分为“前端压缩”与“后端压缩”两部分。所谓“前端压缩”，是指不改变原网络结构的压缩技术，主要包括知识蒸馏、紧凑的模型结构设计以及滤波器层面的剪枝等；而“后端压缩”则包括低秩近似、未加限制的剪枝、参数量化以及二值网络等，其目标在于尽可能地减少模型大小，因而会对原始网络结构造成极大程度的改造。其中，由于“前端压缩”未改变原有的网络结构，仅仅只是在原模型的基础上减少了网络的层数或者滤波器的个数，其最终的模型可完美适配现有的深度学习库。相比之下，“后端压缩”为了追求极致的压缩比，不得不对原有的网络结构进行改造，如对参数进行量化表示等，而这样的改造往往是不可逆的。同时，为了获得理想的压缩效果，必须开发相配套的运行库，甚至是专门的硬件设备，其最终的结果往往是一种压缩技术对应于一套运行库，从而带来了巨大的维护成本。 剪枝与稀疏约束知识蒸馏参数量化二值网络参考 《解析卷积神经网络—深度学习实践手册》 Keras文档","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"基础知识","slug":"深度学习/基础知识","permalink":"http://blog.a-stack.com/categories/深度学习/基础知识/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"模型","slug":"模型","permalink":"http://blog.a-stack.com/tags/模型/"},{"name":"算法","slug":"算法","permalink":"http://blog.a-stack.com/tags/算法/"}]},{"title":"深度学习基础之数据准备与预处理","slug":"深度学习基础之数据预处理","date":"2018-03-01T06:35:01.000Z","updated":"2018-05-15T09:14:33.808Z","comments":true,"path":"2018/03/01/深度学习基础之数据预处理/","link":"","permalink":"http://blog.a-stack.com/2018/03/01/深度学习基础之数据预处理/","excerpt":"","text":"由于人工智能面向的应用和场景的多样性，导致需要分析的数据无论是从维度还是格式上都存在巨大差异，数据准备阶段需要解决数据的数值化、归一化、特征工程等共性的问题。 深度学习基础篇将从几个不同的层面来总结在过去一段时间对于深度学习关键技术的理解，通过知识体系的归纳了解知识体系的不足，提升对核心技术点的认识。所有系列文章将在未来一段时间内容随着掌握了解的深入迭代更新。目前主要希望对如下几个领域进行归纳汇总： 问题定义 目标及评估 数据准备与预处理 激活函数的归纳及总结 优化算法的归纳及总结 正则化与泛化性能 模型压缩 数据扩充 由于人工智能面向的应用和场景的多样性，导致需要分析的数据无论是从维度还是格式上都存在巨大差异，数据准备阶段需要解决数据的数值化、归一化、特征工程等共性的问题。 深度学习的端到端学习能力并不意味着在实际的业务处理中把原始数据直接丢进网络模型，与传统的机器学习技术类似必要的数据预处理工作无论是对于提升模型的收敛效率还是提升模型的训练质量都具备十分重要的意义。 数值化由于神经网络的输入限定为数值数据，所以对于字符串数据、文本数据、类别数据，在导入网络模型之前需要进行数值化处理，转换为数值数据。其中类别数据可以采用one-hot编码等方式进行编码。 数据归一化数据归一化是属于预处理阶段经常采用的一种手段。虽然这里有一系列可行的方法，但是这一步通常是根据数据的具体情况而明确选择的。特征归一化常用的方法包含如下几种： 简单缩放 逐样本均值消减(也称为移除直流分量) 特征标准化(使数据集中所有特征都具有零均值和单位方差) 简单缩放在简单缩放中，我们的目的是通过对数据的每一个维度的值进行重新调节（这些维度可能是相互独立的），使得最终的数据向量落在 [0,1]或[ − 1,1] 的区间内（根据数据情况而定）。这对后续的处理十分重要，因为很多默认参数（如 PCA-白化中的 epsilon）都假定数据已被缩放到合理区间。 例子:在处理自然图像时，我们获得的像素值在 [0,255] 区间中，常用的处理是将这些像素值除以 255，使它们缩放到 [0,1] 中. 逐样本均值消减如果你的数据是平稳的（即数据每一个维度的统计都服从相同分布），那么你可以考虑在每个样本上减去数据的统计平均值(逐样本计算)。 例子：对于图像，这种归一化可以移除图像的平均亮度值 (intensity)。很多情况下我们对图像的照度并不感兴趣，而更多地关注其内容，这时对每个数据点移除像素的均值是有意义的。注意：虽然该方法广泛地应用于图像，但在处理彩色图像时需要格外小心，具体来说，是因为不同色彩通道中的像素并不都存在平稳特性。 特征标准化特征标准化指的是（独立地）使得数据的每一个维度具有零均值和单位方差。这是归一化中最常见的方法并被广泛地使用（例如，在使用支持向量机（SVM）时，特征标准化常被建议用作预处理的一部分）。在实际应用中，特征标准化的具体做法是：首先计算每一个维度上数据的均值（使用全体数据计算），之后在每一个维度上都减去该均值。下一步便是在数据的每一维度上除以该维度上数据的标准差。 例子:处理音频数据时，常用 Mel 倒频系数 MFCCs 来表征数据。然而MFCC特征的第一个分量（表示直流分量）数值太大，常常会掩盖其他分量。这种情况下，为了平衡各个分量的影响，通常对特征的每个分量独立地使用标准化处理。 原理： 在每个样本中减去数据的统计平均值，可以移除数据的共同部分，凸显个体差异。 注意： 数据归一化中采取的统计平均值和均方差值都来源于训练数据，由于理论上不应该从验证集和测试集中获取信息，所以对于验证集和测试集的处理也使用训练集的结果。 标准流程在这一部分中，我们将介绍几种在一些数据集上有良好表现的预处理标准流程. 自然灰度图像灰度图像具有平稳特性，我们通常在第一步对每个数据样本分别做均值消减（即减去直流分量），然后采用 PCA/ZCA 白化处理，其中的 epsilon 要足够大以达到低通滤波的效果。 彩色图像对于彩色图像，色彩通道间并不存在平稳特性。因此我们通常首先对数据进行特征缩放（使像素值位于 [0,1] 区间），然后使用足够大的 epsilon 来做 PCA/ZCA。注意在进行 PCA 变换前需要对特征进行分量均值归零化。 音频 (MFCC/频谱图)对于音频数据 (MFCC 和频谱图)，每一维度的取值范围（方差）不同。例如 MFCC 的第一分量是直流分量，通常其幅度远大于其他分量，尤其当特征中包含时域导数 (temporal derivatives) 时（这是音频处理中的常用方法）更是如此。因此，对这类数据的预处理通常从简单的数据标准化开始（即使得数据的每一维度均值为零、方差为 1），然后进行 PCA/ZCA 白化（使用合适的 epsilon）。 MNIST 手写数字MNIST 数据集的像素值在 [0,255] 区间中。我们首先将其缩放到 [0,1] 区间。实际上，进行逐样本均值消去也有助于特征学习。注：也可选择以对 MNIST 进行 PCA/ZCA 白化，但这在实践中不常用。 图像预处理Mean Subtraction data normalization 目的：减少不同图片受光照变化的影响。 R = R - \\mu _R G = G - \\mu _G B = B - \\mu _B12345678910111213141516171819202122import cv2class MeanPreprocessor: def __init__(self, rMean, gMean, bMean): # store the Red, Green, and Blue channel averages across a # training set self.rMean = rMean self.gMean = gMean self.bMean = bMean def preprocess(self, image): # split the image into its respective Red, Green, and Blue # channels (B, G, R) = cv2.split(image.astype(\"float32\")) # subtract the means for each channel R -= self.rMean G -= self.gMean B -= self.bMean # merge the channels back together and return the image return cv2.merge([B, G, R]) Patch Extraction 从原始图像中随机采样MxN的区域，当原始图像的稀疏度较高时可以采用该方法。 降低过拟合的概率 256x256 ===&gt; 227x227 12345678910111213from sklearn.feature_extraction.image import extract_patches_2dclass PatchPreprocessor: def __init__(self, width, height): # store the target width and height of the image self.width = width self.height = height def preprocess(self, image): # extract a random crop from the image with the target width # and height return extract_patches_2d(image, (self.height, self.width), max_patches=1)[0] Cropping(Over-Sampling) 使用扣取方法可以从原始图像的四个角+中心位置进行扣取，实验证明该方法可以提升1-2个百分比的分类精度； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport cv2class CropPreprocessor: def __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA): # store the target image width, height, whether or not # horizontal flips should be included, along with the # interpolation method used when resizing self.width = width self.height = height self.horiz = horiz self.inter = inter def preprocess(self, image): # initialize the list of crops crops = [] # grab the width and height of the image then use these # dimensions to define the corners of the image based (h, w) = image.shape[:2] coords = [ [0, 0, self.width, self.height], [w - self.width, 0, w, self.height], [w - self.width, h - self.height, w, h], [0, h - self.height, self.width, h]] # compute the center crop of the image as well dW = int(0.5 * (w - self.width)) dH = int(0.5 * (h - self.height)) coords.append([dW, dH, w - dW, h - dH]) # loop over the coordinates, extract each of the crops, # and resize each of them to a fixed size for (startX, startY, endX, endY) in coords: crop = image[startY:endY, startX:endX] crop = cv2.resize(crop, (self.width, self.height), interpolation=self.inter) crops.append(crop) # check to see if the horizontal flips should be taken if self.horiz: # compute the horizontal mirror flips for each crop mirrors = [cv2.flip(c, 1) for c in crops] crops.extend(mirrors) # return the set of crops return np.array(crops) Keras中的数据预处理功能http://keras-cn.readthedocs.io/en/latest/preprocessing/sequence/ A. 设置随机种子 1np.random.seed(1337) # for reproducibility B. 输入数据维度规格化，这里每个样本只是size为784的一维数组。1X_train = X_train.reshape(60000, 784) 将类别标签转换为one-hot encoding， 这一步对多分类是必须的 1one_hot_labels = keras.utils.np_utils.to_categorical(labels, num_classes=10) C. 输入数据类型转换，数值归一化 12X_train = X_train.astype('float32')X_train /= 255 序列预处理 文本预处理 图片预处理 样本数据序列化为HDF5文件 目的：减少多次IO读取的延时 参考 《解析卷积神经网络—深度学习实践手册》 Keras文档 http://deeplearning.stanford.edu/wiki/index.php/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.a-stack.com/categories/深度学习/"},{"name":"基础知识","slug":"深度学习/基础知识","permalink":"http://blog.a-stack.com/categories/深度学习/基础知识/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"预处理","slug":"预处理","permalink":"http://blog.a-stack.com/tags/预处理/"}]},{"title":"Tips for Keras","slug":"Tips-for-Keras","date":"2018-02-22T05:20:28.000Z","updated":"2018-05-15T09:14:33.839Z","comments":true,"path":"2018/02/22/Tips-for-Keras/","link":"","permalink":"http://blog.a-stack.com/2018/02/22/Tips-for-Keras/","excerpt":"撰写一篇博文用于记录Keras使用过程中的一些关键用法，以方便速查。","text":"撰写一篇博文用于记录Keras使用过程中的一些关键用法，以方便速查。 模型可视化1keras.utils.vis_utils模块提供了画出Keras模型的函数（利用graphviz） 该函数将画出模型结构图，并保存成图片： 12from keras.utils import plot_modelplot_model(model, to_file='model.png') plot_model接收两个可选参数： show_shapes：指定是否显示输出数据的形状，默认为False show_layer_names:指定是否显示层名称,默认为True 我们也可以直接获取一个pydot.Graph对象，然后按照自己的需要配置它，例如，如果要在ipython中展示图片 1234from IPython.display import SVGfrom keras.utils.vis_utils import model_to_dotSVG(model_to_dot(model).create(prog='dot', format='svg')) 【Tips】依赖 pydot-ng 和 graphviz，若出现错误，用命令行输入 pip install pydot-ng &amp; brew install graphviz 参考 1. http://keras-cn.readthedocs.io/en/latest/other/visualization/ &#8617;","categories":[{"name":"工具","slug":"工具","permalink":"http://blog.a-stack.com/categories/工具/"},{"name":"Keras","slug":"工具/Keras","permalink":"http://blog.a-stack.com/categories/工具/Keras/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://blog.a-stack.com/tags/AI/"},{"name":"人工智能","slug":"人工智能","permalink":"http://blog.a-stack.com/tags/人工智能/"},{"name":"Keras","slug":"Keras","permalink":"http://blog.a-stack.com/tags/Keras/"},{"name":"技巧","slug":"技巧","permalink":"http://blog.a-stack.com/tags/技巧/"}]},{"title":"博客相关配置","slug":"hexo_config","date":"2018-02-19T16:00:00.000Z","updated":"2018-05-23T12:45:41.660Z","comments":true,"path":"2018/02/20/hexo_config/","link":"","permalink":"http://blog.a-stack.com/2018/02/20/hexo_config/","excerpt":"2018年2月20日，重新启用个人blog系统，用于整理技术文档并进行技术分享，主要围绕人工智能相关技术最新技术进展和动态。 概述2018年2月20日，重新启用个人blog系统，用于整理技术文档并进行技术分享，主要围绕人工智能相关技术最新技术进展和动态。 使用框架： hexo(v1.0.4) 主题： hexo-theme-melody 相关依赖: hexo-renderer-jade hexo-renderer-stylus hexo-algoliasearch hexo-wordcount hexo-deployer-git hexo-generator-searchdb​","text":"2018年2月20日，重新启用个人blog系统，用于整理技术文档并进行技术分享，主要围绕人工智能相关技术最新技术进展和动态。 概述2018年2月20日，重新启用个人blog系统，用于整理技术文档并进行技术分享，主要围绕人工智能相关技术最新技术进展和动态。 使用框架： hexo(v1.0.4) 主题： hexo-theme-melody 相关依赖: hexo-renderer-jade hexo-renderer-stylus hexo-algoliasearch hexo-wordcount hexo-deployer-git hexo-generator-searchdb​ 安装部署 Hexo Server的部署 git submodule 设置 额外修改配置DaoVoice 首先在 daovoice 注册账号,邀请码是0f81ff2f，注册完成后会得到一个 app_id； 在head.pug文件插入如下代码： 1234567if theme.daovoice script. (function(i,s,o,g,r,a,m)&#123;i[\"DaoVoiceObject\"]=r;i[r]=i[r]||function()&#123;(i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset=\"utf-8\";m.parentNode.insertBefore(a,m)&#125;)(window,document,\"script\",('https:' == document.location.protocol ? 'https:' : 'http:') + \"//widget.daovoice.io/widget/d29c876c.js\",\"daovoice\") daovoice('init', &#123; app_id: '!&#123;theme.daovoice_app_id&#125;' &#125;); daovoice('update'); 在_config.yml文件中增加如下内容： 123# Online contact daovoice: truedaovoice_app_id: 这里填你的刚才获得的 app_id 修改标题字体显示样式pos.styl文件 其它bLF will be replacedWindows 提交命令的时候出现 warning: LF will be replaced by CRLF in XXXXXXXXXXXXXX 的警告。输入命令： 解决办法 1git config --global core.autocrlf false To-Do List CDN SOE hexo-theme-doc 集成 ​","categories":[{"name":"工具","slug":"工具","permalink":"http://blog.a-stack.com/categories/工具/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://blog.a-stack.com/tags/博客/"},{"name":"配置","slug":"配置","permalink":"http://blog.a-stack.com/tags/配置/"}]},{"title":"Markdown Formats","slug":"Markdown_Reference","date":"2017-12-31T16:00:00.000Z","updated":"2018-05-23T12:45:14.341Z","comments":true,"path":"2018/01/01/Markdown_Reference/","link":"","permalink":"http://blog.a-stack.com/2018/01/01/Markdown_Reference/","excerpt":"Markdown相关语法总结","text":"Markdown相关语法总结 Markdown 主要格式及效果OverviewMarkdown is created by Daring Fireball, the original guideline is here. Its syntax, however, varies between different parsers or editors. Typora is using GitHub Flavored Markdown. Please note that HTML fragments in markdown source will be recognized but not parsed or rendered. Also, there may be small reformatting on the original markdown source code after saving. 数学公式基本 行内公式 1$...$ 居中显示： 1$$...$$ ​ \\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ \\frac{\\partial X}{\\partial u} & \\frac{\\partial Y}{\\partial u} & 0 \\\\ \\frac{\\partial X}{\\partial v} & \\frac{\\partial Y}{\\partial v} & 0 \\\\ \\end{vmatrix} In markdown source file, math block is LaTeX expression wrapped by ‘$$’ mark: 1234567$$\\mathbf&#123;V&#125;_1 \\times \\mathbf&#123;V&#125;_2 = \\begin&#123;vmatrix&#125; \\mathbf&#123;i&#125; &amp; \\mathbf&#123;j&#125; &amp; \\mathbf&#123;k&#125; \\\\\\frac&#123;\\partial X&#125;&#123;\\partial u&#125; &amp; \\frac&#123;\\partial Y&#125;&#123;\\partial u&#125; &amp; 0 \\\\\\frac&#123;\\partial X&#125;&#123;\\partial v&#125; &amp; \\frac&#123;\\partial Y&#125;&#123;\\partial v&#125; &amp; 0 \\\\\\end&#123;vmatrix&#125;$$ 希腊字母 显示 命令 显示 命令 α \\alpha β \\beta γ \\gamma δ \\delta ε \\epsilon ζ \\zeta η \\eta θ \\theta ι \\iota κ \\kappa λ \\lambda μ \\mu ν \\nu ξ \\xi π \\pi ρ \\rho σ \\sigma τ \\tau υ \\upsilon φ \\phi χ \\chi ψ \\psi ω \\omega 若大写字母，命令首字母大写即可：$\\Omega$,的代码为\\Omega 若需要斜体，命令前添加var前缀：$\\varOmega$,代码为\\varOmega 修饰符 上下标 上标使用：^ 下标使用：_ 举例： x_{n} ^2 矢量 \\vec a : $\\vec a$ \\overrightarrow {xy}: $\\overrightarrow {xy}$ 字体 Typewriter： \\mathtt {A}: $ \\mathtt {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$ Blackbloard Bold: \\mathbb {A}: $\\mathbb {ABCDEFGHIJKLMNOPQRSTUVWXY}$ SansSerif: \\mathsf {A}: $\\mathsf {ABCDEFGHIJKLMNOPQRSTUVWXY}$ 分组显示 {...} 括号 () [] \\langle, \\rangle: $\\langle … \\rangle$ 求和、极限与积分 求和： \\sum $\\sum_{i=1}^n{a_i}$ 极限： \\lim_{x\\to 0} $\\lim_{x\\to 0}$ 积分： \\int_0^\\infty{x}dx $\\int_0^\\infty{x}dx$ 分式与根号 分式：\\frac{分子}{分母} $\\frac{分子}{分母}$ 根式： \\sqrt[x]{y} $\\sqrt[x]{y}$ 特殊函数 \\sin x \\ln x \\max(A,B,C) $\\sin x$ ,$\\ln x$, $max(A,B,C)$ 特殊符号 显示 命令 显示 命令 显示 命令 $\\lt$ \\lt $\\cup$ \\cup $\\to$ \\to $\\gt$ \\gt $\\cap$ \\cap $\\forall$ \\forall $\\le$ \\le $\\setminus$ \\setminus $\\exists$ \\exists $\\ge$ \\ge $\\subset$ \\subset $\\lnot$ lnot $\\neq$ \\neq $\\subseteq$ \\subseteq $\\nabla$ \\nabla $\\not$ not $\\subsetneq$ \\subsetneq $\\partial$ \\partial $\\times$ times $\\supset$ \\supset $\\approx$ \\approx $\\div$ div $\\in$ \\in $\\ldots$ \\ldots $\\pm$ pm $\\notin$ \\notin $\\bullet$ \\bullet $\\cdot$ cdot $\\emptyset$ \\emptyset $\\circ$ \\circ 空格 小空格： a\\ b 4空格： a\\quad b 多行公式 1234567891011$$sign(x)=\\begin&#123;equation&#125; \\left\\&#123; \\begin&#123;array&#125;&#123;lr&#125; +1, &amp; x \\ge 0\\\\ -1, &amp; x \\lt 0 \\end&#123;array&#125; \\right. \\end&#123;equation&#125; $$ sign(x)= \\begin{equation} \\left\\{ \\begin{array}{lr} +1, & x \\ge 0\\\\ -1, & x \\lt 0 \\end{array} \\right. \\end{equation} 公式编号 123$$\\begin&#123;equation&#125; h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n = \\theta ^T x \\tag&#123;1.a&#125; \\end&#123;equation&#125;$$ \\begin{equation} h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n = \\theta ^T x \\tag{1.a} \\end{equation}参考 MathJax Baisic Tutorial https://www.jianshu.com/p/a0aa94ef8ab2 Block ElementsParagraph and line breaksA paragraph is simply one or more consecutive lines of text. In markdown source code, paragraphs are separated by more than one blank lines. In Typora, you only need to press Return to create a new paragraph. Press Shift + Return to create a single line break. However, most markdown parser will ignore single line break, to make other markdown parsers recognize your line break, you can leave two whitespace at the end of the line, or insert &lt;br/&gt;. HeadersHeaders use 1-6 hash characters at the start of the line, corresponding to header levels 1-6. For example: 12345# This is an H1## This is an H2###### This is an H6 In typora, input ‘#’s followed by title content, and press Return key will create a header. BlockquotesMarkdown uses email-style &gt; characters for block quoting. They are presented as: 1234567&gt; This is a blockquote with two paragraphs. This is first paragraph.&gt;&gt; This is second pragraph.Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.&gt; This is another blockquote with one paragraph. There is three empty line to seperate two blockquote. In typora, just input ‘&gt;’ followed by quote contents a block quote is generated. Typora will insert proper ‘&gt;’ or line break for you. Block quote inside anther block quote is allowed by adding additional levels of ‘&gt;’. ListsInput * list item 1 will create an un-ordered list, the * symbol can be replace with + or -. Input 1. list item 1 will create an ordered list, their markdown source code is like: 123456789## un-ordered list* Red* Green* Blue## ordered list1. Red2. Green3. Blue Task ListTask lists are lists with items marked as either [ ] or [x] (incomplete or complete). For example: 12345- [ ] a task list item- [ ] list syntax required- [ ] normal **formatting**, @mentions, #1234 refs- [ ] incomplete- [x] completed You can change the complete/incomplete state by click the checkbox before the item. (Fenced) Code BlocksTypora only support fences in Github Flavored Markdown. Original code blocks in markdown is not supported. Using fences is easy: Input ``` and press return. Add an optional language identifier after ``` and we’ll run it through syntax highlighting: 123Here&apos;s an example:​ function test() { console.log(“notice the blank line before this function?”);}​1234567syntax highlighting:​```rubyrequire 'redcarpet'markdown = Redcarpet.new(\"Hello World!\")puts markdown.to_html​ 12345678910111213141516### TablesInput `| First Header | Second Header |` and press `return` key will create a table with two column.After table is created, focus on that table will pop up a toolbar for table, where you can resize, align, or delete table. You can also use context menu to copy and add/delete column/row.Following descriptions can be skipped, as markdown source code for tables are generated by typora automatically.In markdown source code, they look like:``` markdown| First Header | Second Header || ------------- | ------------- || Content Cell | Content Cell || Content Cell | Content Cell | You can also include inline Markdown such as links, bold, italics, or strikethrough. Finally, by including colons : within the header row, you can define text to be left-aligned, right-aligned, or center-aligned: 12345| Left-Aligned | Center Aligned | Right Aligned || :------------ |:---------------:| -----:|| col 3 is | some wordy text | $1600 || col 2 is | centered | $12 || zebra stripes | are neat | $1 | A colon on the left-most side indicates a left-aligned column; a colon on the right-most side indicates a right-aligned column; a colon on both sides indicates a center-aligned column. Footnotes123You can create footnotes like this[^footnote].[^footnote]: Here is the *text* of the **footnote**. will produce: You can create footnotes like thisfootnote. footnote. Here is the text of the footnote. &#8617; Mouse on the ‘footnote’ superscript to see content of the footnote. Horizontal RulesInput *** or --- on a blank line and press return will draw a horizontal line. YAML Front MatterTypora support YAML Front Matter now. Input --- at the top of the article and then press Enter will introduce one. Or insert one metadata block from the menu. Table of Contents (TOC)Input [toc] then press Return key will create a section for “Table of Contents” extracting all headers from one’s writing, its contents will be updated automatically. Diagrams (Sequence, Flowchart and Mermaid)Typora supports, sequence, flowchart and mermaid, after this feature is enabled from preference panel. See this document for detail. Span ElementsSpan elements will be parsed and rendered right after your typing. Moving cursor in middle of those span elements will expand those elements into markdown source. Following will explain the syntax of those span element. LinksMarkdown supports two style of links: inline and reference. In both styles, the link text is delimited by [square brackets]. To create an inline link, use a set of regular parentheses immediately after the link text’s closing square bracket. Inside the parentheses, put the URL where you want the link to point, along with an optional title for the link, surrounded in quotes. For example: 123This is [an example](http://example.com/ \"Title\") inline link.[This link](http://example.net/) has no title attribute. will produce: This is an example inline link. (&lt;p&gt;This is &lt;a href=&quot;http://example.com/&quot; title=&quot;Title&quot;&gt;) This link has no title attribute. (&lt;p&gt;&lt;a href=&quot;http://example.net/&quot;&gt;This link&lt;/a&gt; has no) Internal LinksYou can set the href to headers, which will create a bookmark that allow you to jump to that section after clicking. For example: Command(on Windows: Ctrl) + Click This link will jump to header Block Elements. To see how to write that, please move cursor or click that link with ⌘ key pressed to expand the element into markdown source. Reference LinksReference-style links use a second set of square brackets, inside which you place a label of your choosing to identify the link: 12345This is [an example][id] reference-style link.Then, anywhere in the document, you define your link label like this, on a line by itself:[id]: http://example.com/ \"Optional Title Here\" In typora, they will be rendered like: This is an example reference-style link. The implicit link name shortcut allows you to omit the name of the link, in which case the link text itself is used as the name. Just use an empty set of square brackets — e.g., to link the word “Google” to the google.com web site, you could simply write: 1234[Google][]And then define the link:[Google]: http://google.com/ In typora click link will expand it for editing, command+click will open the hyperlink in web browser. URLsTypora allows you to insert urls as links, wrapped by &lt;brackets&gt;. &lt;i@typora.io&gt; becomes &#105;&#x40;&#x74;&#121;&#x70;&#x6f;&#x72;&#97;&#46;&#105;&#111;. Typora will aslo auto link standard URLs. e.g: www.google.com. ImagesImage looks similar with links, but it requires an additional ! char before the start of link. Image syntax looks like this: 123![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg \"Optional title\") You are able to use drag &amp; drop to insert image from image file or we browser. And modify the markdown source code by clicking on the image. Relative path will be used if image is in same directory or sub-directory with current editing document when drag &amp; drop. For more tips on images, please read http://support.typora.io//Images/ EmphasisMarkdown treats asterisks (*) and underscores (_) as indicators of emphasis. Text wrapped with one * or _ will be wrapped with an HTML &lt;em&gt; tag. E.g: 123*single asterisks*_single underscores_ output: single asterisks single underscores GFM will ignores underscores in words, which is commonly used in code and names, like this: wow_great_stuff do_this_and_do_that_and_another_thing. To produce a literal asterisk or underscore at a position where it would otherwise be used as an emphasis delimiter, you can backslash escape it: 1\\*this text is surrounded by literal asterisks\\* Typora recommends to use * symbol. Strongdouble *’s or _’s will be wrapped with an HTML &lt;strong&gt; tag, e.g: 123**double asterisks**__double underscores__ output: double asterisks double underscores Typora recommends to use ** symbol. CodeTo indicate a span of code, wrap it with backtick quotes (`). Unlike a pre-formatted code block, a code span indicates code within a normal paragraph. For example: 1Use the `printf()` function. will produce: Use the printf() function. StrikethroughGFM adds syntax to create strikethrough text, which is missing from standard Markdown. ~~Mistaken text.~~ becomes Mistaken text. UnderlineUnderline is powered by raw HTML. &lt;u&gt;Underline&lt;/u&gt; becomes Underline. Emoji :happy:Input emoji with syntax :smile:. User can trigger auto-complete suggestions for emoji by pressing ESC key, or trigger it automatically after enable it on preference panel. Also, input UTF8 emoji char directly from Edit -&gt; Emoji &amp; Symbols from menu bar is also supported. HTMLTypora cannot render html fragments. But typora can parse and render very limited HTML fragments, as an extension of Markdown, including: Underline: &lt;u&gt;underline&lt;/u&gt; Image: &lt;img src=&quot;http://www.w3.org/html/logo/img/mark-word-icon.png&quot; width=&quot;200px&quot; /&gt; (And width, height attribute in HTML tag, and width, height, zoom style in style attribute will be applied.) Comments: &lt;!-- This is some comments --&gt; Hyperlink: &lt;a href=&quot;http://typora.io&quot; target=&quot;_blank&quot;&gt;link&lt;/a&gt;. Most of their attributes, styles, or classes will be ignored. For other tags, typora will render them as raw HTML snippets. But those HTML will be exported on print or export. SubscriptTo use this feature, first, please enable it in Preference Panel -&gt; Markdown Tab. Then use ~ to wrap subscript content, for example: H~2~O, X~long\\ text~/ SuperscriptTo use this feature, first, please enable it in Preference Panel -&gt; Markdown Tab. Then use ^ to wrap superscript content, for example: X^2^. HighlightTo use this feature, first, please enable it in Preference Panel -&gt; Markdown Tab. Then use == to wrap superscript content, for example: ==highlight==.","categories":[{"name":"工具","slug":"工具","permalink":"http://blog.a-stack.com/categories/工具/"},{"name":"MarkDown","slug":"工具/MarkDown","permalink":"http://blog.a-stack.com/categories/工具/MarkDown/"}],"tags":[{"name":"技巧","slug":"技巧","permalink":"http://blog.a-stack.com/tags/技巧/"},{"name":"总结","slug":"总结","permalink":"http://blog.a-stack.com/tags/总结/"},{"name":"Markdown","slug":"Markdown","permalink":"http://blog.a-stack.com/tags/Markdown/"}]},{"title":"读书笔记-<腾讯传：中国互联网公司进化论>","slug":"读书笔记-腾讯传","date":"2017-02-02T07:33:43.000Z","updated":"2018-05-15T13:32:58.866Z","comments":true,"path":"2017/02/02/读书笔记-腾讯传/","link":"","permalink":"http://blog.a-stack.com/2017/02/02/读书笔记-腾讯传/","excerpt":"2017年1月份，整理了一堆计划在2017年阅读的书籍清单，其中包括一些读过多遍的老书，也有一些刚刚问世的新书。希望通过一轮全新的阅读能能够获得更多的感悟。其中《腾讯传：中国互联网公司进化论》在2016年12月刚刚出版，又是关于BAT巨头的全新著作，不免先睹为快。","text":"2017年1月份，整理了一堆计划在2017年阅读的书籍清单，其中包括一些读过多遍的老书，也有一些刚刚问世的新书。希望通过一轮全新的阅读能能够获得更多的感悟。其中《腾讯传：中国互联网公司进化论》在2016年12月刚刚出版，又是关于BAT巨头的全新著作，不免先睹为快。 写在前面 ​ 关于这本书，官方（出版社及网络宣传方）给出的宣传语为： 腾讯官方唯一授权的权威传记 著名财经作家吴晓波倾力之作 当市值最高的中国互联网公司，遇上中国财经界最冷静的一双眼睛 读懂腾讯，读懂中国互联网 ​ 豆瓣读书上,我刚开始读的时候也给出了接近9分的高评分，当然随着时间的推移，越来越来的读者接入，评分逐渐回落到了8分左右。 ​ 据说，为了写这本书，吴晓波花了五年时间整理材料，不免让人想起，当年著名作家沃尔特·艾萨克森（Walter Isaacson）在2009年花了两年与乔布斯面对面交流40多次、对乔布斯100多位家庭成员、朋友、竞争对手和同事的采访的基础上撰写而成《史蒂夫·乔布斯传》。 先吐槽为快​ 基于写在前面部分提到的三点，我对这部著作充满了期待，期望中国也能有个可以将中国科技巨头完美呈现的作家。但实际上，让一个财经记者为中国科技界把脉还是有些勉强。中国缺少这么一个能把科技界的事情讲明白的文人。但期待科技圈外面的人能够把有些内容讲清楚却给别人讲不清楚自己做的内容，我们往往善于去创造科技成果，却极少总结、积累这些成果。所以吴晓波在开篇就强调，自己期望完成一部巨著，但现实十分残忍，腾讯这家公司没有足够的史料积累，哪怕一些重要会议的会议纪要都没有沉淀在文字上。所以他除了访谈让关键人物去回忆，然后不停的去验证回忆的正确性，只能通过被有限公开的腾讯内部邮件中攫取一些信息。 ​ 最终的结果就是，《腾讯传》成为了一部大量互联网上公开信息的整理，通过时间流水账的形式展示了出来，及时偶尔加载其中的哪些奇闻异事也大多被我们嚼过了，多多少少有些失望。 ​ 另一方面，于《腾讯传》而言，吴晓波俨然成为了腾讯的官方发言人，他明显没有从一个中立作家的身份去评价发生在特定历史时期的腾讯事件。至少在《乔布斯传》上，沃尔特对乔布斯除了溢美之词外也常常能够看到对乔布斯的中性批判，这些从客观角度的反思和批判给了读者很多反思和思想碰撞。诚然，腾讯能够在几次IT变迁之后能够生存和发展有其历史必然，而不单单是运气那么简单，但《腾讯传》这种拼命企图洗白的方式，多少让人觉得不够客观，而且对一些历史事件（3Q大战，小米 米聊于微信的移动客户端之争）只能够简单的陈述，而我所期待的根本性分析明显还不够透彻。 ​ 当然，整体而言，吴晓波还是很用心的在构造中国互联网的发展演进历史和趋势演进，所以吐槽归吐槽，我将用更多的篇幅来描述这本著作给我的感悟。 中国互联网的演进马化腾的产品经理思维关于互联网+的思考","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/categories/读书笔记/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://blog.a-stack.com/tags/技术/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/tags/读书笔记/"}]},{"title":"Git使用总结","slug":"读书笔记-Pro-Git","date":"2017-02-02T04:45:13.000Z","updated":"2018-05-15T09:14:33.908Z","comments":false,"path":"2017/02/02/读书笔记-Pro-Git/","link":"","permalink":"http://blog.a-stack.com/2017/02/02/读书笔记-Pro-Git/","excerpt":"一直在用Git作为代码版本控制工具，一般都是现查现用，难得把一本专门介绍git的书从头读到尾。做个笔记，记录以下新Get到的一些技能，以备以后快速查看。","text":"一直在用Git作为代码版本控制工具，一般都是现查现用，难得把一本专门介绍git的书从头读到尾。做个笔记，记录以下新Get到的一些技能，以备以后快速查看。 写在前面​ 《Pro Git》由GitHub员工Scott Chacon和另一位爱好者Ben Straub共同编写，主要介绍了Git使用基础和原理，适合Git爱好者和初学者参考。 The entire Pro Git book, written by Scott Chacon and Ben Straub and published by Apress, is available here. All content is licensed under the Creative Commons Attribution Non Commercial Share Alike 3.0 license. Print versions of the book are available on Amazon.com. 可通过如下地址获取（另外本文末尾提供了不同格式下载的地址）： 官网：http://git-scm.com/book/en/v2（第二版） 中文翻译：https://git-scm.com/book/zh/v2（第二版）​ Take a Note for Reading 这次笔记整理我将以2014年的第二版英文版为基础，记录这个过程中所获得的新的技能点。 同类工具 Subversion Perforce Bazaar Git特点 数据存储方式—-》a stream of snapshots 离线处理和编辑 Checksum 算法：SHA-1 hash 0. 环境配置 Git全局配置文件的改写通过添加--global实现Git的配置文件（此处仅针对某个repo）位置为.git/config 基本信息配置 1234567$ git config --global user.name \"John Doe\"$ git config --global user.email johndoe@example.com$ git config --global pull.rebase true$ git config --global core.editor emacs$ git config --global color.ui true# Windows环境下，编辑器的配置如下$ git config --global core.editor \"'C:/Program Files (x86)/Notepad++/notepad++.exe' -multiInst 配置完成之后，可以通过如下命令检查配置情况： 1$ git config --list 别名设置 123$ git config --global alias.unstage 'reset HEAD --'$ git config --global alias.last 'log -1 HEAD'... 1. Git基础​ 下图描述了git处理文件的生命周期 关于.gitignore文件​ 文件 .gitignore 的格式规范如下： 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以/开头防止递归。 匹配模式可以以/结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号!取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号*匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号?只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如[0-9]表示匹配所有 0 到 9 的数字）。 使用两个星号** 表示匹配任意中间目录，比如a/**/z 可以匹配 a/z, a/b/z 或 a/b/c/z等。更多内容请参考：https://github.com/github/gitignore ​ 举个🌰[栗子]：1234567891011121314151617# no .a files*.a# but do track lib.a, even though you're ignoring .a files above!lib.a# only ignore the TODO file in the current directory, not subdir/TODO/TODO# ignore all files in the build/ directorybuild/# ignore doc/notes.txt, but not doc/server/arch.txtdoc/*.txt# ignore all .pdf files in the doc/ directorydoc/**/*.pdf git rm​ 我们想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。 换句话说，你想让文件保留在磁盘，但是并不想让 Git 继续跟踪。 当你忘记添加 .gitignore 文件，不小心把一个很大的日志文件或一堆 .a这样的编译生成文件添加到暂存区时，这一做法尤其有用。 为达到这一目的，使用—cached 选项：1$ git rm --cached README ​ git rm命令后面可以列出文件或者目录的名字，也可以使用 glob 模式。 比方说：1$ git rm log/*.log ​ 注意到星号 * 之前的反斜杠\\， 因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开。 此命令删除 log/ 目录下扩展名为 .log 的所有文件。 类似的比如：1$ git rm *~ ​ 该命令为删除以 ~ 结尾的所有文件。 git log​ git log 有许多选项可以帮助你搜寻你所要找的提交。 一个常用的选项是-p，用来显示每次提交的内容差异。 你也可以加上-2 来仅显示最近两次提交： 1$ git log -p -2 如果你想看到每次提交的简略的统计信息，你可以使用 —stat 选项： 123456789101112131415161718EbbyMandeMBP:blog 520XM$ git log --statcommit 1a158fd41bece4caed80210526e8700dae73a527Author: ddebby &lt;ebby.dd@gmail.com&gt;Date: Thu Feb 2 19:02:31 2017 +0800 新增两篇读书笔记 Signed-off-by: ddebby &lt;ebby.dd@gmail.com&gt; ...344\\271\\246\\347\\254\\224\\350\\256\\260-Pro-Git.md\" | 79 +++++++++++++++++++++ ...56\\260-\\350\\205\\276\\350\\256\\257\\344\\274\\240.md\" | 65 +++++++++++++++++ source/images/blog/Pro Git Version 2.jpg | Bin 0 -&gt; 134581 bytes source/images/blog/pro-git-cover.jpeg | Bin 0 -&gt; 21289 bytes ...56\\257\\344\\274\\240\\345\\260\\201\\351\\235\\242.jpg\" | Bin 0 -&gt; 12898 bytes ...0\\206-\\350\\205\\276\\350\\256\\257\\344\\274\\240.PNG\" | Bin 0 -&gt; 24520 bytes themes/next/source/css/_custom/custom.styl | 3 +- themes/next/source/css/_variables/custom.styl | 12 ++-- 8 files changed, 152 insertions(+), 7 deletions(-) 还可以给出若干搜索条件，列出符合的提交。 用 --author 选项显示指定作者的提交，用 --grep 选项搜索提交说明中的关键字。 （请注意，如果要得到同时满足这两个选项搜索条件的提交，就必须用 --all-match选项。否则，满足任意一个条件的提交都会被匹配出来） 另一个非常有用的筛选选项是 -S，可以列出那些添加或移除了某些字符串的提交。 比如说，你想找出添加或移除了某一个特定函数的引用的提交，你可以这样使用： 1$ git log -S function_name 最后一个很实用的 git log 选项是路径（path）， 如果只关心某些文件或者目录的历史提交，可以在 git log 选项的最后指定它们的路径。 因为是放在最后位置上的选项，所以用两个短划线（—）隔开之前的选项和后面限定的路径名。 选项 说明 -(n) 仅显示最近的 n 条提交 —since, —after 仅显示指定时间之后的提交。 —until, —before 仅显示指定时间之前的提交。 —author 仅显示指定作者相关的提交。 —committer 仅显示指定提交者相关的提交。 —grep 仅显示含指定关键字的提交 -S 仅显示添加或移除了某个关键字的提交 git diff Git 提供了一种比较便捷的方式：三点语法。 对于 diff 命令来说，你可以通过把 ... 置于另一个分支名后来对该分支的最新提交与两个分支的共同祖先进行比较： 1$ git diff master...contrib 该命令仅会显示自当前特性分支与 master 分支的共同祖先起，该分支中的工作。 这个语法很有用，应该牢记。 远程仓库添加远程仓库​ 我在之前的章节中已经提到并展示了如何添加远程仓库的示例，不过这里将告诉你如何明确地做到这一点。 运行 git remote add &lt;shortname&gt; &lt;url&gt; 添加一个新的远程 Git 仓库。从远程仓库中抓取与拉取​ 如果你想拉取远程仓库中有但你没有的信息，可以运行：1$ git fetch [remote-name] ​ 必须注意 git fetch 命令会将数据拉取到你的本地仓库 - 它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。 标签附注标签在 Git 中创建一个附注标签是很简单的。 最简单的方式是当你在运行 tag 命令时指定-a 选项：123456$ git tag -a v1.4 -m 'my version 1.4'$ git tagv0.1v1.3v1.4 ​ -m选项指定了一条将会存储在标签中的信息。 如果没有为附注标签指定一条信息，Git 会运行编辑器要求你输入信息。 后期打标签 要在那个提交上打标签，你需要在命令的末尾指定提交的校验和（或部分校验和）:1$ git tag -a v1.2 9fceb02 共享标签​ 默认情况下，git push 命令并不会传送标签到远程仓库服务器上。 在创建完标签后你必须显式地推送标签到共享服务器上。 这个过程就像共享远程分支一样 - 你可以运行123$ git push origin [tagname] # or$ git push origin --tags 检出标签​ 在 Git 中你并不能真的检出一个标签，因为它们并不能像分支一样来回移动。 如果你想要工作目录与仓库中特定的标签版本完全一样，可以使用 git checkout -b [branchname][tagname]在特定的标签上创建一个新分支：12$ git checkout -b version2 v2.0.0Switched to a new branch 'version2' 分支管理 git branch​ 如果需要查看每一个分支的最后一次提交，可以运行 git branch -v命令：123$ git branch -v* dev 1a158fd 新增两篇读书笔记 master f8f05fb Accept Merge Request #1 Merge Resource branch to Master branch : (resource -&gt; master) ​ —merged 与 --no-merged 这两个有用的选项可以过滤这个列表中已经合并或尚未合并到当前分支的分支。 如果要查看哪些分支已经合并到当前分支，可以运行：123$ git branch --merged iss53* master ​ 因为之前已经合并了 iss53 分支，所以现在看到它在列表中。 在这个列表中分支名字前没有 * 号的分支通常可以使用 git branch -d 删除掉；你已经将它们的工作整合到了另一个分支，所以并不会失去任何东西。​ 查看所有包含未合并工作的分支，可以运行：12$ git branch --no-merged testing ​ 这里显示了其他分支。 未被合并的分支默认是无法被删除，不过可以通过git branch -D &lt;branch_name&gt;来强制删除。 拉取远程分支： 1$ git checkout -b [branch] [remotename]/[branch] 如果想要查看设置的所有跟踪分支，可以使用 git branch 的 -vv 选项。 这会将所有的本地分支列出来并且包含更多的信息，如每一个分支正在跟踪哪个远程分支与本地分支是否是领先、落后或是都有。 12345$ git branch -vv iss53 7e424c3 [origin/iss53: ahead 2] forgot the brackets master 1ae2a45 [origin/master] deploying index fix* serverfix f8674d9 [teamone/server-fix-good: ahead 3, behind 1] this should do it testing 5ea463a trying something new 删除远程服务器的分支 123$ git push origin --delete serverfixTo https://github.com/schacon/simplegit - [deleted] serverfix git merge与git rebase 总的原则是，只对尚未推送或分享给别人的本地修改执行变基操作清理历史，从不对已推送至别处的提交执行变基操作，这样，你才能享受到两种方式带来的便利。详情参考：https://git-scm.com/book/zh/v2/Git-分支-变基 将当前分支推送为远程新分支 1git push origin local_branch:remote_branch git stash储藏与清理 在一个分支上做了一些修改，在不想commit的情况下，切换分支，需要将修改的内容存储起来，使用如下命令： 12345git stash | git stash savegit stash listgit stash apply# 移除某个stach内容git stach drop stach@&#123;0&#125; 2. Git 服务器 生成裸仓库​在开始架设 Git 服务器前，需要把现有仓库导出为裸仓库——即一个不包含当前工作目录的仓库。 这通常是很简单的。 为了通过克隆你的仓库来创建一个新的裸仓库，你需要在克隆命令后加上 --bare选项 按照惯例，裸仓库目录名以 .git 结尾，就像这样：123$ git clone --bare my_project my_project.gitCloning into bare repository 'my_project.git'...done. 现在，你的 my_project.git 目录中应该有 Git 目录的副本了。 把裸仓库放到服务器上只需要将上述my_project.git复制到某个目录下即可通过如下命令访问：1$ git clone user@git.example.com:/opt/git/my_project.git ​ 如果到该项目目录中运行 git init 命令，并加上 --shared选项，那么 Git 会自动修改该仓库目录的组权限为可写。123$ ssh user@git.example.com$ cd /opt/git/my_project.git$ git init --bare --shared 基于以上简单配置的简单权限管理策略： 主机上建立一个 git 账户，让每个需要写权限的人发送一个 SSH 公钥，然后将其加入 git 账户的 ~/.ssh/authorized_keys 文件。 这样一来，所有人都将通过 git 账户访问主机。 这一点也不会影响提交的数据——访问主机用的身份不会影响提交对象的提交者信息。 另一个办法是让 SSH 服务器通过某个 LDAP 服务，或者其他已经设定好的集中授权机制，来进行授权。 只要每个用户可以获得主机的 shell 访问权限，任何 SSH 授权机制你都可视为是有效的。 维护项目使用format-patch生成补丁 适用于邮件通知项目管理者合并补丁内容 使用 apply 命令应用补丁 ​ 如果你收到了一个使用 git diff 或 Unix diff 命令（不推荐使用这种方式，具体见下一节）创建的补丁，可以使用 git apply 命令来应用。 假设你将补丁保存在了 /tmp/patch-ruby-client.patch 中，可以这样应用补丁： 1$ git apply /tmp/patch-ruby-client.patch 这会修改工作目录中的文件。 它与运行 patch -p1 命令来应用补丁几乎是等效的，但是这种方式更加严格，相对于 patch 来说，它能够接受的模糊匹配更少。 在实际应用补丁前，你还可以使用 git apply 来检查补丁是否可以顺利应用——即对补丁运行 git apply --check 命令： 123$ git apply --check 0001-seeing-if-this-helps-the-gem.patcherror: patch failed: ticgit.gemspec:1error: ticgit.gemspec: patch does not apply 如果没有产生输出，则该补丁可以顺利应用。 如果检查失败了，该命令还会以一个非零的状态退出，所以需要时你也可以在脚本中使用它。 准备一次发布现在你可以发布一个构建了。 其中一件事情就是为那些不使用 Git 的可怜包们创建一个最新的快照归档。 使用 git archive 命令完成此工作： 123$ git archive master --prefix='project/' | gzip &gt; `git describe master`.tar.gz$ ls *.tar.gzv1.6.2-rc1-20-g8c5b85c.tar.gz 如果有人将这个压缩包解压，他就可以得到你的项目文件夹的最新快照。 你也可以以类似的方式创建一个 zip 压缩包，但此时你应该向 git archive 命令传递 --format=zip 选项： 1$ git archive master --prefix='project/' --format=zip &gt; `git describe master`.zip 现在你有了本次发布的一个 tar 包和一个 zip 包，可以将其上传到网站或以电子邮件的形式发送给人们。 制作提交简报现在是时候通知邮件列表里那些好奇你的项目发生了什么的人了。 使用 git shortlog 命令可以快速生成一份包含从上次发布之后项目新增内容的修改日志（changelog）类文档。 它会对你给定范围内的所有提交进行总结；比如，你的上一次发布名称是 v1.0.1，那么下面的命令可以给出上次发布以来所有提交的总结： 1234567891011121314$ git shortlog --no-merges master --not v1.0.1Chris Wanstrath (8): Add support for annotated tags to Grit::Tag Add packed-refs annotated tag support. Add Grit::Commit#to_patch Update version and History.txt Remove stray `puts` Make ls_tree ignore nilsTom Preston-Werner (4): fix dates in history dynamic version method Version bump to 1.0.2 Regenerated gemspec for version 1.0.2 这份整洁的总结包括了自 v1.0.1 以来的所有提交，并且已经按照作者分好组，你可以通过电子邮件将其直接发送到列表中。 Git工具双点最常用的指明提交区间语法是双点。 这种语法可以让 Git 选出在一个分支中而不在另一个分支中的提交。你想要查看 experiment 分支中还有哪些提交尚未被合并入 master 分支。 你可以使用 master..experiment 来让 Git 显示这些提交。也就是 “在 experiment 分支中而不在 master 分支中的提交”： 1$ git log master..experiment 反过来，如果你想查看在 master 分支中而不在 experiment 分支中的提交，你只要交换分支名即可。experiment..master 会显示在 master 分支中而不在 experiment 分支中的提交： 1$ git log experiment..master 这可以让你保持 experiment 分支跟随最新的进度以及查看你即将合并的内容。 另一个常用的场景是查看你即将推送到远端的内容： 1$ git log origin/master..HEAD 这个命令会输出在你当前分支中而不在远程 origin 中的提交。 如果你执行了 git push 并且你的当前分支正在跟踪 origin/master，git log origin/master..HEAD 所输出的提交将会被传输到远端服务器。 如果你留空了其中的一边， Git 会默认为 HEAD。 例如， git log origin/master.. 将会输出与之前例子相同的结果 —— Git 使用 HEAD 来代替留空的一边。 多点双点语法很好用，但有时候你可能需要两个以上的分支才能确定你所需要的修订，比如查看哪些提交是被包含在某些分支中的一个，但是不在你当前的分支上。 Git 允许你在任意引用前加上 ^ 字符或者 --not来指明你不希望提交被包含其中的分支。 因此下列3个命令是等价的： 123$ git log refA..refB$ git log ^refA refB$ git log refB --not refA 这个语法很好用，因为你可以在查询中指定超过两个的引用，这是双点语法无法实现的。 比如，你想查看所有被 refA 或 refB 包含的但是不被 refC 包含的提交，你可以输入下面中的任意一个命令 12$ git log refA refB ^refC$ git log refA refB --not refC 这就构成了一个十分强大的修订查询系统，你可以通过它来查看你的分支里包含了哪些东西。 搜索例如，如果我们想找到 ZLIB_BUF_MAX 常量是什么时候引入的，我们可以使用 -S 选项来显示新增和删除该字符串的提交。 123$ git log -SZLIB_BUF_MAX --onelinee01503b zlib: allow feeding more than 4GB in one goef49a7a zlib: zlib can only process 4GB at a time 如果我们查看这些提交的 diff，我们可以看到在 ef49a7a 这个提交引入了常量，并且在 e01503b 这个提交中被修改了。 行日志搜索行日志搜索是另一个相当高级并且有用的日志搜索功能。 这是一个最近新增的不太知名的功能，但却是十分有用。 在 git log 后加上 -L 选项即可调用，它可以展示代码中一行或者一个函数的历史。 例如，假设我们想查看 zlib.c 文件中git_deflate_bound 函数的每一次变更，我们可以执行 git log -L :git_deflate_bound:zlib.c。 Git 会尝试找出这个函数的范围，然后查找历史记录，并且显示从函数创建之后一系列变更对应的补丁。 压缩提交以下命令将HEAD调回到两次提交之前，做一次压缩提交。更多内容详见：Pro Git中的说明 1git reset --soft HEAD~2 还原提交如果移动分支指针并不适合你，Git 给你一个生成一个新提交的选项，提交将会撤消一个已存在提交的所有修改。 Git 称这个操作为 “还原”，在这个特定的场景下，你可以像这样调用它： 12$ git revert -m 1 HEAD[master b1d8379] Revert \"Merge branch 'topic'\" -m 1 标记指出 “mainline” 需要被保留下来的父结点。 当你引入一个合并到 HEAD（git merge topic），新提交有两个父结点：第一个是 HEAD（C6），第二个是将要合并入分支的最新提交（C4）。 在本例中，我们想要撤消所有由父结点 #2（C4）合并引入的修改，同时保留从父结点 #1（C4）开始的所有内容。 撤销提交如果这个不想要的合并提交只存在于你的本地仓库中，最简单且最好的解决方案是移动分支到你想要它指向的地方。 大多数情况下，如果你在错误的 git merge 后运行 git reset --hard HEAD~，这会重置分支指向。 这个方法的缺点是它会重写历史，在一个共享的仓库中这会造成问题的。 查阅 变基的风险 来了解更多可能发生的事情；用简单的话说就是如果其他人已经有你将要重写的提交，你应当避免使用 reset。 如果有任何其他提交在合并之后创建了，那么这个方法也会无效；移动引用实际上会丢失那些改动。 GitHub的配置及基本测试 ssh key的创建 1$ ssh-keygen -t rsa -C \"your_email@youremail.com\" 上传id_rsa.pub 到github配置 测试 1$ ssh -T git@github.com 正确的返回结果 123456gaoc@DataScience:/data2/ml-cousera$ ssh -T git@github.comThe authenticity of host 'github.com (52.74.223.119)' can't be established.RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'github.com,52.74.223.119' (RSA) to the list of known hosts.Hi ddebby! You've successfully authenticated, but GitHub does not provide shell access. 添加origin源 1$ git remote add origin git@github.com:yourName/yourRepo.git 不要直接使用https的链接地址，还需要不停的输入用户名/密码验证 TO-DO关于书中第十章Git内部原理部分，目前还没有时间研读，留作后面作业。限于本文篇幅，这部分内容将新建一篇博文，此处插入链接即可。 获取不同版本的电子书 PDF格式 Epub格式 Mobi格式 HTML在线阅读","categories":[{"name":"工具","slug":"工具","permalink":"http://blog.a-stack.com/categories/工具/"}],"tags":[{"name":"技术","slug":"技术","permalink":"http://blog.a-stack.com/tags/技术/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://blog.a-stack.com/tags/读书笔记/"},{"name":"Git","slug":"Git","permalink":"http://blog.a-stack.com/tags/Git/"}]}]}