<!DOCTYPE html>
<html  lang="zh">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Batch-Normalization(批量归一化) - Ebby&#39;s Notes</title>


    <meta name="description" content="摘要： 借重读论文的机会，重新整理一下Batch Normalization的关键技术。">
<meta name="keywords" content="人工智能,深度学习,技术,算法">
<meta property="og:type" content="article">
<meta property="og:title" content="Batch-Normalization(批量归一化)">
<meta property="og:url" content="http://blog.a-stack.com/2019/01/12/Batch-Normalization-批量归一化/index.html">
<meta property="og:site_name" content="Ebby&#39;s Notes">
<meta property="og:description" content="摘要： 借重读论文的机会，重新整理一下Batch Normalization的关键技术。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/banner/05.jpg">
<meta property="og:updated_time" content="2020-03-21T06:08:31.497Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Batch-Normalization(批量归一化)">
<meta name="twitter:description" content="摘要： 借重读论文的机会，重新整理一下Batch Normalization的关键技术。">
<meta name="twitter:image" content="http://blog.a-stack.com/qnsource/banner/05.jpg">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/tomorrow.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?684368b6943d7a5b6689dba5e7bf30ad";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo02.png" alt="Batch-Normalization(批量归一化)" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/reading">读书</a>
                
                <a class="navbar-item"
                href="/resources">资源</a>
                
                <a class="navbar-item"
                href="/notebooks">📝</a>
                
                <a class="navbar-item"
                href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/../qnsource/banner/05.jpg" alt="Batch-Normalization(批量归一化)">
        </span>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-01-12T14:45:29.000Z">2019-01-12</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    15 分钟 读完 (大约 2177 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                Batch-Normalization(批量归一化)
            
        </h1>
        <div class="content">
            <p><img src="/qnsource/banner/07.jpg" alt="BN不得不了解的深度学习技术~"></p>
<p><strong>摘要：</strong> 借重读论文的机会，重新整理一下Batch Normalization的关键技术。</p>
<a id="more"></a>
<blockquote>
<p>近期在重新做CS231n作业的时候，再次被Batch Normalization给困住，无奈从论文和资料重新开始查阅，这次尽量把相关技术研究透，整理以备速查。</p>
<p>CS231n中关于Batch Normalization的作业出在Assignment 2的第二个作业，可通过该<a href="https://github.com/ddebby/cs231n/blob/master/assignment2/BatchNormalization.ipynb">链接</a>访问。</p>
</blockquote>
<h2 id="为什么需要Batch-Normalization"><a href="#为什么需要Batch-Normalization" class="headerlink" title="为什么需要Batch Normalization"></a>为什么需要Batch Normalization</h2><p>BN是由Google于2015年提出，这是一个深度神经网络训练的技巧，它不仅可以加快了模型的收敛速度，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。所以目前BN已经成为几乎所有卷积神经网络的标配技巧了。</p>
<p>在机器学习算法中，一般都假设所有输入特征符合独立同分布的特性，所以将输入数据归一化为单位高斯分布有利于提升模型的训练效果。这也是白化对于提升模型训练起的良好效果的原因。在深度神经网络中，我们通过对输入数据的约束，已经可以实现在输入层的激活可以保证数据的独立同分布，但随着网络传播过程，非线性激活函数的作用，导致数据的分布不断变化，数据的独立性（层的数据之间存在不同程度的相关性）被破坏。数据不再是零均值或者单位方差。同时随着梯度的更新，每层数据分布也在产生不同程度的偏移。</p>
<h2 id="Batch-Normalization的思想及实现"><a href="#Batch-Normalization的思想及实现" class="headerlink" title="Batch Normalization的思想及实现"></a>Batch Normalization的思想及实现</h2><p>模型训练阶段，BN层通过估计mini-batch中数据的均值和方差，将数据进行归一化。同时这些估计结果的滑动均值被记录下来，用于推理阶段。</p>
<p>Batch Normalization的计算</p>
<script type="math/tex; mode=display">
\begin{align}
\mu_i = \frac{1}{m} \sum_{k \in S_i} x_k\\
\sigma_i = \sqrt{\frac{1}{m}\sum_{k\in S_i} (x_k-\mu_i)^2 + \epsilon}\\
\hat x_i = \frac{x_i - \mu_i}{\sigma_i}\\
y_i = \lambda \hat x_i + \beta
\end{align}</script><blockquote>
<p>公式（4）是为了保证被BN层处理之后的数据仍具备一定的非线性特征而进行的恢复操作，不然网络所有层都是线性变换，无法进行模型参数更新。</p>
</blockquote>
<h3 id="前向传播的实现"><a href="#前向传播的实现" class="headerlink" title="前向传播的实现"></a>前向传播的实现</h3><p>参考BN作者论文的思路如下：</p>
<p><img src="/2019/01/12/Batch-Normalization-批量归一化/BN_Forward.PNG" alt="BN_Forward"></p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the training-time forward pass for batch norm.      #</span></span><br><span class="line">        <span class="comment"># Use minibatch statistics to compute the mean and variance, use      #</span></span><br><span class="line">        <span class="comment"># these statistics to normalize the incoming data, and scale and      #</span></span><br><span class="line">        <span class="comment"># shift the normalized data using gamma and beta.                     #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should store the output in the variable out. Any intermediates  #</span></span><br><span class="line">        <span class="comment"># that you need for the backward pass should be stored in the cache   #</span></span><br><span class="line">        <span class="comment"># variable.                                                           #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should also use your computed sample mean and variance together #</span></span><br><span class="line">        <span class="comment"># with the momentum variable to update the running mean and running   #</span></span><br><span class="line">        <span class="comment"># variance, storing your result in the running_mean and running_var   #</span></span><br><span class="line">        <span class="comment"># variables.                                                          #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># Note that though you should be keeping track of the running         #</span></span><br><span class="line">        <span class="comment"># variance, you should normalize the data based on the standard       #</span></span><br><span class="line">        <span class="comment"># deviation (square root of variance) instead!                        # </span></span><br><span class="line">        <span class="comment"># Referencing the original paper (https://arxiv.org/abs/1502.03167)   #</span></span><br><span class="line">        <span class="comment"># might prove to be helpful.                                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#pass</span></span><br><span class="line">        sample_mean = np.mean(x,axis=<span class="number">0</span>)</span><br><span class="line">        sample_var = np.var(x,axis=<span class="number">0</span>)</span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line">        x_hat = (x - sample_mean)/np.sqrt(sample_var + eps)</span><br><span class="line">        out = gamma * x_hat  + beta</span><br><span class="line"></span><br><span class="line">        cache =(x, x_hat, gamma, beta,eps,sample_mean, sample_var)</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test-time forward pass for batch normalization. #</span></span><br><span class="line">        <span class="comment"># Use the running mean and variance to normalize the incoming data,   #</span></span><br><span class="line">        <span class="comment"># then scale and shift the normalized data using gamma and beta.      #</span></span><br><span class="line">        <span class="comment"># Store the result in the out variable.                               #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#pass</span></span><br><span class="line">        x_hat = (x - running_mean)/np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma * x_hat  + beta</span><br><span class="line">        cache =(x,x_hat, gamma, beta, eps, running_mean, running_var)</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<h3 id="反向传播的实现"><a href="#反向传播的实现" class="headerlink" title="反向传播的实现"></a>反向传播的实现</h3><p><img src="/2019/01/12/Batch-Normalization-批量归一化/BN_Backward.PNG" alt="BN_Backward"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Alternative backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation you should work out the derivatives for the batch</span></span><br><span class="line"><span class="string">    normalizaton backward pass on paper and simplify as much as possible. You</span></span><br><span class="line"><span class="string">    should be able to derive a simple expression for the backward pass. </span></span><br><span class="line"><span class="string">    See the jupyter notebook for more hints.</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">    Note: This implementation should expect to receive the same cache variable</span></span><br><span class="line"><span class="string">    as batchnorm_backward, but might not use all of the values in the cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs / outputs: Same as batchnorm_backward</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></span><br><span class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># After computing the gradient with respect to the centered inputs, you   #</span></span><br><span class="line">    <span class="comment"># should be able to compute gradients with respect to the inputs in a     #</span></span><br><span class="line">    <span class="comment"># single statement; our implementation fits on a single 80-character line.#</span></span><br><span class="line">    <span class="comment">###########################################################################    </span></span><br><span class="line">    x, x_hat, gamma, beta, eps, mean, var = cache</span><br><span class="line"></span><br><span class="line">    dgamma = np.sum(dout*x_hat,axis=<span class="number">0</span>)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dx_hat = dout * gamma</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># temp vars</span></span><br><span class="line">    xmu = x - mean</span><br><span class="line">    istd = <span class="number">1</span>/np.sqrt(var + eps)</span><br><span class="line">    m = x.shape[<span class="number">0</span>]   </span><br><span class="line">    </span><br><span class="line">    dvar = np.sum(dx_hat*xmu*(<span class="number">-0.5</span>)*np.power((var+eps),<span class="number">-1.5</span>),axis=<span class="number">0</span>)</span><br><span class="line">    dmean = np.sum(dx_hat*(<span class="number">-1</span>)*istd,axis=<span class="number">0</span>) + np.sum(<span class="number">-2</span>*xmu*dvar,axis=<span class="number">0</span>)/m</span><br><span class="line">    dx = dx_hat*istd + dvar*<span class="number">2</span>*xmu/m + dmean/m    </span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h3 id="BN层参数的学习过程中"><a href="#BN层参数的学习过程中" class="headerlink" title="BN层参数的学习过程中"></a>BN层参数的学习过程中</h3><p>在每个BN层，除了w和b之外，还需要学习<code>gamma</code>和<code>beta</code>两个额外的参数。</p>
<h2 id="Batch-Normalization带来的好处"><a href="#Batch-Normalization带来的好处" class="headerlink" title="Batch Normalization带来的好处"></a>Batch Normalization带来的好处</h2><ol>
<li>不仅极大的提升了训练速度，收敛过程也大大的加快了；</li>
<li>增强分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；</li>
<li>使用了BN层之后，进一步减少了在处理过拟合过程中对Dropout的依赖；</li>
<li>对于网络的参数初始化要求降低，不再担心网络初始化没控制好导致网络难以训练的问题（尤其是深层次网络）</li>
</ol>
<h2 id="其它的归一化方法"><a href="#其它的归一化方法" class="headerlink" title="其它的归一化方法"></a>其它的归一化方法</h2><p>由于BN会受到batch size大小的影响，如果batch size太小，算出的均值和方差就会不准确，太大存储可能不够用。所以衍生出了几种优化表达。</p>
<p><img src="/2019/01/12/Batch-Normalization-批量归一化/BN_error_with_small_batch_size.png" alt="BN_error_with_small_batch_size"></p>
<p><img src="/qnsource/images/2018-04-28-cs231n-notes/normalization_methods.png" alt="normalization_methods"></p>
<ul>
<li>BatchNorm： batch方向做归一化，计算 <code>H*W*C</code> 的均值；</li>
<li>LayerNorm： channel方向做归一化，计算<code>N*H*W</code> 的均值；</li>
<li>InstanceNorm： 一个channel内做归一化，计算<code>H*W</code>的均值；</li>
<li>GroupNorm： 将Channel方向分为Group，然后每个Group内做归一化，计算<code>(C//G)*H*W</code>的均值</li>
</ul>
<blockquote>
<p>当G=C时，GroupNorm为LayerNorm，当G=1时，GroupNorm为InstanceNorm</p>
</blockquote>
<p>其中，GourpNorm的启发来源于图像的手工特征提取器（比如HOG，SIFT）发现很多不同类型的特征都是成组出现的，利用成组封装的方式进行特征均值、方差提取可能更能反映特征的真实情况，尤其在小批量BN效果下降的情况下作为BN的一个替代品。</p>
<p>但实际上，在使用过程中还是优先选用BN，只有在批量实在很小，如视频分析、图像分割等场景，每个批次只有一两张图像的情况下，选用GroupNorm作为替代。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://arxiv.org/abs/1502.03167">Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”, ICML 2015.</a></li>
<li><a href="https://arxiv.org/abs/1803.08494">Wu, Yuxin, and Kaiming He. “Group Normalization.” arXiv preprint arXiv:1803.08494 (2018).</a></li>
</ol>

        </div>
        
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <span class="is-size-6 has-text-grey has-mr-7">#</span>
                    <a class="has-link-grey -link" href="/tags/人工智能/">人工智能</a>, <a class="has-link-grey -link" href="/tags/文献/">文献</a>, <a class="has-link-grey -link" href="/tags/算法/">算法</a>
                </div>
            </div>
        </div>
        
        
        
        
<div class="sharethis-inline-share-buttons"></div>
<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=5e762f775039a80012d346d9&amp;product=inline-share-buttons&amp;cms=sop' async='async'></script>

        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/site/alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/site/weixin.png" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2019/01/24/2018年总结：来自18年的工作感悟/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">2018年总结：来自18年的工作感悟</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2019/01/02/数据科学工具——Pandas/">
                <span class="level-item">数据科学工具——Pandas</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: false,
        verify: false,
        app_id: 'JQMgg0zbFBNWwL0lLnq2s1G7-gzGzoHsz',
        app_key: 'm5FMVedFNxGutQCnMsVMAaXM',
        placeholder: 'Say Something ...'
    });
</script>

    </div>
</div>
</div>
                
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-3 column-right is-sticky">
    
        

    <div class="card widget" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#为什么需要Batch-Normalization">
        <span class="has-mr-6">1</span>
        <span>为什么需要Batch Normalization</span>
        </a></li><li>
        <a class="is-flex" href="#Batch-Normalization的思想及实现">
        <span class="has-mr-6">2</span>
        <span>Batch Normalization的思想及实现</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#前向传播的实现">
        <span class="has-mr-6">2.1</span>
        <span>前向传播的实现</span>
        </a></li><li>
        <a class="is-flex" href="#反向传播的实现">
        <span class="has-mr-6">2.2</span>
        <span>反向传播的实现</span>
        </a></li><li>
        <a class="is-flex" href="#BN层参数的学习过程中">
        <span class="has-mr-6">2.3</span>
        <span>BN层参数的学习过程中</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Batch-Normalization带来的好处">
        <span class="has-mr-6">3</span>
        <span>Batch Normalization带来的好处</span>
        </a></li><li>
        <a class="is-flex" href="#其它的归一化方法">
        <span class="has-mr-6">4</span>
        <span>其它的归一化方法</span>
        </a></li><li>
        <a class="is-flex" href="#参考">
        <span class="has-mr-6">5</span>
        <span>参考</span>
        </a></li></ul>
            </div>
        </div>
    </div>

    
        <div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2020/03/21/图像数据标注工具推荐-CVAT/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/blog/cvat.jpg" alt="图像数据标注工具推荐-CVAT">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-03-21T13:23:49.000Z">2020-03-21</time></div>
                    <a href="/2020/03/21/图像数据标注工具推荐-CVAT/" class="title has-link-black-ter is-size-6 has-text-weight-normal">图像数据标注工具推荐-CVAT</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/05/07/Pathlib-Python的路径管理库/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/banner/16.jpg" alt="Pathlib--Python的路径管理库">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-07T08:53:05.000Z">2019-05-07</time></div>
                    <a href="/2019/05/07/Pathlib-Python的路径管理库/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Pathlib--Python的路径管理库</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/工具/">工具</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/02/01/从头训练一个图像分类器/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/banner/08.jpg" alt="从头训练一个图像分类器">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-01T02:16:46.000Z">2019-02-01</time></div>
                    <a href="/2019/02/01/从头训练一个图像分类器/" class="title has-link-black-ter is-size-6 has-text-weight-normal">从头训练一个图像分类器</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/01/24/2018年总结：来自18年的工作感悟/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/banner/02.jpg" alt="2018年总结：来自18年的工作感悟">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-01-24T06:50:22.000Z">2019-01-24</time></div>
                    <a href="/2019/01/24/2018年总结：来自18年的工作感悟/" class="title has-link-black-ter is-size-6 has-text-weight-normal">2018年总结：来自18年的工作感悟</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/工作总结/">工作总结</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/01/12/Batch-Normalization-批量归一化/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/banner/05.jpg" alt="Batch-Normalization(批量归一化)">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-01-12T14:45:29.000Z">2019-01-12</time></div>
                    <a href="/2019/01/12/Batch-Normalization-批量归一化/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Batch-Normalization(批量归一化)</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo02.png" alt="Batch-Normalization(批量归一化)" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 Ebby DD&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>  沪ICP备20005404号
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://blog.a-stack.com',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>