<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>弄懂word2vec之原理解析 | Ebby&#39;s Notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="人工智能,深度学习,技术,算法">
  
  
  
  
  <meta name="description" content="摘要：">
<meta name="keywords" content="人工智能,深度学习,技术,算法">
<meta property="og:type" content="article">
<meta property="og:title" content="弄懂word2vec之原理解析">
<meta property="og:url" content="http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/index.html">
<meta property="og:site_name" content="Ebby&#39;s Notes">
<meta property="og:description" content="摘要：">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/banner/07.jpg">
<meta property="og:image" content="http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/word2vec_architectures.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/模型原理示意.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/tokenize_lookup.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/training_data.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/skip_gram_net_arch.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/sampled-softmax.png">
<meta property="og:updated_time" content="2018-08-24T15:18:05.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="弄懂word2vec之原理解析">
<meta name="twitter:description" content="摘要：">
<meta name="twitter:image" content="http://blog.a-stack.com/qnsource/banner/07.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Ebby&#39;s Notes" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/logo.png">
  <link rel="apple-touch-icon" href="/css/images/logo.png">
  
    <link href="/qnsource/css/Source+Code+Pro.css" rel="stylesheet" type="text/css">
  
  <link href="/qnsource/css/Open+Sans-Montserrat-700.css" rel="stylesheet" type="text/css">
  <link href="/qnsource/css/Roboto-400-300.css" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>
  <script src="/js/bootstrap.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css">

  
    <link rel="stylesheet" href="/css/dialog.css">
  

  

  
    <link rel="stylesheet" href="/css/header-post.css">
  

  
  
  
    <link rel="stylesheet" href="/css/vdonate.css">
  

</head>
</html>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 border-width: 0px;  margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="200px" height="100px" alt="Hike News" src="/css/images/logo.png">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">首页</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">归档</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">分类</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">标签</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">关于</a> </li>
                
                  <li> <a class="main-nav-link" href="/reading">读书</a> </li>
                
                  <li> <a class="main-nav-link" href="/resources">资源</a> </li>
                
                  <li> <a class="main-nav-link" href="/notebooks">📝</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="请输入关键词..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(无标题)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-弄懂word2vec之原理解析" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      弄懂word2vec之原理解析
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2018/08/21/弄懂word2vec之原理解析/" class="article-date">
	  <time datetime="2018-08-21T05:36:53.000Z" itemprop="datePublished">2018-08-21</time>
	</a>

      
    <a class="article-category-link" href="/categories/NLP/">NLP</a>

      
      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/qnsource/banner/07.jpg" alt="Test Picture"></p>
<p><strong>摘要：</strong></p>
<a id="more"></a>
<h2 id="关键点提要"><a href="#关键点提要" class="headerlink" title="关键点提要"></a>关键点提要</h2><ul>
<li>word2vec不是深度学习模型（只有输入层、输出层）</li>
<li>效果并非最好，但易用性最强（其实FastText开源之后易用性直逼Word2vec）</li>
<li>受训练语料库影响大，尤其中文语料库本身缺乏</li>
</ul>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Word2vec核心算法包括CBOW和skip-gram，再加上两种目标优化方式层次Softmax和负采样的组合，可以形成总共四种实现方式。需要特别理解的一点，其实词向量是模型的中间产物（词向量矩阵在模型中为隐藏层的权重值）而不是优化目标产物。模型的训练数据为（sourceword，targetword）的词语对。本文简单整理近段时间通过翻看代码、原理解析总结的一些对word2vec的认知，希望能够加深对整合词嵌入模型的理解。</p>
<p><img src="/2018/08/21/弄懂word2vec之原理解析/word2vec_architectures.png" alt="word2vec_architectures"></p>
<p>其中</p>
<ul>
<li>CBOW的实现原理为由中心词来预测前后的相关词；</li>
<li>Skip-Gram正好相反，为由中心词的前后相关词来倒推中心词为某词的可能性；</li>
<li>无论哪种方法，都不需要事先对数据进行标记，换句话说这可以看作一种无监督的学习方法。</li>
</ul>
<blockquote>
<p>有相关研究也对两种方法的实现效果进行了比较，CBOW在小数据集上表现要优于Skip-Gram，在大数据集合上两者相当。</p>
</blockquote>
<p><img src="/2018/08/21/弄懂word2vec之原理解析/模型原理示意.png" alt="模型原理示意"></p>
<h2 id="训练数据的定义"><a href="#训练数据的定义" class="headerlink" title="训练数据的定义"></a>训练数据的定义</h2><blockquote>
<p>详见代码</p>
</blockquote>
<p>在生成训练数据之前先要明确窗口的概念，窗口的概念与n-gram中强调的前后词之前的语义关联有关，用于描述某个词语与其后多个词语有直接或间接的关系。在word2vec中使用的是中心词前后各n个词组成的窗口大小。原则上，窗口设置越大预测效果越好，但计算量等比例增大，一般选择5-8作为恰当的窗口值。</p>
<p><strong>Embedding Lookup的解释</strong>：词向量存储为一个以词典词个数为行，词嵌入维度为列的权重矩阵，而输入单词可以被编码为数字值，利用数值在词向量矩阵中获得词的向量表达的过程成为Embedding Lookup.</p>
<p><img src="/2018/08/21/弄懂word2vec之原理解析/tokenize_lookup.png" alt="tokenize_lookup"></p>
<p>Words that show up often such as “the”, “of”, and “for” don’t provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we’ll discard it with probability given by </p>
<script type="math/tex; mode=display">
 P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}</script><p>where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.</p>
<h3 id="训练数据的切分"><a href="#训练数据的切分" class="headerlink" title="训练数据的切分"></a>训练数据的切分</h3><p><img src="/2018/08/21/弄懂word2vec之原理解析/training_data.png" alt="training_data"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">corpus = <span class="string">''' I read magazine weekly . </span></span><br><span class="line"><span class="string">I read newspaper daily . I like to read books daily .'''</span></span><br><span class="line"></span><br><span class="line">corpus_tokens = []</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus.split(<span class="string">'.'</span>):</span><br><span class="line">    corpus_tokens.append(text.lower().split())</span><br><span class="line"></span><br><span class="line"><span class="comment">########## Create vocabulary </span></span><br><span class="line">vocab = &#123;&#125;</span><br><span class="line">counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> sentence_tokens <span class="keyword">in</span> corpus_tokens:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence_tokens:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            vocab[word] = counter</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########## Create training pairs .</span></span><br><span class="line">window = <span class="number">5</span></span><br><span class="line">target_words = []</span><br><span class="line">target_words_index = []</span><br><span class="line">c = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> corpus_tokens:</span><br><span class="line">    <span class="keyword">for</span> index,center_word <span class="keyword">in</span> enumerate(i):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            right = i[index+<span class="number">1</span>: index+<span class="number">1</span>+window]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            right == []</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            left = i[index<span class="number">-1</span>: (index<span class="number">-1</span>)+window]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            left == []</span><br><span class="line">        total_words = list(set(right+left))</span><br><span class="line">        <span class="keyword">if</span> total_words:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> total_words:</span><br><span class="line">                <span class="keyword">if</span> center_word <span class="keyword">in</span> vocab <span class="keyword">and</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">                    target_words.append((center_word, word))</span><br><span class="line">                    target_words_index.append((vocab[center_word], vocab[word]))</span><br><span class="line">    c += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="网络的构建"><a href="#网络的构建" class="headerlink" title="网络的构建"></a>网络的构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">embedding_dimension = <span class="number">100</span></span><br><span class="line">n_classes           = len(vocab) </span><br><span class="line">X = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>, (len(vocab), embedding_dimension)) <span class="comment"># input features for X </span></span><br><span class="line">W = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>, (len(vocab), embedding_dimension))   </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> each_pair <span class="keyword">in</span> target_words_index:</span><br><span class="line">    </span><br><span class="line">    inp_pair_index = each_pair[<span class="number">0</span>]</span><br><span class="line">    op_pair_index  = each_pair[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    x_input = random_embeddings[inp_pair_index] <span class="comment"># 1 x embedding_dimension </span></span><br><span class="line">    </span><br><span class="line">    probs = softmax(np.dot(W, x_input))        <span class="comment"># 1 x n_classes   </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">####### loss(probs, op_pair_index)</span></span><br><span class="line">    <span class="comment">####### backpropagate , update W</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>虽然示意代码中使用了softmax，但从计算量上来看这并非一种好的方法； Word2vec中使用了两种替代方案，可以大大节约计算成本：分别是层次softmax和副采样算法； </p>
</blockquote>
<h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><p><img src="/2018/08/21/弄懂word2vec之原理解析/skip_gram_net_arch.png" alt="skip_gram_net_arch"></p>
<h2 id="负采样损失函数定义"><a href="#负采样损失函数定义" class="headerlink" title="负采样损失函数定义"></a>负采样损失函数定义</h2><p><img src="/2018/08/21/弄懂word2vec之原理解析/sampled-softmax.png" alt="sampled-softmax"></p>
<p>其中前一部分为真实结果的损失惩罚，后面的K部分为噪声词组的K次采样。</p>
<p>TensorFlow实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Number of negative labels to sample</span></span><br><span class="line">n_sampled = <span class="number">100</span></span><br><span class="line"><span class="keyword">with</span> train_graph.as_default():</span><br><span class="line">    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) <span class="comment"># create softmax weight matrix here</span></span><br><span class="line">    softmax_b = tf.Variable(tf.zeros(n_vocab), name=<span class="string">"softmax_bias"</span>) <span class="comment"># create softmax biases here</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate the loss using negative sampling</span></span><br><span class="line">    loss = tf.nn.sampled_softmax_loss(</span><br><span class="line">        weights=softmax_w,</span><br><span class="line">        biases=softmax_b,</span><br><span class="line">        labels=labels,</span><br><span class="line">        inputs=embed,</span><br><span class="line">        num_sampled=n_sampled,</span><br><span class="line">        num_classes=n_vocab)</span><br><span class="line">    </span><br><span class="line">    cost = tf.reduce_mean(loss)</span><br><span class="line">    optimizer = tf.train.AdamOptimizer().minimize(cost)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/29364112" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29364112</a></li>
<li><a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/representation/word2vec</a></li>
</ol>

      
    </div>
    <footer class="article-footer">

      
        <div id="donation_div"></div>

<script src="/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
  btnText: '打赏支持', // 可选参数，打赏按钮文字
  el: document.getElementById('donation_div'),
  wechatImage: '/qnsource/site/weixin.png',
  alipayImage: '/qnsource/site/alipay.jpg'
});
</script>
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>本文作者:  </strong>Ebby DD</a>
          </li>
          <li class="post-copyright-link">
          <strong>本文链接:  </strong>
          <a href="/2018/08/21/弄懂word2vec之原理解析/" target="_blank" title="弄懂word2vec之原理解析">http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/</a>
          </li>
          <li class="post-copyright-license">
            <strong>版权声明:   </strong>
            本博客所有文章除特别声明外，均采用 <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            许可协议。转载请注明出处
          </li>
         
        </ul>
<div>

      

      
        
 <!-- valine @ebby -->
 <section id="comments" class="comments">
	<style>
		.comments{margin:30px;padding:10px;background:#fff}
		@media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#fff}}
	</style>
	<div id="vcomment" class="comment"></div>
<script src="//cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script>
<script src="//cdn.jsdelivr.net/npm/leancloud-storage@latest/dist/av-min.js"></script>
<script src='//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js'></script>
<script>
   var notify = 'false' == true ? true : false;
   var verify = 'false' == true ? true : false;
   new Valine({
            av: AV,
            el: '#vcomment',
            notify: notify,
            verify: verify,
            app_id: "JQMgg0zbFBNWwL0lLnq2s1G7-gzGzoHsz",
            app_key: "m5FMVedFNxGutQCnMsVMAaXM",
            placeholder: "Say Something ...",
            avatar: "wavatar",
            avatar_cdn: "https://sdn.geekzu.org/avatar/",
            pageSize: "15"
    });
</script>
		
</section>



      
      <!--  -->

      <!--  -->
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/word2vec/">word2vec</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/原理解析/">原理解析</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/词嵌入/">词嵌入</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/08/21/FastText-A-better-choice-for-word-embedding/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          FastText-A better choice for word embedding?
        
      </div>
    </a>
  
  
    <a href="/2018/08/21/弄懂word2vec之实践/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">弄懂word2vec之实践</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#关键点提要"><span class="nav-number">1.</span> <span class="nav-text">关键点提要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#概述"><span class="nav-number">2.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练数据的定义"><span class="nav-number">3.</span> <span class="nav-text">训练数据的定义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#训练数据的切分"><span class="nav-number">3.1.</span> <span class="nav-text">训练数据的切分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网络的构建"><span class="nav-number">3.2.</span> <span class="nav-text">网络的构建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CBOW"><span class="nav-number">4.</span> <span class="nav-text">CBOW</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Skip-gram"><span class="nav-number">5.</span> <span class="nav-text">Skip-gram</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#负采样损失函数定义"><span class="nav-number">6.</span> <span class="nav-text">负采样损失函数定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol>
    
    </div>
  </aside>

</section>      
        
      </div>
      <!--  -->
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2017 - 2020 Ebby&#39;s Notes All Rights Reserved.</p>
	      
	      
  		   	<p id="copyRightCn">Ebby DD 保留所有权利</p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>    
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/reading" class="mobile-nav-link">Reading</a>
  
    <a href="/resources" class="mobile-nav-link">Resources</a>
  
    <a href="/notebooks" class="mobile-nav-link">📝</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/scripts.js"></script>




  <script src="/js/dialog.js"></script>











	<script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?684368b6943d7a5b6689dba5e7bf30ad";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Ebby&#39;s Notes
          </div>
          <div class="panel-body">
            Copyright © 2020 Ebby DD All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>