<!DOCTYPE html>
<html  lang="zh">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>弄懂word2vec之原理解析 - Ebby&#39;s Notes</title>


    <meta name="description" content="摘要：">
<meta name="keywords" content="人工智能,深度学习,技术,算法">
<meta property="og:type" content="article">
<meta property="og:title" content="弄懂word2vec之原理解析">
<meta property="og:url" content="http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/index.html">
<meta property="og:site_name" content="Ebby&#39;s Notes">
<meta property="og:description" content="摘要：">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.a-stack.com/images/og_image.png">
<meta property="og:updated_time" content="2018-08-24T15:18:05.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="弄懂word2vec之原理解析">
<meta name="twitter:description" content="摘要：">
<meta name="twitter:image" content="http://blog.a-stack.com/images/og_image.png">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/tomorrow-night-eighties.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?684368b6943d7a5b6689dba5e7bf30ad";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo02.png" alt="弄懂word2vec之原理解析" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/reading">读书</a>
                
                <a class="navbar-item"
                href="/resources">资源</a>
                
                <a class="navbar-item"
                href="/notebooks">📝</a>
                
                <a class="navbar-item"
                href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                

                <time class="level-item has-text-grey" datetime="2018-08-21T05:36:53.000Z">2018-08-21</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/NLP/">NLP</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    8 分钟 读完 (大约 1256 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-angle-double-right"></i>弄懂word2vec之原理解析
            
        </h1>
        <div class="content">
            <p><img src="/qnsource/banner/07.jpg" alt="Test Picture"></p>
<p><strong>摘要：</strong></p>
<a id="more"></a>
<h2 id="关键点提要"><a href="#关键点提要" class="headerlink" title="关键点提要"></a>关键点提要</h2><ul>
<li>word2vec不是深度学习模型（只有输入层、输出层）</li>
<li>效果并非最好，但易用性最强（其实FastText开源之后易用性直逼Word2vec）</li>
<li>受训练语料库影响大，尤其中文语料库本身缺乏</li>
</ul>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Word2vec核心算法包括CBOW和skip-gram，再加上两种目标优化方式层次Softmax和负采样的组合，可以形成总共四种实现方式。需要特别理解的一点，其实词向量是模型的中间产物（词向量矩阵在模型中为隐藏层的权重值）而不是优化目标产物。模型的训练数据为（sourceword，targetword）的词语对。本文简单整理近段时间通过翻看代码、原理解析总结的一些对word2vec的认知，希望能够加深对整合词嵌入模型的理解。</p>
<p><img src="/2018/08/21/弄懂word2vec之原理解析/word2vec_architectures.png" alt="word2vec_architectures"></p>
<p>其中</p>
<ul>
<li>CBOW的实现原理为由中心词来预测前后的相关词；</li>
<li>Skip-Gram正好相反，为由中心词的前后相关词来倒推中心词为某词的可能性；</li>
<li>无论哪种方法，都不需要事先对数据进行标记，换句话说这可以看作一种无监督的学习方法。</li>
</ul>
<blockquote>
<p>有相关研究也对两种方法的实现效果进行了比较，CBOW在小数据集上表现要优于Skip-Gram，在大数据集合上两者相当。</p>
</blockquote>
<p><img src="/2018/08/21/弄懂word2vec之原理解析/模型原理示意.png" alt="模型原理示意"></p>
<h2 id="训练数据的定义"><a href="#训练数据的定义" class="headerlink" title="训练数据的定义"></a>训练数据的定义</h2><blockquote>
<p>详见代码</p>
</blockquote>
<p>在生成训练数据之前先要明确窗口的概念，窗口的概念与n-gram中强调的前后词之前的语义关联有关，用于描述某个词语与其后多个词语有直接或间接的关系。在word2vec中使用的是中心词前后各n个词组成的窗口大小。原则上，窗口设置越大预测效果越好，但计算量等比例增大，一般选择5-8作为恰当的窗口值。</p>
<p><strong>Embedding Lookup的解释</strong>：词向量存储为一个以词典词个数为行，词嵌入维度为列的权重矩阵，而输入单词可以被编码为数字值，利用数值在词向量矩阵中获得词的向量表达的过程成为Embedding Lookup.</p>
<p><img src="/2018/08/21/弄懂word2vec之原理解析/tokenize_lookup.png" alt="tokenize_lookup"></p>
<p>Words that show up often such as “the”, “of”, and “for” don’t provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we’ll discard it with probability given by </p>
<script type="math/tex; mode=display">
 P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}</script><p>where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.</p>
<h3 id="训练数据的切分"><a href="#训练数据的切分" class="headerlink" title="训练数据的切分"></a>训练数据的切分</h3><p><img src="/2018/08/21/弄懂word2vec之原理解析/training_data.png" alt="training_data"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">corpus = <span class="string">''' I read magazine weekly . </span></span><br><span class="line"><span class="string">I read newspaper daily . I like to read books daily .'''</span></span><br><span class="line"></span><br><span class="line">corpus_tokens = []</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus.split(<span class="string">'.'</span>):</span><br><span class="line">    corpus_tokens.append(text.lower().split())</span><br><span class="line"></span><br><span class="line"><span class="comment">########## Create vocabulary </span></span><br><span class="line">vocab = &#123;&#125;</span><br><span class="line">counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> sentence_tokens <span class="keyword">in</span> corpus_tokens:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence_tokens:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            vocab[word] = counter</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########## Create training pairs .</span></span><br><span class="line">window = <span class="number">5</span></span><br><span class="line">target_words = []</span><br><span class="line">target_words_index = []</span><br><span class="line">c = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> corpus_tokens:</span><br><span class="line">    <span class="keyword">for</span> index,center_word <span class="keyword">in</span> enumerate(i):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            right = i[index+<span class="number">1</span>: index+<span class="number">1</span>+window]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            right == []</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            left = i[index<span class="number">-1</span>: (index<span class="number">-1</span>)+window]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            left == []</span><br><span class="line">        total_words = list(set(right+left))</span><br><span class="line">        <span class="keyword">if</span> total_words:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> total_words:</span><br><span class="line">                <span class="keyword">if</span> center_word <span class="keyword">in</span> vocab <span class="keyword">and</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">                    target_words.append((center_word, word))</span><br><span class="line">                    target_words_index.append((vocab[center_word], vocab[word]))</span><br><span class="line">    c += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="网络的构建"><a href="#网络的构建" class="headerlink" title="网络的构建"></a>网络的构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">embedding_dimension = <span class="number">100</span></span><br><span class="line">n_classes           = len(vocab) </span><br><span class="line">X = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>, (len(vocab), embedding_dimension)) <span class="comment"># input features for X </span></span><br><span class="line">W = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>, (len(vocab), embedding_dimension))   </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> each_pair <span class="keyword">in</span> target_words_index:</span><br><span class="line">    </span><br><span class="line">    inp_pair_index = each_pair[<span class="number">0</span>]</span><br><span class="line">    op_pair_index  = each_pair[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    x_input = random_embeddings[inp_pair_index] <span class="comment"># 1 x embedding_dimension </span></span><br><span class="line">    </span><br><span class="line">    probs = softmax(np.dot(W, x_input))        <span class="comment"># 1 x n_classes   </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">####### loss(probs, op_pair_index)</span></span><br><span class="line">    <span class="comment">####### backpropagate , update W</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>虽然示意代码中使用了softmax，但从计算量上来看这并非一种好的方法； Word2vec中使用了两种替代方案，可以大大节约计算成本：分别是层次softmax和副采样算法； </p>
</blockquote>
<h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><p><img src="/2018/08/21/弄懂word2vec之原理解析/skip_gram_net_arch.png" alt="skip_gram_net_arch"></p>
<h2 id="负采样损失函数定义"><a href="#负采样损失函数定义" class="headerlink" title="负采样损失函数定义"></a>负采样损失函数定义</h2><p><img src="/2018/08/21/弄懂word2vec之原理解析/sampled-softmax.png" alt="sampled-softmax"></p>
<p>其中前一部分为真实结果的损失惩罚，后面的K部分为噪声词组的K次采样。</p>
<p>TensorFlow实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Number of negative labels to sample</span></span><br><span class="line">n_sampled = <span class="number">100</span></span><br><span class="line"><span class="keyword">with</span> train_graph.as_default():</span><br><span class="line">    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) <span class="comment"># create softmax weight matrix here</span></span><br><span class="line">    softmax_b = tf.Variable(tf.zeros(n_vocab), name=<span class="string">"softmax_bias"</span>) <span class="comment"># create softmax biases here</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate the loss using negative sampling</span></span><br><span class="line">    loss = tf.nn.sampled_softmax_loss(</span><br><span class="line">        weights=softmax_w,</span><br><span class="line">        biases=softmax_b,</span><br><span class="line">        labels=labels,</span><br><span class="line">        inputs=embed,</span><br><span class="line">        num_sampled=n_sampled,</span><br><span class="line">        num_classes=n_vocab)</span><br><span class="line">    </span><br><span class="line">    cost = tf.reduce_mean(loss)</span><br><span class="line">    optimizer = tf.train.AdamOptimizer().minimize(cost)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/29364112">https://zhuanlan.zhihu.com/p/29364112</a></li>
<li><a href="https://www.tensorflow.org/tutorials/representation/word2vec">https://www.tensorflow.org/tutorials/representation/word2vec</a></li>
</ol>

        </div>
        
        <hr style="height:1px;margin:1rem 0"/>
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/word2vec/">word2vec</a>,&nbsp;<a class="has-link-grey -link" href="/tags/原理解析/">原理解析</a>,&nbsp;<a class="has-link-grey -link" href="/tags/词嵌入/">词嵌入</a>
                </div>
            </div>
        </div>
        
        
        
        
<div class="sharethis-inline-share-buttons"></div>
<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=5e762f775039a80012d346d9&amp;product=inline-share-buttons&amp;cms=sop' async='async'></script>

        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/site/alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/site/weixin.png" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2018/08/21/FastText-A-better-choice-for-word-embedding/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">FastText-A better choice for word embedding?</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2018/08/21/弄懂word2vec之实践/">
                <span class="level-item">弄懂word2vec之实践</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: false,
        verify: false,
        app_id: 'JQMgg0zbFBNWwL0lLnq2s1G7-gzGzoHsz',
        app_key: 'm5FMVedFNxGutQCnMsVMAaXM',
        placeholder: 'Say Something ...'
    });
</script>

    </div>
</div>
</div>
                
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-3 column-right is-sticky">
    
        

    <div class="card widget" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#关键点提要">
        <span class="has-mr-6">1</span>
        <span>关键点提要</span>
        </a></li><li>
        <a class="is-flex" href="#概述">
        <span class="has-mr-6">2</span>
        <span>概述</span>
        </a></li><li>
        <a class="is-flex" href="#训练数据的定义">
        <span class="has-mr-6">3</span>
        <span>训练数据的定义</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#训练数据的切分">
        <span class="has-mr-6">3.1</span>
        <span>训练数据的切分</span>
        </a></li><li>
        <a class="is-flex" href="#网络的构建">
        <span class="has-mr-6">3.2</span>
        <span>网络的构建</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#CBOW">
        <span class="has-mr-6">4</span>
        <span>CBOW</span>
        </a></li><li>
        <a class="is-flex" href="#Skip-gram">
        <span class="has-mr-6">5</span>
        <span>Skip-gram</span>
        </a></li><li>
        <a class="is-flex" href="#负采样损失函数定义">
        <span class="has-mr-6">6</span>
        <span>负采样损失函数定义</span>
        </a></li><li>
        <a class="is-flex" href="#参考">
        <span class="has-mr-6">7</span>
        <span>参考</span>
        </a></li></ul>
            </div>
        </div>
    </div>

    
        <div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2021/02/05/那些效率提升的工具与方法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="那些效率提升的工具与方法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2021-02-05T14:10:00.000Z">2021-02-05</time></div>
                    <a href="/2021/02/05/那些效率提升的工具与方法/" class="title has-link-black-ter is-size-6 has-text-weight-normal">那些效率提升的工具与方法</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/总结/">总结</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/blog/rl.jpg" alt="Reinforcement Learning:Tips and Tricks">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2021-01-28T02:56:30.000Z">2021-01-28</time></div>
                    <a href="/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Reinforcement Learning:Tips and Tricks</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/强化学习/">强化学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/11/19/pytorch模型的导出与部署/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="pytorch模型的导出与部署">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-11-19T05:00:18.000Z">2020-11-19</time></div>
                    <a href="/2020/11/19/pytorch模型的导出与部署/" class="title has-link-black-ter is-size-6 has-text-weight-normal">pytorch模型的导出与部署</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/05/04/读书笔记-《神经网络与深度学习》/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/blog/qxp.jpg" alt="读书笔记-《神经网络与深度学习》">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-05-04T13:50:21.000Z">2020-05-04</time></div>
                    <a href="/2020/05/04/读书笔记-《神经网络与深度学习》/" class="title has-link-black-ter is-size-6 has-text-weight-normal">读书笔记-《神经网络与深度学习》</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/04/19/计算机视觉目标检测研究札记/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/banner/02.jpg" alt="计算机视觉目标检测研究札记">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-04-19T07:10:45.000Z">2020-04-19</time></div>
                    <a href="/2020/04/19/计算机视觉目标检测研究札记/" class="title has-link-black-ter is-size-6 has-text-weight-normal">计算机视觉目标检测研究札记</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo02.png" alt="弄懂word2vec之原理解析" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Ebby DD&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>  沪ICP备20005404号
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://blog.a-stack.com',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>