<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>„ÄäÊú∫Âô®Â≠¶‰π†„ÄãËØæÁ®ãÂÆûÈ™åÂõûÈ°æ | Ebby&#39;s Notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="‰∫∫Â∑•Êô∫ËÉΩ,Ê∑±Â∫¶Â≠¶‰π†,ÊäÄÊúØ,ÁÆóÊ≥ï">
  
  
  
  
  <meta name="description" content="ÊëòË¶ÅÔºö Cousera‰∏äÊú∫Âô®Â≠¶‰π†ËØæÁ®ãÊèê‰æõ‰∫ÜÂÖ´‰∏™ÂÆûÈ™å‰Ωú‰∏öÔºå‰ΩøÁî®MATLABËøõË°åÂä®ÊâãÂÆûÈ™åÔºåÁÜüÊÇâÁõ∏ÂÖ≥ÊäÄÊúØÔºåÊú¨ÊñáÂØπÂÆûÈ™å‰∏≠ÁöÑÂÖ≥ÈîÆÂÜÖÂÆπËøõË°åÊÄªÁªìÂΩíÁ∫≥ÔºåÊñπ‰æø‰ª•ÂêéÂèäÊó∂Êü•Êâæ„ÄÇ  Ê¶ÇË¶Å ÂèØ‰ª•‰ΩøÁî®Matlab OnlineËøõË°å‰ª£Á†ÅÊµãËØïÔºåÂú∞ÂùÄ; Áõ∏ÂÖ≥‰ª£Á†ÅÂ∑≤Áªè‰º†ËæìËá≥GithubÔºö https://github.com/ddebby/machine_learning_cousera.git  Lab1: Á∫øÊÄßÂõûÂΩí1. ÁÆÄÂçïÁ∫øÊÄß">
<meta name="keywords" content="‰∫∫Â∑•Êô∫ËÉΩ,Ê∑±Â∫¶Â≠¶‰π†,ÊäÄÊúØ,ÁÆóÊ≥ï">
<meta property="og:type" content="article">
<meta property="og:title" content="„ÄäÊú∫Âô®Â≠¶‰π†„ÄãËØæÁ®ãÂÆûÈ™åÂõûÈ°æ">
<meta property="og:url" content="http://blog.a-stack.com/2018/04/26/machinelearning-labs/index.html">
<meta property="og:site_name" content="Ebby&#39;s Notes">
<meta property="og:description" content="ÊëòË¶ÅÔºö Cousera‰∏äÊú∫Âô®Â≠¶‰π†ËØæÁ®ãÊèê‰æõ‰∫ÜÂÖ´‰∏™ÂÆûÈ™å‰Ωú‰∏öÔºå‰ΩøÁî®MATLABËøõË°åÂä®ÊâãÂÆûÈ™åÔºåÁÜüÊÇâÁõ∏ÂÖ≥ÊäÄÊúØÔºåÊú¨ÊñáÂØπÂÆûÈ™å‰∏≠ÁöÑÂÖ≥ÈîÆÂÜÖÂÆπËøõË°åÊÄªÁªìÂΩíÁ∫≥ÔºåÊñπ‰æø‰ª•ÂêéÂèäÊó∂Êü•Êâæ„ÄÇ  Ê¶ÇË¶Å ÂèØ‰ª•‰ΩøÁî®Matlab OnlineËøõË°å‰ª£Á†ÅÊµãËØïÔºåÂú∞ÂùÄ; Áõ∏ÂÖ≥‰ª£Á†ÅÂ∑≤Áªè‰º†ËæìËá≥GithubÔºö https://github.com/ddebby/machine_learning_cousera.git  Lab1: Á∫øÊÄßÂõûÂΩí1. ÁÆÄÂçïÁ∫øÊÄß">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.a-stack.com/OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_01.jpg">
<meta property="og:image" content="http://blog.a-stack.com/OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_02.jpg">
<meta property="og:image" content="http://blog.a-stack.com/OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_03.jpg">
<meta property="og:image" content="http://blog.a-stack.com/OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_04.jpg">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab7_01.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab7_02.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab7_03.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab7_04.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab7_05.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab7_10.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab7_09.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab8_01.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab8_02.png">
<meta property="og:image" content="http://blog.a-stack.com/2018/04/qnsource/images/2018-04-26-machinelearning-labs/lab8_03.png">
<meta property="og:updated_time" content="2018-05-15T09:14:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="„ÄäÊú∫Âô®Â≠¶‰π†„ÄãËØæÁ®ãÂÆûÈ™åÂõûÈ°æ">
<meta name="twitter:description" content="ÊëòË¶ÅÔºö Cousera‰∏äÊú∫Âô®Â≠¶‰π†ËØæÁ®ãÊèê‰æõ‰∫ÜÂÖ´‰∏™ÂÆûÈ™å‰Ωú‰∏öÔºå‰ΩøÁî®MATLABËøõË°åÂä®ÊâãÂÆûÈ™åÔºåÁÜüÊÇâÁõ∏ÂÖ≥ÊäÄÊúØÔºåÊú¨ÊñáÂØπÂÆûÈ™å‰∏≠ÁöÑÂÖ≥ÈîÆÂÜÖÂÆπËøõË°åÊÄªÁªìÂΩíÁ∫≥ÔºåÊñπ‰æø‰ª•ÂêéÂèäÊó∂Êü•Êâæ„ÄÇ  Ê¶ÇË¶Å ÂèØ‰ª•‰ΩøÁî®Matlab OnlineËøõË°å‰ª£Á†ÅÊµãËØïÔºåÂú∞ÂùÄ; Áõ∏ÂÖ≥‰ª£Á†ÅÂ∑≤Áªè‰º†ËæìËá≥GithubÔºö https://github.com/ddebby/machine_learning_cousera.git  Lab1: Á∫øÊÄßÂõûÂΩí1. ÁÆÄÂçïÁ∫øÊÄß">
<meta name="twitter:image" content="http://blog.a-stack.com/OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_01.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Ebby&#39;s Notes" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/logo.png">
  <link rel="apple-touch-icon" href="/css/images/logo.png">
  
    <link href="/qnsource/css/Source+Code+Pro.css" rel="stylesheet" type="text/css">
  
  <link href="/qnsource/css/Open+Sans-Montserrat-700.css" rel="stylesheet" type="text/css">
  <link href="/qnsource/css/Roboto-400-300.css" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>
  <script src="/js/bootstrap.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css">

  
    <link rel="stylesheet" href="/css/dialog.css">
  

  

  
    <link rel="stylesheet" href="/css/header-post.css">
  

  
  
  
    <link rel="stylesheet" href="/css/vdonate.css">
  

</head>
</html>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 border-width: 0px;  margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="200px" height="100px" alt="Hike News" src="/css/images/logo.png">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">È¶ñÈ°µ</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">ÂΩíÊ°£</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">ÂàÜÁ±ª</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Ê†áÁ≠æ</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">ÂÖ≥‰∫é</a> </li>
                
                  <li> <a class="main-nav-link" href="/reading">ËØª‰π¶</a> </li>
                
                  <li> <a class="main-nav-link" href="/resources">ËµÑÊ∫ê</a> </li>
                
                  <li> <a class="main-nav-link" href="/notebooks">üìù</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="ËØ∑ËæìÂÖ•ÂÖ≥ÈîÆËØç..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'ÊñáÁ´†',
            PAGES: 'È°µÈù¢',
            CATEGORIES: 'ÂàÜÁ±ª',
            TAGS: 'Ê†áÁ≠æ',
            UNTITLED: '(Êó†Ê†áÈ¢ò)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-machinelearning-labs" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      „ÄäÊú∫Âô®Â≠¶‰π†„ÄãËØæÁ®ãÂÆûÈ™åÂõûÈ°æ
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2018/04/26/machinelearning-labs/" class="article-date">
	  <time datetime="2018-04-26T05:56:28.000Z" itemprop="datePublished">2018-04-26</time>
	</a>

      
    <a class="article-category-link" href="/categories/Âä®ÊâãÂÆûË∑µËê•/">Âä®ÊâãÂÆûË∑µËê•</a>

      
      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>ÊëòË¶ÅÔºö</strong> Cousera‰∏äÊú∫Âô®Â≠¶‰π†ËØæÁ®ãÊèê‰æõ‰∫ÜÂÖ´‰∏™ÂÆûÈ™å‰Ωú‰∏öÔºå‰ΩøÁî®MATLABËøõË°åÂä®ÊâãÂÆûÈ™åÔºåÁÜüÊÇâÁõ∏ÂÖ≥ÊäÄÊúØÔºåÊú¨ÊñáÂØπÂÆûÈ™å‰∏≠ÁöÑÂÖ≥ÈîÆÂÜÖÂÆπËøõË°åÊÄªÁªìÂΩíÁ∫≥ÔºåÊñπ‰æø‰ª•ÂêéÂèäÊó∂Êü•Êâæ„ÄÇ</p>
<!-- excerpt -->
<h2 id="Ê¶ÇË¶Å"><a href="#Ê¶ÇË¶Å" class="headerlink" title="Ê¶ÇË¶Å"></a>Ê¶ÇË¶Å</h2><ul>
<li>ÂèØ‰ª•‰ΩøÁî®Matlab OnlineËøõË°å‰ª£Á†ÅÊµãËØïÔºå<a href="https://matlab.mathworks.com/" target="_blank" rel="noopener">Âú∞ÂùÄ</a>;</li>
<li>Áõ∏ÂÖ≥‰ª£Á†ÅÂ∑≤Áªè‰º†ËæìËá≥GithubÔºö <a href="https://github.com/ddebby/machine_learning_cousera.git" target="_blank" rel="noopener">https://github.com/ddebby/machine_learning_cousera.git</a></li>
</ul>
<h2 id="Lab1-Á∫øÊÄßÂõûÂΩí"><a href="#Lab1-Á∫øÊÄßÂõûÂΩí" class="headerlink" title="Lab1: Á∫øÊÄßÂõûÂΩí"></a>Lab1: Á∫øÊÄßÂõûÂΩí</h2><h3 id="1-ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí"><a href="#1-ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí" class="headerlink" title="1. ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí"></a>1. ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí</h3><p>Âú®Á¨¨‰∏Ä‰∏™ÂÆûÈ™å‰∏≠Êèê‰æõ‰∫ÜÂçïÂèòÈáèÁ∫øÊÄßÂõûÂΩíÁöÑÊï∞ÊçÆÁî®Êù•È¢ÑÊµãÂø´È§êËΩ¶ÁöÑÁõàÂà©ÊÉÖÂÜµÔºåÂÖ∂‰∏≠ËæìÂÖ•‰∏∫ÂüéÂ∏Ç‰∏≠ÁöÑ‰∫∫Âè£‰ø°ÊÅØÔºåËæìÂá∫‰∏∫ËØ•ÂüéÂ∏ÇÂø´È§êËΩ¶ÁöÑÁõàÂà©ÊÉÖÂÜµÔºåÊï∞ÊçÆÂàÜÂ∏ÉÊÉÖÂÜµËØ¶ËßÅ‰∏ãÂõæÔºö</p>
<p><img src="/2018/04/26/machinelearning-labs/../../../../../OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_01.jpg" alt="linear_regression_01"></p>
<h4 id="‰ª£‰ª∑ÂáΩÊï∞ÂíåÊ¢ØÂ∫¶Êõ¥Êñ∞ÂÆûÁé∞"><a href="#‰ª£‰ª∑ÂáΩÊï∞ÂíåÊ¢ØÂ∫¶Êõ¥Êñ∞ÂÆûÁé∞" class="headerlink" title="‰ª£‰ª∑ÂáΩÊï∞ÂíåÊ¢ØÂ∫¶Êõ¥Êñ∞ÂÆûÁé∞"></a>‰ª£‰ª∑ÂáΩÊï∞ÂíåÊ¢ØÂ∫¶Êõ¥Êñ∞ÂÆûÁé∞</h4><p>ËÆ°ÁÆó‰ª£‰ª∑ÂáΩÊï∞ <code>computeCost.m</code> </p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">J</span> = <span class="title">computeCost</span><span class="params">(X, y, theta)</span></span></span><br><span class="line"><span class="comment">%COMPUTECOST Compute cost for linear regression</span></span><br><span class="line"><span class="comment">%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the</span></span><br><span class="line"><span class="comment">%   parameter for linear regression to fit the data points in X and y</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly </span></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line"><span class="comment">% Instructions: Compute the cost of a particular choice of theta</span></span><br><span class="line"><span class="comment">%               You should set J to the cost.</span></span><br><span class="line">J = <span class="number">1</span>/(<span class="number">2</span>*m) * (X*theta - y)'*(X*theta -y)</span><br><span class="line"><span class="comment">% =========================================================================</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>Ê¢ØÂ∫¶Êõ¥Êñ∞ <code>gradientDescent.m</code></p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></span><br><span class="line"><span class="comment">%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by </span></span><br><span class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line"></span><br><span class="line">    <span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment">% Instructions: Perform a single gradient step on the parameter vector</span></span><br><span class="line">    <span class="comment">%               theta. </span></span><br><span class="line">    <span class="comment">%</span></span><br><span class="line">    <span class="comment">% Hint: While debugging, it can be useful to print out the values</span></span><br><span class="line">    <span class="comment">%       of the cost function (computeCost) and gradient here.</span></span><br><span class="line">    <span class="comment">%</span></span><br><span class="line"></span><br><span class="line">    theta = theta - alpha/m * X'*(X*theta - y);</span><br><span class="line"></span><br><span class="line">    <span class="comment">% ============================================================</span></span><br><span class="line">    <span class="comment">% Save the cost J in every iteration    </span></span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>ÂÆûÁé∞Âπ∂ÊµãËØïÔºö</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">data = load(<span class="string">'ex1data1.txt'</span>);</span><br><span class="line">X = data(:, <span class="number">1</span>); y = data(:, <span class="number">2</span>);</span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line"></span><br><span class="line">X = [ones(m, <span class="number">1</span>), data(:,<span class="number">1</span>)]; <span class="comment">% Add a column of ones to x</span></span><br><span class="line">theta = <span class="built_in">zeros</span>(<span class="number">2</span>, <span class="number">1</span>); <span class="comment">% initialize fitting parameters</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Some gradient descent settings</span></span><br><span class="line">iterations = <span class="number">1500</span>;</span><br><span class="line">alpha = <span class="number">0.01</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% compute and display initial cost</span></span><br><span class="line">J = computeCost(X, y, theta);</span><br><span class="line"></span><br><span class="line"><span class="comment">% further testing of the cost function</span></span><br><span class="line">J = computeCost(X, y, [<span class="number">-1</span> ; <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line"><span class="comment">% run gradient descent</span></span><br><span class="line">theta = gradientDescent(X, y, theta, alpha, iterations);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Plot the linear fit</span></span><br><span class="line">hold on; <span class="comment">% keep previous plot visible</span></span><br><span class="line">plot(X(:,<span class="number">2</span>), X*theta, <span class="string">'-'</span>)</span><br><span class="line">legend(<span class="string">'Training data'</span>, <span class="string">'Linear regression'</span>)</span><br><span class="line">hold off <span class="comment">% don't overlay any more plots on this figure</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Predict values for population sizes of 35,000 and 70,000</span></span><br><span class="line">predict1 = [<span class="number">1</span>, <span class="number">3.5</span>] *theta;</span><br><span class="line">fprintf(<span class="string">'For population = 35,000, we predict a profit of %f\n'</span>,...</span><br><span class="line">    predict1*<span class="number">10000</span>);</span><br><span class="line">predict2 = [<span class="number">1</span>, <span class="number">7</span>] * theta;</span><br><span class="line">fprintf(<span class="string">'For population = 70,000, we predict a profit of %f\n'</span>,...</span><br><span class="line">    predict2*<span class="number">10000</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2018/04/26/machinelearning-labs/../../../../../OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_02.jpg" alt="linear_regression_02"></p>
<h4 id="J-theta-ÁöÑÂèØËßÜÂåñ"><a href="#J-theta-ÁöÑÂèØËßÜÂåñ" class="headerlink" title="$J(\theta)$ ÁöÑÂèØËßÜÂåñ"></a>$J(\theta)$ ÁöÑÂèØËßÜÂåñ</h4><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% ============= Part 4: Visualizing J(theta_0, theta_1) =============</span></span><br><span class="line">fprintf(<span class="string">'Visualizing J(theta_0, theta_1) ...\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">% Grid over which we will calculate J</span></span><br><span class="line">theta0_vals = <span class="built_in">linspace</span>(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">100</span>);</span><br><span class="line">theta1_vals = <span class="built_in">linspace</span>(<span class="number">-1</span>, <span class="number">4</span>, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% initialize J_vals to a matrix of 0's</span></span><br><span class="line">J_vals = <span class="built_in">zeros</span>(<span class="built_in">length</span>(theta0_vals), <span class="built_in">length</span>(theta1_vals));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Fill out J_vals</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">length</span>(theta0_vals)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">length</span>(theta1_vals)</span><br><span class="line">	  t = [theta0_vals(i); theta1_vals(j)];</span><br><span class="line">	  J_vals(<span class="built_in">i</span>,<span class="built_in">j</span>) = computeCost(X, y, t);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Because of the way meshgrids work in the surf command, we need to</span></span><br><span class="line"><span class="comment">% transpose J_vals before calling surf, or else the axes will be flipped</span></span><br><span class="line">J_vals = J_vals';</span><br><span class="line"><span class="comment">% Surface plot</span></span><br><span class="line">figure;</span><br><span class="line">surf(theta0_vals, theta1_vals, J_vals)</span><br><span class="line">xlabel(<span class="string">'\theta_0'</span>); ylabel(<span class="string">'\theta_1'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Contour plot</span></span><br><span class="line">figure;</span><br><span class="line"><span class="comment">% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100</span></span><br><span class="line">contour(theta0_vals, theta1_vals, J_vals, <span class="built_in">logspace</span>(<span class="number">-2</span>, <span class="number">3</span>, <span class="number">20</span>))</span><br><span class="line">xlabel(<span class="string">'\theta_0'</span>); ylabel(<span class="string">'\theta_1'</span>);</span><br><span class="line">hold on;</span><br><span class="line">plot(theta(<span class="number">1</span>), theta(<span class="number">2</span>), <span class="string">'rx'</span>, <span class="string">'MarkerSize'</span>, <span class="number">10</span>, <span class="string">'LineWidth'</span>, <span class="number">2</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2018/04/26/machinelearning-labs/../../../../../OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_03.jpg" alt="linear_regression_03"></p>
<p><img src="/2018/04/26/machinelearning-labs/../../../../../OneDrive%20-%20%E5%BE%90%E6%B1%87%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E6%BC%95%E6%B2%B3%E6%B3%BE%E5%BC%80%E5%8F%91%E5%8C%BA%E5%90%88%E4%BD%9C%E7%BB%BC%E5%90%88%E5%8D%8F%E8%B0%83%E5%8A%9E%E5%85%AC%E5%AE%A4/Ebby/blog/qnsource/images/2018-04-25-machine-learning-notes/linear_regression_04.jpg" alt="linear_regression_04"></p>
<h3 id="2-Â§öÂÖÉÁ∫øÊÄßÂõûÂΩí"><a href="#2-Â§öÂÖÉÁ∫øÊÄßÂõûÂΩí" class="headerlink" title="2. Â§öÂÖÉÁ∫øÊÄßÂõûÂΩí"></a>2. Â§öÂÖÉÁ∫øÊÄßÂõûÂΩí</h3><p>Âà©Áî®Â§öÂÖÉÁ∫øÊÄßÂõûÂΩíÈ¢ÑÊµãÊàø‰ª∑ÔºåËæìÂÖ•ÁöÑÁâπÂæÅÂåÖÊã¨ÊàøÂ≠êÂ§ßÂ∞è„ÄÅÊàøÈó¥Êï∞ÁõÆÔºåËæìÂá∫‰∏∫ÊàøÂ≠ê‰ª∑Ê†º„ÄÇ</p>
<h3 id="2-1-ÁâπÂæÅÂΩí‰∏ÄÂåñ"><a href="#2-1-ÁâπÂæÅÂΩí‰∏ÄÂåñ" class="headerlink" title="2.1 ÁâπÂæÅÂΩí‰∏ÄÂåñ"></a>2.1 ÁâπÂæÅÂΩí‰∏ÄÂåñ</h3><p><code>featureNormalize.m</code></p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[X_norm, mu, sigma]</span> = <span class="title">featureNormalize</span><span class="params">(X)</span></span></span><br><span class="line"><span class="comment">%FEATURENORMALIZE Normalizes the features in X </span></span><br><span class="line"><span class="comment">%   FEATURENORMALIZE(X) returns a normalized version of X where</span></span><br><span class="line"><span class="comment">%   the mean value of each feature is 0 and the standard deviation</span></span><br><span class="line"><span class="comment">%   is 1. This is often a good preprocessing step to do when</span></span><br><span class="line"><span class="comment">%   working with learning algorithms.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to set these values correctly</span></span><br><span class="line">X_norm = X;</span><br><span class="line">mu = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="built_in">size</span>(X, <span class="number">2</span>));</span><br><span class="line">sigma = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="built_in">size</span>(X, <span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line"><span class="comment">%       </span></span><br><span class="line">mu = mean(X,<span class="number">1</span>);</span><br><span class="line">sigma = std(X,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">X_norm = (X - mu)./sigma;</span><br><span class="line"><span class="comment">% ============================================================</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ul>
<li>The <code>bsxfun</code> is helpful for applying a function (limited to two arguments) in an element-wise fashion to rows of a matrix using a vector of source values. This is useful for feature normalization. An example you can enter at the octave command line:</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Z=[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>; <span class="number">2</span> <span class="number">2</span> <span class="number">2</span>;];</span><br><span class="line"></span><br><span class="line">v=[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="built_in">bsxfun</span>(@minus,Z,v);</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>In this case, the corresponding elements of v are subtracted from each row of Z. The minus(a,b) function is equivalent to computing (a-b).</li>
</ul>
<ul>
<li>‰ª£‰ª∑ÂáΩÊï∞ÂíåÊ¢ØÂ∫¶Êõ¥Êñ∞‰∏éÁÆÄÂçïÁ∫øÊÄßÂõûÂΩíÁõ∏ÂêåÔºõ</li>
</ul>
<h3 id="2-2-ËÆ°ÁÆó-theta-ÁöÑÊï∞ÂÄºËß£"><a href="#2-2-ËÆ°ÁÆó-theta-ÁöÑÊï∞ÂÄºËß£" class="headerlink" title="2.2 ËÆ°ÁÆó $\theta$ ÁöÑÊï∞ÂÄºËß£"></a>2.2 ËÆ°ÁÆó $\theta$ ÁöÑÊï∞ÂÄºËß£</h3><p><code>normalEqn.m</code></p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta]</span> = <span class="title">normalEqn</span><span class="params">(X, y)</span></span></span><br><span class="line"><span class="comment">%NORMALEQN Computes the closed-form solution to linear regression </span></span><br><span class="line"><span class="comment">%   NORMALEQN(X,y) computes the closed-form solution to linear </span></span><br><span class="line"><span class="comment">%   regression using the normal equations.</span></span><br><span class="line"></span><br><span class="line">theta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">2</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line"><span class="comment">% Instructions: Complete the code to compute the closed form solution</span></span><br><span class="line"><span class="comment">%               to linear regression and put the result in theta.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% ---------------------- Sample Solution ----------------------</span></span><br><span class="line">theta = pinv(X'*X)*X'*y;</span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% ============================================================</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h2 id="Lab-2-Lab-5"><a href="#Lab-2-Lab-5" class="headerlink" title="Lab 2 - Lab 5:"></a>Lab 2 - Lab 5:</h2><blockquote>
<p>Áïô‰ΩúÂêéÁª≠Êõ¥Êñ∞ ‚Ä¶</p>
</blockquote>
<h2 id="Lab-6-SVM"><a href="#Lab-6-SVM" class="headerlink" title="Lab 6: SVM"></a>Lab 6: SVM</h2><p>Êú¨ÂÆûÈ™åÂà©Áî®SVMÂÆûÁé∞ÈùûÁ∫øÊÄßÂàÜÁ±ªÂô®ÔºåÊ†∏ÂáΩÊï∞ÈÄâÊã©È´òÊñØÊ†∏ÂáΩÊï∞ÔºåÈ´òÊñØÊ†∏ÂáΩÊï∞ÁöÑÂÆö‰πâÂ¶Ç‰∏ãÔºö</p>
<script type="math/tex; mode=display">
K_{gaussian}(x^{(i)},x^{(j)}) = exp(-\frac{||x^{(i)}-x^{(j)}||^2}{2\sigma^2})</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% ========== Part 5: Training SVM with RBF Kernel (Dataset 2) ==========</span></span><br><span class="line"><span class="comment">%  After you have implemented the kernel, we can now use it to train the </span></span><br><span class="line"><span class="comment">%  SVM classifier.</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line">load(<span class="string">'ex6data2.mat'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% SVM Parameters</span></span><br><span class="line">C = <span class="number">1</span>; sigma = <span class="number">0.1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% We set the tolerance and max_passes lower here so that the code will run</span></span><br><span class="line"><span class="comment">% faster. However, in practice, you will want to run the training to</span></span><br><span class="line"><span class="comment">% convergence.</span></span><br><span class="line">model= svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma)); </span><br><span class="line">visualizeBoundary(X, y, model);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Áî±‰∫éÁõÆÂâçscikit-learnÂ∫ìÂØπÂêÑÁßçSVMÁÆóÊ≥ïÈÉΩÊúâÊØîËæÉÂ•ΩÁöÑÂ∞ÅË£ÖÔºåÊâÄ‰ª•‰πü‰∏çÂú®Ëøõ‰∏ÄÊ≠•Ê∑±Á©∂matlabÁöÑÂÆûÁé∞‰∫Ü„ÄÇ</p>
</blockquote>
<h2 id="Lab-7-1-K-Means"><a href="#Lab-7-1-K-Means" class="headerlink" title="Lab 7.1: K-Means"></a>Lab 7.1: K-Means</h2><p>Êú¨ÂÆûÈ™å‰ΩøÁî®K-MeansËøõË°åÂõæÂÉèÂéãÁº©ÔºåK-MeansÁÆóÊ≥ïÁöÑÂü∫Êú¨ÂÆûÁé∞Ôºö</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Initialize centroids</span></span><br><span class="line">centroids = kMeansInitCentroids(X, K);</span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:iterations</span><br><span class="line">    <span class="comment">% Cluster assignment step: Assign each data point to the</span></span><br><span class="line">    <span class="comment">% closest centroid. idx(i) corresponds to cÀÜ(i), the index</span></span><br><span class="line">    <span class="comment">% of the centroid assigned to example i</span></span><br><span class="line">    idx = findClosestCentroids(X, centroids);</span><br><span class="line">    <span class="comment">% Move centroid step: Compute means based on centroid</span></span><br><span class="line">    <span class="comment">% assignments</span></span><br><span class="line">    centroids = computeMeans(X, idx, K);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>Âú®Âæ™ÁéØ‰∏≠ÂÖ≥ÈîÆÂÆûÁé∞‰∏§‰∏™Ê≠•È™§Ôºö1ÔºâÈáçÊñ∞ÂàíÂàÜËÅöÁ±ªÔºõ2ÔºâËÆ°ÁÆóÊñ∞ÁöÑÂùáÂÄºÂèä‰∏≠ÂøÉÁÇπ</p>
<ol>
<li>ÂØªÊâæÊúÄËøëÁöÑËÅöÁ±ª‰∏≠ÂøÉÁÇπÔºåÂπ∂ËÅöÁ±ª<script type="math/tex; mode=display">
c^{(i)}:=j \ that \ minimizes \ ||x^{(i)} - \mu_j ||^2</script></li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">idx</span> = <span class="title">findClosestCentroids</span><span class="params">(X, centroids)</span></span></span><br><span class="line"><span class="comment">%FINDCLOSESTCENTROIDS computes the centroid memberships for every example</span></span><br><span class="line"><span class="comment">%   idx = FINDCLOSESTCENTROIDS (X, centroids) returns the closest centroids</span></span><br><span class="line"><span class="comment">%   in idx for a dataset X where each row is a single example. idx = m x 1 </span></span><br><span class="line"><span class="comment">%   vector of centroid assignments (i.e. each entry in range [1..K])</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Set K</span></span><br><span class="line">K = <span class="built_in">size</span>(centroids, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly.</span></span><br><span class="line">idx = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X,<span class="number">1</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Instructions: Go over every example, find its closest centroid, and store</span></span><br><span class="line"><span class="comment">%               the index inside idx at the appropriate location.</span></span><br><span class="line"><span class="comment">%               Concretely, idx(i) should contain the index of the centroid</span></span><br><span class="line"><span class="comment">%               closest to example i. Hence, it should be a value in the </span></span><br><span class="line"><span class="comment">%               range 1..K</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Note: You can use a for-loop over the examples to compute this.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="built_in">size</span>(X,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>: <span class="built_in">size</span>(centroids,<span class="number">1</span>)</span><br><span class="line">         dis(<span class="built_in">j</span>) = sum((centroids(<span class="built_in">j</span>, :) - X(<span class="built_in">i</span>, :)) .^ <span class="number">2</span>, <span class="number">2</span>);  </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    [t,idx(i)] = min(dis);</span><br><span class="line"><span class="keyword">end</span>  </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol>
<li>Êõ¥Êñ∞ËÅöÁ±ª‰∏≠ÂøÉÁÇπ‰ΩçÁΩÆ</li>
</ol>
<script type="math/tex; mode=display">
\mu_k = \frac{1}{|C_k|} \sum_{i \in C_k} x^{(i)}</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">centroids</span> = <span class="title">computeCentroids</span><span class="params">(X, idx, K)</span></span></span><br><span class="line"><span class="comment">%COMPUTECENTROIDS returns the new centroids by computing the means of the </span></span><br><span class="line"><span class="comment">%data points assigned to each centroid.</span></span><br><span class="line"><span class="comment">%   centroids = COMPUTECENTROIDS(X, idx, K) returns the new centroids by </span></span><br><span class="line"><span class="comment">%   computing the means of the data points assigned to each centroid. It is</span></span><br><span class="line"><span class="comment">%   given a dataset X where each row is a single data point, a vector</span></span><br><span class="line"><span class="comment">%   idx of centroid assignments (i.e. each entry in range [1..K]) for each</span></span><br><span class="line"><span class="comment">%   example, and K, the number of centroids. You should return a matrix</span></span><br><span class="line"><span class="comment">%   centroids, where each row of centroids is the mean of the data points</span></span><br><span class="line"><span class="comment">%   assigned to it.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Useful variables</span></span><br><span class="line">[m n] = <span class="built_in">size</span>(X);</span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly.</span></span><br><span class="line">centroids = <span class="built_in">zeros</span>(K, n);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:K</span><br><span class="line">    centroids(<span class="built_in">i</span>,:) = mean(X(<span class="built_in">find</span>(idx==<span class="built_in">i</span>),:));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab7_01.png" alt="lab7_01"></p>
<p>3.ÂàùÂßãËµ∑ÁÇπÁöÑÈöèÊú∫ÈÄâÂèñ</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Randomly reorder the indices of examples </span></span><br><span class="line">randidx = randperm(<span class="built_in">size</span>(X, <span class="number">1</span>)); </span><br><span class="line"><span class="comment">% Take the first K examples as centroids </span></span><br><span class="line">centroids = X(randidx(<span class="number">1</span>:K), :);</span><br></pre></td></tr></table></figure>
<ol>
<li>‰ΩøÁî®K-MeansÂéãÁº©ÂõæÁâá</li>
</ol>
<p>Êàë‰ª¨Â∞Ü‰∏ÄÂº†24bitÁöÑÁÖßÁâáÔºåÂéãÁº©‰∏∫4-bitÔºà16‰∏™Ëâ≤ÂΩ©ÔºâÔºõÂØπ‰∫éÊØè‰∏™ÂÉèÁ¥†ÁÇπÔºåÂ∞ÜÈÄâÊã©ÊúÄËøëÁöÑÁ±ªÁ∞áËøõË°åÈ¢úËâ≤Ë°®Á§∫„ÄÇÈÄöËøáÂéãÁº©Â∞Ü128x128x24=393,216bitsÁöÑÂõæÂÉèÂéãÁº©‰∏∫16x24 + 128x128x4=65,920bitsÔºàÂÖ∂‰∏≠ÈúÄË¶ÅÈ¢ùÂ§ñÊØèÁßçÈ¢úËâ≤ÈúÄË¶Å‰∏Ä‰∏™24bitÁ©∫Èó¥Â≠òÂÇ®ÂêÑ‰∏™È¢úËâ≤Â≠óÂÖ∏Ôºâ„ÄÇ</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% ============= Part 4: K-Means Clustering on Pixels ===============</span></span><br><span class="line"><span class="comment">%  In this exercise, you will use K-Means to compress an image. To do this,</span></span><br><span class="line"><span class="comment">%  you will first run K-Means on the colors of the pixels in the image and</span></span><br><span class="line"><span class="comment">%  then you will map each pixel onto its closest centroid.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Load an image of a bird</span></span><br><span class="line">A = double(imread(<span class="string">'bird_small.png'</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">% If imread does not work for you, you can try instead</span></span><br><span class="line"><span class="comment">%   load ('bird_small.mat');</span></span><br><span class="line"></span><br><span class="line">A = A / <span class="number">255</span>; <span class="comment">% Divide by 255 so that all values are in the range 0 - 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Size of the image</span></span><br><span class="line">img_size = <span class="built_in">size</span>(A);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Reshape the image into an Nx3 matrix where N = number of pixels.</span></span><br><span class="line"><span class="comment">% Each row will contain the Red, Green and Blue pixel values</span></span><br><span class="line"><span class="comment">% This gives us our dataset matrix X that we will use K-Means on.</span></span><br><span class="line">X = <span class="built_in">reshape</span>(A, img_size(<span class="number">1</span>) * img_size(<span class="number">2</span>), <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Run your K-Means algorithm on this data</span></span><br><span class="line"><span class="comment">% You should try different values of K and max_iters here</span></span><br><span class="line">K = <span class="number">16</span>; </span><br><span class="line">max_iters = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% When using K-Means, it is important the initialize the centroids</span></span><br><span class="line"><span class="comment">% randomly. </span></span><br><span class="line"><span class="comment">% You should complete the code in kMeansInitCentroids.m before proceeding</span></span><br><span class="line">initial_centroids = kMeansInitCentroids(X, K);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Run K-Means</span></span><br><span class="line">[centroids, idx] = runkMeans(X, initial_centroids, max_iters);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ================= Part 5: Image Compression ======================</span></span><br><span class="line"><span class="comment">%  In this part of the exercise, you will use the clusters of K-Means to</span></span><br><span class="line"><span class="comment">%  compress an image. To do this, we first find the closest clusters for</span></span><br><span class="line"><span class="comment">%  each example. After that, we </span></span><br><span class="line"><span class="comment">% Find closest cluster members</span></span><br><span class="line">idx = findClosestCentroids(X, centroids);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Essentially, now we have represented the image X as in terms of the</span></span><br><span class="line"><span class="comment">% indices in idx. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">% We can now recover the image from the indices (idx) by mapping each pixel</span></span><br><span class="line"><span class="comment">% (specified by its index in idx) to the centroid value</span></span><br><span class="line">X_recovered = centroids(idx,:);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Reshape the recovered image into proper dimensions</span></span><br><span class="line">X_recovered = <span class="built_in">reshape</span>(X_recovered, img_size(<span class="number">1</span>), img_size(<span class="number">2</span>), <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Display the original image </span></span><br><span class="line">subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>);</span><br><span class="line">imagesc(A); </span><br><span class="line">title(<span class="string">'Original'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Display compressed image side by side</span></span><br><span class="line">subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>);</span><br><span class="line">imagesc(X_recovered)</span><br><span class="line">title(sprintf(<span class="string">'Compressed, with %d colors.'</span>, K));</span><br></pre></td></tr></table></figure>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab7_02.png" alt="lab7_02"></p>
<h2 id="Lab-7-2Ôºö-PCA"><a href="#Lab-7-2Ôºö-PCA" class="headerlink" title="Lab 7.2Ôºö PCA"></a>Lab 7.2Ôºö PCA</h2><p>Êú¨ÂÆûÈ™åÂà©Áî®PCAÂÆûÁé∞Êï∞ÊçÆÈôçÁª¥ÂèäÂèØËßÜÂåñ„ÄÇ</p>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab7_03.png" alt="lab7_03"></p>
<ol>
<li>Êï∞ÊçÆÂΩí‰∏ÄÂåñ</li>
<li>ËÆ°ÁÆóÂçèÊñπÂ∑ÆÂíåÂ•áÂºÇÂÄºÂàÜËß£</li>
</ol>
<script type="math/tex; mode=display">
\Sigma =\frac{1}{m}X^TX</script><script type="math/tex; mode=display">
[U, S, V] = svd(\Sigma)</script><p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab7_04.png" alt="lab7_04"></p>
<ol>
<li>ÈôçÁª¥</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = X * U(:,<span class="number">1</span>:K);</span><br></pre></td></tr></table></figure>
<ol>
<li>Êï∞ÊçÆÊÅ¢Â§ç</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_rec = Z * U(:,<span class="number">1</span>:K)';</span><br></pre></td></tr></table></figure>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab7_05.png" alt="lab7_05"></p>
<h3 id="Êï∞ÊçÆÂèØËßÜÂåñÁöÑÈôçÁª¥ÊòæÁ§∫"><a href="#Êï∞ÊçÆÂèØËßÜÂåñÁöÑÈôçÁª¥ÊòæÁ§∫" class="headerlink" title="Êï∞ÊçÆÂèØËßÜÂåñÁöÑÈôçÁª¥ÊòæÁ§∫"></a>Êï∞ÊçÆÂèØËßÜÂåñÁöÑÈôçÁª¥ÊòæÁ§∫</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Use PCA to project this cloud to 2D for visualization</span></span><br><span class="line"><span class="comment">% X (16383,3)</span></span><br><span class="line"><span class="comment">% Subtract the mean to use PCA</span></span><br><span class="line">[X_norm, mu, sigma] = featureNormalize(X);</span><br><span class="line"></span><br><span class="line"><span class="comment">% PCA and project the data to 2D</span></span><br><span class="line">[U, S] = pca(X_norm);</span><br><span class="line">Z = projectData(X_norm, U, <span class="number">2</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab7_10.png" alt="lab7_10"></p>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab7_09.png" alt="lab7_09"></p>
<h2 id="Lab-8-1-ÂºÇÂ∏∏Ê£ÄÊµã"><a href="#Lab-8-1-ÂºÇÂ∏∏Ê£ÄÊµã" class="headerlink" title="Lab 8.1: ÂºÇÂ∏∏Ê£ÄÊµã"></a>Lab 8.1: ÂºÇÂ∏∏Ê£ÄÊµã</h2><p>Êú¨ÂÆûÈ™åÊê≠Âª∫‰∏Ä‰∏™ÂºÇÂ∏∏Ê£ÄÊµãÁ≥ªÁªüÔºåÁî®Êù•Ê£ÄÊµãÊúçÂä°Âô®ÁöÑÂºÇÂ∏∏‰ø°ÊÅØÔºåËæìÂÖ•‰∏∫ÊØèÂè∞ÊúçÂä°Âô®ÁöÑÊØèÂàÜÈíüÂêûÂêê(mb/s)ÂíåÂìçÂ∫îÂª∂Êó∂ÔºàmsÔºâÔºåÊï∞ÊçÆÊèê‰æõ‰∫Üm=307ÁªÑÊ†∑Êú¨Êï∞ÊçÆ„ÄÇÊúüÊúõÈÄöËøáÈùûÁõëÁù£Â≠¶‰π†ÁöÑÊâãÊÆµÊù•Ê£ÄÊµãÂºÇÂ∏∏„ÄÇ</p>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab8_01.png" alt="lab8_01"></p>
<p>‰∏∫‰∫ÜÂØπÊï∞ÊçÆÂºÇÂ∏∏ËøõË°åÊ£ÄÊµãÔºåÈ¶ñÂÖàÈúÄË¶ÅÂ∞ÜÂéüÂßãÊï∞ÊçÆÊãüÂêàÂà∞‰∏Ä‰∏™ÂàÜÂ∏ÉÊ®°ÂûãÈáåÔºåÊàë‰ª¨ÈÄâÊã©‰ΩøÁî®Â§öÂÖÉÈ´òÊñØÊ®°ÂûãËøõË°åÊï∞ÊçÆÊãüÂêà:</p>
<script type="math/tex; mode=display">
p(x;\mu,\Sigma) = \dfrac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(-1/2(x-\mu)^T\Sigma^{-1}(x-\mu))</script><ol>
<li>È¶ñÂÖàÈúÄË¶ÅÂØπÂèÇÊï∞$\mu$ ,$\sigma$ ËøõË°å‰º∞ËÆ°ÔºåÂèÇËßÅÂ¶Ç‰∏ã‰ª£Á†ÅÔºö</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mu = mean(X, <span class="number">1</span>);</span><br><span class="line">sigma2 = var(X, <span class="number">1</span>);</span><br><span class="line">p = multivariateGaussian(X, mu, sigma2);</span><br><span class="line"><span class="comment">%  Visualize the fit</span></span><br><span class="line">visualizeFit(X,  mu, sigma2);</span><br></pre></td></tr></table></figure>
<p>ÊãüÂêàÈ´òÊñØÂàÜÂ∏ÉÁöÑËΩÆÂªìÂõæÂ¶Ç‰∏ãÔºö</p>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab8_02.png" alt="lab8_02"></p>
<p>Â§öÂÖÉÈ´òÊñØÂàÜÂ∏ÉÂáΩÊï∞<code>multivariateGaussian()</code>ÁöÑÂÆö‰πâÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">multivariateGaussian</span><span class="params">(X, mu, Sigma2)</span></span></span><br><span class="line"><span class="comment">%MULTIVARIATEGAUSSIAN Computes the probability density function of the</span></span><br><span class="line"><span class="comment">%multivariate gaussian distribution.</span></span><br><span class="line"><span class="comment">%    p = MULTIVARIATEGAUSSIAN(X, mu, Sigma2) Computes the probability </span></span><br><span class="line"><span class="comment">%    density function of the examples X under the multivariate gaussian </span></span><br><span class="line"><span class="comment">%    distribution with parameters mu and Sigma2. If Sigma2 is a matrix, it is</span></span><br><span class="line"><span class="comment">%    treated as the covariance matrix. If Sigma2 is a vector, it is treated</span></span><br><span class="line"><span class="comment">%    as the \sigma^2 values of the variances in each dimension (a diagonal</span></span><br><span class="line"><span class="comment">%    covariance matrix)</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line">k = <span class="built_in">length</span>(mu);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">size</span>(Sigma2, <span class="number">2</span>) == <span class="number">1</span>) || (<span class="built_in">size</span>(Sigma2, <span class="number">1</span>) == <span class="number">1</span>)</span><br><span class="line">    Sigma2 = <span class="built_in">diag</span>(Sigma2);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">X = <span class="built_in">bsxfun</span>(@minus, X, mu(:)');</span><br><span class="line">p = (<span class="number">2</span> * <span class="built_in">pi</span>) ^ (- k / <span class="number">2</span>) * det(Sigma2) ^ (<span class="number">-0.5</span>) * ...</span><br><span class="line">    <span class="built_in">exp</span>(<span class="number">-0.5</span> * sum(<span class="built_in">bsxfun</span>(@times, X * pinv(Sigma2), X), <span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol>
<li>ÈòàÂÄº$\epsilon$ ÁöÑÈÄâÊã©ÔºåÈÄöËøáËÆ°ÁÆóÊØè‰∏™ÈòàÂÄºÁöÑF1ÂæóÂàÜÊù•ËØÑ‰ª∑È™åËØÅÈõÜÊï∞ÊçÆÔºåÈÄâÊã©ÊúÄ‰ºòÂæóÂàÜÁöÑÈòàÂÄºÔºö</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[bestEpsilon bestF1]</span> = <span class="title">selectThreshold</span><span class="params">(yval, pval)</span></span></span><br><span class="line"><span class="comment">%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting</span></span><br><span class="line"><span class="comment">%outliers</span></span><br><span class="line"><span class="comment">%   [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best</span></span><br><span class="line"><span class="comment">%   threshold to use for selecting outliers based on the results from a</span></span><br><span class="line"><span class="comment">%   validation set (pval) and the ground truth (yval).</span></span><br><span class="line"></span><br><span class="line">bestEpsilon = <span class="number">0</span>;</span><br><span class="line">bestF1 = <span class="number">0</span>;</span><br><span class="line">F1 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">stepsize = (max(pval) - min(pval)) / <span class="number">1000</span>;</span><br><span class="line"><span class="keyword">for</span> epsilon = min(pval):stepsize:max(pval)</span><br><span class="line">    <span class="comment">% Instructions: Compute the F1 score of choosing epsilon as the</span></span><br><span class="line">    <span class="comment">%               threshold and place the value in F1. The code at the</span></span><br><span class="line">    <span class="comment">%               end of the loop will compare the F1 score for this</span></span><br><span class="line">    <span class="comment">%               choice of epsilon and set it to be the best epsilon if</span></span><br><span class="line">    <span class="comment">%               it is better than the current choice of epsilon.</span></span><br><span class="line">    <span class="comment">%               </span></span><br><span class="line">    <span class="comment">% Note: You can use predictions = (pval &lt; epsilon) to get a binary vector</span></span><br><span class="line">    <span class="comment">%       of 0's and 1's of the outlier predictions</span></span><br><span class="line">    cvPredictions = (pval &lt; epsilon);</span><br><span class="line">    fp = sum((cvPredictions == <span class="number">1</span>)&amp; (yval == <span class="number">0</span>));</span><br><span class="line">    tp = sum((cvPredictions == <span class="number">1</span>)&amp;(yval == <span class="number">1</span>));</span><br><span class="line">    fn = sum((cvPredictions == <span class="number">0</span>)&amp;(yval == <span class="number">1</span>));</span><br><span class="line">    </span><br><span class="line">    prec = tp/(tp + fp);</span><br><span class="line">    rec = tp/(tp + fn);</span><br><span class="line"></span><br><span class="line">    F1 = <span class="number">2</span>*prec*rec/(prec + rec);</span><br><span class="line">    <span class="keyword">if</span> F1 &gt; bestF1</span><br><span class="line">       bestF1 = F1;</span><br><span class="line">       bestEpsilon = epsilon;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">pval = multivariateGaussian(Xval, mu, sigma2);</span><br><span class="line">[epsilon F1] = selectThreshold(yval, pval);</span><br></pre></td></tr></table></figure>
<p><img src="/2018/04/26/machinelearning-labs/../../qnsource/images/2018-04-26-machinelearning-labs/lab8_03.png" alt="lab8_03"></p>
<ol>
<li>Âú®Êõ¥Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®ÔºåÁ§∫‰æãÊèê‰æõ‰∫Ü‰∏™ËæìÂÖ•‰∏∫11Áª¥ÁâπÂæÅÁöÑÊï∞ÊçÆÔºåÂØπÂÖ∂ÂºÇÂ∏∏Êï∞ÊçÆËøõË°åÈ¢ÑÊµãÔºå‰ª£Á†ÅÁâáÊÆµÂ¶Ç‰∏ãÔºö</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%  Apply the same steps to the larger dataset</span></span><br><span class="line">[mu sigma2] = estimateGaussian(X);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Training set </span></span><br><span class="line">p = multivariateGaussian(X, mu, sigma2);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Cross-validation set</span></span><br><span class="line">pval = multivariateGaussian(Xval, mu, sigma2);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Find the best threshold</span></span><br><span class="line">[epsilon F1] = selectThreshold(yval, pval);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">'Best epsilon found using cross-validation: %e\n'</span>, epsilon);</span><br><span class="line">fprintf(<span class="string">'Best F1 on Cross Validation Set:  %f\n'</span>, F1);</span><br><span class="line">fprintf(<span class="string">'# Outliers found: %d\n\n'</span>, sum(p &lt; epsilon));</span><br></pre></td></tr></table></figure>
<p>‚Äã</p>
<h2 id="Lab-8-2ÔºöÊé®ËçêÁ≥ªÁªü"><a href="#Lab-8-2ÔºöÊé®ËçêÁ≥ªÁªü" class="headerlink" title="Lab 8.2ÔºöÊé®ËçêÁ≥ªÁªü"></a>Lab 8.2ÔºöÊé®ËçêÁ≥ªÁªü</h2><p>Êú¨ÂÆûÈ™åÂ∞ÜÂÆûÁé∞ÂçèÂêåËøáÊª§ÁÆóÊ≥ïÔºåÂπ∂Â∫îÁî®Âà∞ÁîµÂΩ±Êé®Ëçê‰πã‰∏≠„ÄÇÊï∞ÊçÆÈõÜÊù•Ëá™943‰∏™Áî®Êà∑ÁöÑ1682ÈÉ®ÁîµÂΩ±ÁöÑËØÑÂàÜÊï∞ÊçÆÔºåÊØè‰∏™ËØÑÂàÜËåÉÂõ¥‰∏∫1-5ÔºåÂèØ‰ª•Â≠òÂú®Â§ö‰∏™Êú™ËØÑÂàÜÊï∞ÊçÆ„ÄÇ</p>
<ul>
<li>$X$ ‰∏∫num_movies x num_featuresÔºåÊòØÁîµÂΩ±ÁöÑÁâπÂæÅÁü©Èòµ</li>
<li>$Y$ ‰∏∫num_movies x num_userÁü©ÈòµÔºåÊèèËø∞‰∫ÜÊØè‰∏™ËØÑÂàÜÂÄº$y^{(i,j)}$</li>
<li>$R$ ‰∏∫‰∏é$Y$ ÂêåÁª¥Â∫¶ÁöÑ‰∫åÂÄºÁü©ÈòµÔºå$R(i,j)=1$ ‰ª£Ë°®Áî®Êà∑$j$ ÂØπÁîµÂΩ±$i$ ËøõË°å‰∫ÜËØÑÂàÜ</li>
</ul>
<ol>
<li>‰ª£‰ª∑ÂáΩÊï∞ÁöÑÂÆö‰πâ</li>
</ol>
<script type="math/tex; mode=display">
J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">cofiCostFunc</span><span class="params">(params, Y, R, num_users, num_movies, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                  num_features, lambda)</span></span></span><br><span class="line"><span class="comment">%COFICOSTFUNC Collaborative filtering cost function</span></span><br><span class="line"><span class="comment">%   [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...</span></span><br><span class="line"><span class="comment">%   num_features, lambda) returns the cost and gradient for the</span></span><br><span class="line"><span class="comment">%   collaborative filtering problem.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Unfold the U and W matrices from params</span></span><br><span class="line">X = <span class="built_in">reshape</span>(params(<span class="number">1</span>:num_movies*num_features), num_movies, num_features);</span><br><span class="line">Theta = <span class="built_in">reshape</span>(params(num_movies*num_features+<span class="number">1</span>:<span class="keyword">end</span>), ...</span><br><span class="line">                num_users, num_features);</span><br><span class="line">            </span><br><span class="line"><span class="comment">% You need to return the following values correctly</span></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">X_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X));</span><br><span class="line">Theta_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Instructions: Compute the cost function and gradient for collaborative</span></span><br><span class="line"><span class="comment">%               filtering. </span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Notes: X - num_movies  x num_features matrix of movie features</span></span><br><span class="line"><span class="comment">%        Theta - num_users  x num_features matrix of user features</span></span><br><span class="line"><span class="comment">%        Y - num_movies x num_users matrix of user ratings of movies</span></span><br><span class="line"><span class="comment">%        R - num_movies x num_users matrix, where R(i, j) = 1 if the </span></span><br><span class="line"><span class="comment">%            i-th movie was rated by the j-th user</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% You should set the following variables correctly:</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%        X_grad - num_movies x num_features matrix, containing the </span></span><br><span class="line"><span class="comment">%                 partial derivatives w.r.t. to each element of X</span></span><br><span class="line"><span class="comment">%        Theta_grad - num_users x num_features matrix, containing the </span></span><br><span class="line"><span class="comment">%                     partial derivatives w.r.t. to each element of Theta</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line">J = <span class="number">1</span>/<span class="number">2</span> * sum(sum(((X * Theta' - Y).*R).^<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">X_grad = ((X * Theta' - Y).*R) * Theta;</span><br><span class="line">Theta_grad = ((X*Theta' -Y).*R)' * X;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Regular version</span></span><br><span class="line">J = J + lambda/<span class="number">2</span> * sum(sum(Theta .^<span class="number">2</span>)) + lambda/<span class="number">2</span> * sum(sum(X.^<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">X_grad = X_grad + lambda * X;</span><br><span class="line">Theta_grad = Theta_grad + lambda * Theta;</span><br><span class="line"></span><br><span class="line">grad = [X_grad(:); Theta_grad(:)];</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Evaluate cost function</span></span><br><span class="line">J = cofiCostFunc([X(:) ; Theta(:)], Y, R, num_users, num_movies, num_features, <span class="number">1.5</span>);</span><br></pre></td></tr></table></figure>
<ol>
<li>ÂèëËµ∑È¢ÑÊµãÔºåÂú®Êï∞ÊçÆÂ∫ì‰∏≠Âä†ÂÖ•Ëá™Â∑±ÁöÑÊï∞ÊçÆÔºåÈöè‰æøÂØπ‰∏ÄÈÉ®ÂàÜÁîµÂΩ±ËøõË°åÊâìÂàÜÔºö</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% ============== Part 6: Entering ratings for a new user ===============</span></span><br><span class="line"><span class="comment">%  Before we will train the collaborative filtering model, we will first</span></span><br><span class="line"><span class="comment">%  add ratings that correspond to a new user that we just observed. This</span></span><br><span class="line"><span class="comment">%  part of the code will also allow you to put in your own ratings for the</span></span><br><span class="line"><span class="comment">%  movies in our dataset!</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line">movieList = loadMovieList();</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Initialize my ratings</span></span><br><span class="line">my_ratings = <span class="built_in">zeros</span>(<span class="number">1682</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Check the file movie_idx.txt for id of each movie in our dataset</span></span><br><span class="line"><span class="comment">% For example, Toy Story (1995) has ID 1, so to rate it "4", you can set</span></span><br><span class="line">my_ratings(<span class="number">1</span>) = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Or suppose did not enjoy Silence of the Lambs (1991), you can set</span></span><br><span class="line">my_ratings(<span class="number">98</span>) = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% We have selected a few movies we liked / did not like and the ratings we</span></span><br><span class="line"><span class="comment">% gave are as follows:</span></span><br><span class="line">my_ratings(<span class="number">7</span>) = <span class="number">3</span>;</span><br><span class="line">my_ratings(<span class="number">12</span>)= <span class="number">5</span>;</span><br><span class="line">my_ratings(<span class="number">54</span>) = <span class="number">4</span>;</span><br><span class="line">my_ratings(<span class="number">64</span>)= <span class="number">5</span>;</span><br><span class="line">my_ratings(<span class="number">66</span>)= <span class="number">3</span>;</span><br><span class="line">my_ratings(<span class="number">69</span>) = <span class="number">5</span>;</span><br><span class="line">my_ratings(<span class="number">183</span>) = <span class="number">4</span>;</span><br><span class="line">my_ratings(<span class="number">226</span>) = <span class="number">5</span>;</span><br><span class="line">my_ratings(<span class="number">355</span>)= <span class="number">5</span>;</span><br></pre></td></tr></table></figure>
<p>Êàë‰ª¨Êã•Êúâ‰∫Ü‰∏Ä‰ªΩÊñ∞ÁöÑÁî®Êà∑ÂØπÁîµÂΩ±ÊâìÂàÜÁöÑÊï∞ÊçÆÔºö</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">New user ratings:</span><br><span class="line">Rated 4 <span class="keyword">for</span> Toy Story (1995)</span><br><span class="line">Rated 3 <span class="keyword">for</span> Twelve Monkeys (1995)</span><br><span class="line">Rated 5 <span class="keyword">for</span> Usual Suspects, The (1995)</span><br><span class="line">Rated 4 <span class="keyword">for</span> Outbreak (1995)</span><br><span class="line">Rated 5 <span class="keyword">for</span> Shawshank Redemption, The (1994)</span><br><span class="line">Rated 3 <span class="keyword">for</span> While You Were Sleeping (1995)</span><br><span class="line">Rated 5 <span class="keyword">for</span> Forrest Gump (1994)</span><br><span class="line">Rated 2 <span class="keyword">for</span> Silence of the Lambs, The (1991)</span><br><span class="line">Rated 4 <span class="keyword">for</span> Alien (1979)</span><br><span class="line">Rated 5 <span class="keyword">for</span> Die Hard 2 (1990)</span><br><span class="line">Rated 5 <span class="keyword">for</span> Sphere (1998)</span><br></pre></td></tr></table></figure>
<ol>
<li>ËÆ≠ÁªÉÁÆóÊ≥ï</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% ================== Part 7: Learning Movie Ratings ====================</span></span><br><span class="line"><span class="comment">%  Now, you will train the collaborative filtering model on a movie rating </span></span><br><span class="line"><span class="comment">%  dataset of 1682 movies and 943 users</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Load data</span></span><br><span class="line">load(<span class="string">'ex8_movies.mat'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Add our own ratings to the data matrix</span></span><br><span class="line">Y = [my_ratings Y];</span><br><span class="line">R = [(my_ratings ~= <span class="number">0</span>) R];</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Normalize Ratings</span></span><br><span class="line">[Ynorm, Ymean] = normalizeRatings(Y, R);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Useful Values</span></span><br><span class="line">num_users = <span class="built_in">size</span>(Y, <span class="number">2</span>);</span><br><span class="line">num_movies = <span class="built_in">size</span>(Y, <span class="number">1</span>);</span><br><span class="line">num_features = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Set Initial Parameters (Theta, X)</span></span><br><span class="line">X = <span class="built_in">randn</span>(num_movies, num_features);</span><br><span class="line">Theta = <span class="built_in">randn</span>(num_users, num_features);</span><br><span class="line"></span><br><span class="line">initial_parameters = [X(:); Theta(:)];</span><br><span class="line"></span><br><span class="line"><span class="comment">% Set options for fmincg</span></span><br><span class="line">options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Set Regularization</span></span><br><span class="line">lambda = <span class="number">10</span>;</span><br><span class="line">theta = fmincg (@(t)(cofiCostFunc(t, Ynorm, R, num_users, num_movies, ...</span><br><span class="line">                                num_features, lambda)), ...</span><br><span class="line">                initial_parameters, options);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Unfold the returned theta back into U and W</span></span><br><span class="line">X = <span class="built_in">reshape</span>(theta(<span class="number">1</span>:num_movies*num_features), num_movies, num_features);</span><br><span class="line">Theta = <span class="built_in">reshape</span>(theta(num_movies*num_features+<span class="number">1</span>:<span class="keyword">end</span>), ...</span><br><span class="line">                num_users, num_features);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">'Recommender system learning completed.\n'</span>);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">'\nProgram paused. Press enter to continue.\n'</span>);</span><br><span class="line">pause;</span><br></pre></td></tr></table></figure>
<p>ÂÖ∂‰∏≠ÔºåÂáΩÊï∞<code>normalizeRatings</code> Áî®‰∫éÂØπÊï∞ÊçÆËøõË°åÂΩí‰∏ÄÂåñÔºåÂáèÂéªÂùáÂÄºÔºõÂáΩÊï∞<code>fmincg</code>‰∏∫MATLABËá™Â∏¶ÁöÑ‰ºòÂåñÂáΩÊï∞Ôºõ</p>
<ol>
<li>Âà©Áî®ÁÆóÊ≥ïËøõË°åÊé®Ëçê</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% ================== Part 8: Recommendation for you ====================</span></span><br><span class="line"><span class="comment">%  After training the model, you can now make recommendations by computing</span></span><br><span class="line"><span class="comment">%  the predictions matrix.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line">p = X * Theta';</span><br><span class="line">my_predictions = p(:,<span class="number">1</span>) + Ymean;</span><br><span class="line"></span><br><span class="line">movieList = loadMovieList();</span><br><span class="line"></span><br><span class="line">[r, ix] = sort(my_predictions, <span class="string">'descend'</span>);</span><br><span class="line">fprintf(<span class="string">'\nTop recommendations for you:\n'</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">10</span></span><br><span class="line">    <span class="built_in">j</span> = ix(<span class="built_in">i</span>);</span><br><span class="line">    fprintf(<span class="string">'Predicting rating %.1f for movie %s\n'</span>, my_predictions(<span class="built_in">j</span>), ...</span><br><span class="line">            movieList&#123;j&#125;);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>ËæìÂá∫ÁªìÊûúÔºö</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Top recommendations <span class="keyword">for</span> you:</span><br><span class="line">Predicting rating 5.0 <span class="keyword">for</span> movie Someone Else<span class="string">'s America (1995)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie Aiqing wansui (1994)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie Great Day in Harlem, A (1994)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie Prefontaine (1997)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie They Made Me a Criminal (1939)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie Entertaining Angels: The Dorothy Day Story (1996)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie Saint of Fort Washington, The (1993)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie Santa with Muscles (1996)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie Star Kid (1997)</span></span><br><span class="line"><span class="string">Predicting rating 5.0 for movie Marlene Dietrich: Shadow and Light (1996)</span></span><br></pre></td></tr></table></figure>
<h2 id="ÂèÇËÄÉ"><a href="#ÂèÇËÄÉ" class="headerlink" title="ÂèÇËÄÉ"></a>ÂèÇËÄÉ</h2><ol>
<li>‚Äã</li>
</ol>

      
    </div>
    <footer class="article-footer">

      
        <div id="donation_div"></div>

<script src="/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: 'Â¶ÇÊûúËßâÂæóÊàëÁöÑÊñáÁ´†ÂØπÊÇ®ÊúâÁî®ÔºåËØ∑ÈöèÊÑèÊâìËµè„ÄÇÊÇ®ÁöÑÊîØÊåÅÂ∞ÜÈºìÂä±ÊàëÁªßÁª≠Âàõ‰Ωú!', // ÂèØÈÄâÂèÇÊï∞ÔºåÊâìËµèÊ†áÈ¢ò
  btnText: 'ÊâìËµèÊîØÊåÅ', // ÂèØÈÄâÂèÇÊï∞ÔºåÊâìËµèÊåâÈíÆÊñáÂ≠ó
  el: document.getElementById('donation_div'),
  wechatImage: '/qnsource/site/weixin.png',
  alipayImage: '/qnsource/site/alipay.jpg'
});
</script>
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Êú¨Êñá‰ΩúËÄÖ:  </strong>Ebby DD</a>
          </li>
          <li class="post-copyright-link">
          <strong>Êú¨ÊñáÈìæÊé•:  </strong>
          <a href="/2018/04/26/machinelearning-labs/" target="_blank" title="„ÄäÊú∫Âô®Â≠¶‰π†„ÄãËØæÁ®ãÂÆûÈ™åÂõûÈ°æ">http://blog.a-stack.com/2018/04/26/machinelearning-labs/</a>
          </li>
          <li class="post-copyright-license">
            <strong>ÁâàÊùÉÂ£∞Êòé:   </strong>
            Êú¨ÂçöÂÆ¢ÊâÄÊúâÊñáÁ´†Èô§ÁâπÂà´Â£∞ÊòéÂ§ñÔºåÂùáÈááÁî® <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            ËÆ∏ÂèØÂçèËÆÆ„ÄÇËΩ¨ËΩΩËØ∑Ê≥®ÊòéÂá∫Â§Ñ
          </li>
         
        </ul>
<div>

      

      
      <!--  -->

      <!--  -->
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ÂÖ¨ÂºÄËØæ/">ÂÖ¨ÂºÄËØæ</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ÂÆûÈ™å/">ÂÆûÈ™å</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Êú∫Âô®Â≠¶‰π†/">Êú∫Âô®Â≠¶‰π†</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Á∫øÊÄßÂõûÂΩí/">Á∫øÊÄßÂõûÂΩí</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ÈÄªËæëÂõûÂΩí/">ÈÄªËæëÂõûÂΩí</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/04/29/Everything-About-SVM/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">‰∏ä‰∏ÄÁØá</strong>
      <div class="article-nav-title">
        
          Everything-About-SVM
        
      </div>
    </a>
  
  
    <a href="/2018/04/26/machinelearning-notes/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">‰∏ã‰∏ÄÁØá</strong>
      <div class="article-nav-title">Machine Learning Notes</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">ÊñáÁ´†ÁõÆÂΩï</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ê¶ÇË¶Å"><span class="nav-number">1.</span> <span class="nav-text">Ê¶ÇË¶Å</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lab1-Á∫øÊÄßÂõûÂΩí"><span class="nav-number">2.</span> <span class="nav-text">Lab1: Á∫øÊÄßÂõûÂΩí</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí"><span class="nav-number">2.1.</span> <span class="nav-text">1. ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#‰ª£‰ª∑ÂáΩÊï∞ÂíåÊ¢ØÂ∫¶Êõ¥Êñ∞ÂÆûÁé∞"><span class="nav-number">2.1.1.</span> <span class="nav-text">‰ª£‰ª∑ÂáΩÊï∞ÂíåÊ¢ØÂ∫¶Êõ¥Êñ∞ÂÆûÁé∞</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#J-theta-ÁöÑÂèØËßÜÂåñ"><span class="nav-number">2.1.2.</span> <span class="nav-text">$J(\theta)$ ÁöÑÂèØËßÜÂåñ</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Â§öÂÖÉÁ∫øÊÄßÂõûÂΩí"><span class="nav-number">2.2.</span> <span class="nav-text">2. Â§öÂÖÉÁ∫øÊÄßÂõûÂΩí</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-ÁâπÂæÅÂΩí‰∏ÄÂåñ"><span class="nav-number">2.3.</span> <span class="nav-text">2.1 ÁâπÂæÅÂΩí‰∏ÄÂåñ</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-ËÆ°ÁÆó-theta-ÁöÑÊï∞ÂÄºËß£"><span class="nav-number">2.4.</span> <span class="nav-text">2.2 ËÆ°ÁÆó $\theta$ ÁöÑÊï∞ÂÄºËß£</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lab-2-Lab-5"><span class="nav-number">3.</span> <span class="nav-text">Lab 2 - Lab 5:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lab-6-SVM"><span class="nav-number">4.</span> <span class="nav-text">Lab 6: SVM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lab-7-1-K-Means"><span class="nav-number">5.</span> <span class="nav-text">Lab 7.1: K-Means</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lab-7-2Ôºö-PCA"><span class="nav-number">6.</span> <span class="nav-text">Lab 7.2Ôºö PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Êï∞ÊçÆÂèØËßÜÂåñÁöÑÈôçÁª¥ÊòæÁ§∫"><span class="nav-number">6.1.</span> <span class="nav-text">Êï∞ÊçÆÂèØËßÜÂåñÁöÑÈôçÁª¥ÊòæÁ§∫</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lab-8-1-ÂºÇÂ∏∏Ê£ÄÊµã"><span class="nav-number">7.</span> <span class="nav-text">Lab 8.1: ÂºÇÂ∏∏Ê£ÄÊµã</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lab-8-2ÔºöÊé®ËçêÁ≥ªÁªü"><span class="nav-number">8.</span> <span class="nav-text">Lab 8.2ÔºöÊé®ËçêÁ≥ªÁªü</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ÂèÇËÄÉ"><span class="nav-number">9.</span> <span class="nav-text">ÂèÇËÄÉ</span></a></li></ol>
    
    </div>
  </aside>

</section>      
        
      </div>
      <!--  -->
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2017 - 2020 Ebby&#39;s Notes All Rights Reserved.</p>
	      
	      
  		   	<p id="copyRightCn">Ebby DD ‰øùÁïôÊâÄÊúâÊùÉÂà©</p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>    
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/reading" class="mobile-nav-link">Reading</a>
  
    <a href="/resources" class="mobile-nav-link">Resources</a>
  
    <a href="/notebooks" class="mobile-nav-link">üìù</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/scripts.js"></script>




  <script src="/js/dialog.js"></script>











	<script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?684368b6943d7a5b6689dba5e7bf30ad";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">ËÆæÁΩÆ</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              Ê≠£ÊñáÂ≠óÂè∑Â§ßÂ∞è
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            ÊÇ®Â∑≤Ë∞ÉÊï¥È°µÈù¢Â≠ó‰ΩìÂ§ßÂ∞è
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              Â§úÈó¥Êä§ÁúºÊ®°Âºè
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            Â§úÈó¥Ê®°ÂºèÂ∑≤ÁªèÂºÄÂêØÔºåÂÜçÊ¨°ÂçïÂáªÊåâÈíÆÂç≥ÂèØÂÖ≥Èó≠ 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ÂÖ≥ ‰∫é&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Ebby&#39;s Notes
          </div>
          <div class="panel-body">
            Copyright ¬© 2020 Ebby DD All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">√ó</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>