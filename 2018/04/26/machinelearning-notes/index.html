<!DOCTYPE html>
<html  lang="zh">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Machine Learning Notes - Ebby&#39;s Notes</title>


    <meta name="description" content="摘要： 最近花了两周时间刷完了吴恩达在Cousera上关于机器学习的经典课程, 这已经是近两个月来刷完的第十个Cousera课程了。虽然课程是多年前开设的，但相关机器学习理论和方法内容介绍仍然具备很强的时效性，是机器学习入门的必选课程。">
<meta name="keywords" content="人工智能,深度学习,技术,算法">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Notes">
<meta property="og:url" content="http://blog.a-stack.com/2018/04/26/machinelearning-notes/index.html">
<meta property="og:site_name" content="Ebby&#39;s Notes">
<meta property="og:description" content="摘要： 最近花了两周时间刷完了吴恩达在Cousera上关于机器学习的经典课程, 这已经是近两个月来刷完的第十个Cousera课程了。虽然课程是多年前开设的，但相关机器学习理论和方法内容介绍仍然具备很强的时效性，是机器学习入门的必选课程。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.a-stack.com/images/og_image.png">
<meta property="og:updated_time" content="2020-03-04T12:18:52.836Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Notes">
<meta name="twitter:description" content="摘要： 最近花了两周时间刷完了吴恩达在Cousera上关于机器学习的经典课程, 这已经是近两个月来刷完的第十个Cousera课程了。虽然课程是多年前开设的，但相关机器学习理论和方法内容介绍仍然具备很强的时效性，是机器学习入门的必选课程。">
<meta name="twitter:image" content="http://blog.a-stack.com/images/og_image.png">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/tomorrow-night-eighties.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?684368b6943d7a5b6689dba5e7bf30ad";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo02.png" alt="Machine Learning Notes" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/reading">读书</a>
                
                <a class="navbar-item"
                href="/resources">资源</a>
                
                <a class="navbar-item"
                href="/notebooks">📝</a>
                
                <a class="navbar-item"
                href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                

                <time class="level-item has-text-grey" datetime="2018-04-26T05:56:13.000Z">2018-04-26</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/机器学习/">机器学习</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    19 分钟 读完 (大约 2873 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-angle-double-right"></i>Machine Learning Notes
            
        </h1>
        <div class="content">
            <p><strong>摘要：</strong> 最近花了两周时间刷完了吴恩达在Cousera上关于<a href="https://www.coursera.org/learn/machine-learning">机器学习</a>的经典课程, 这已经是近两个月来刷完的第十个Cousera课程了。虽然课程是多年前开设的，但相关机器学习理论和方法内容介绍仍然具备很强的时效性，是机器学习入门的必选课程。</p>
<a id="more"></a>
<p>@[toc]</p>
<blockquote>
<p>最近花了两周时间刷完了吴恩达在Cousera上关于<a href="https://www.coursera.org/learn/machine-learning">机器学习</a>的经典课程, 这已经是近两个月来刷完的第十个Cousera课程了。虽然课程是多年前开设的，但相关机器学习理论和方法内容介绍仍然具备很强的时效性，是机器学习入门的必选课程。为了巩固对课程内容和课后实验的理解，将在未来一周实践写几篇总结性笔记，作为对课程内容的回顾和总结。</p>
</blockquote>
<h2 id="Lecture-1：-机器学习的定义"><a href="#Lecture-1：-机器学习的定义" class="headerlink" title="Lecture 1： 机器学习的定义"></a>Lecture 1： 机器学习的定义</h2><blockquote>
<p>Tom Mitchell provides a modern definition: “A computer program is said to learn from experience <strong>E</strong> with respect to some class of tasks <strong>T</strong> and performance measure <strong>P</strong>, if its performance at tasks in <strong>T</strong>, as measured by <strong>P</strong>, improves with experience <strong>E</strong>.”</p>
</blockquote>
<ol>
<li>监督学习的一个主要特点，是在拿到数据的那一刻，我们已经知道正确的输出目标范围；<ul>
<li>监督学习可以分为分类问题和回归问题；</li>
</ul>
</li>
<li>鸡尾酒会问题及鸡尾酒会算法：<a href="https://en.wikipedia.org/wiki/Cocktail_party_effect">Wiki</a>, 属于非聚类的非监督问题</li>
</ol>
<h2 id="Lecture-2：-线性回归（多属性）"><a href="#Lecture-2：-线性回归（多属性）" class="headerlink" title="Lecture 2： 线性回归（多属性）"></a>Lecture 2： 线性回归（多属性）</h2><h3 id="变量定义"><a href="#变量定义" class="headerlink" title="变量定义"></a>变量定义</h3><p>$x_j^{(i)}$ : 第$i^{th}$训练样本的第$j$个特征向量；</p>
<p>$x^{(i)}$ : 第$i^{th}$训练样本的特征向量；</p>
<p>$m$ ： 训练样本数目；</p>
<p>$n$ ： 特征数目</p>
<h3 id="多元线性规划模型"><a href="#多元线性规划模型" class="headerlink" title="多元线性规划模型"></a>多元线性规划模型</h3><h4 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h4><script type="math/tex; mode=display">
\begin{equation} h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n = \theta ^T x  \tag{1.a} \end{equation}</script><blockquote>
<p>以上表达式中对$\theta$ 和 $x$ 进行了维度变换，$x_0=1,for(i\in1,…,m)$</p>
<p>$\theta .shape = (n+1, 1)$</p>
<p>$x.shape = (m,n+1)$</p>
</blockquote>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><script type="math/tex; mode=display">
J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (x^{(i)}) - y^{(i)} \right)^2 = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})</script><h4 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h4><script type="math/tex; mode=display">
\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})</script><h3 id="特征正则化"><a href="#特征正则化" class="headerlink" title="特征正则化"></a>特征正则化</h3><blockquote>
<ol>
<li>Feature scaling 和 mean normalization实现;</li>
<li>归一化的目的，是为了收敛域对其，更容易收敛，梯度下降更容易找到最佳方向；</li>
<li>吴恩达打了个很好的比喻，两个参数范围不一致，梯度下降是一个椭圆；</li>
</ol>
</blockquote>
<script type="math/tex; mode=display">
x_i := \dfrac{x_i - \mu_i}{s_i}</script><h3 id="参数分析"><a href="#参数分析" class="headerlink" title="参数分析"></a>参数分析</h3><script type="math/tex; mode=display">
\theta = (X^T X)^{-1}X^T y</script><div class="table-container">
<table>
<thead>
<tr>
<th>Gradient Descent</th>
<th>Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Need to choose alpha</td>
<td>No need to choose alpha</td>
</tr>
<tr>
<td>Needs many iterations</td>
<td>No need to iterate</td>
</tr>
<tr>
<td>$O(kn^2)$</td>
<td>$O(n^3)$ ,计算复杂度来自$X^TX$ 的计算</td>
</tr>
<tr>
<td>Works well when n is large</td>
<td>Slow if n is very large</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Lecture-3：逻辑回归"><a href="#Lecture-3：逻辑回归" class="headerlink" title="Lecture 3：逻辑回归"></a>Lecture 3：逻辑回归</h2><blockquote>
<p><strong>逻辑回归名称的由来</strong>：逻辑回归来源于Logistic 分布，虽然名字带回归，但解决的是分类问题。而其密度函数曲线正对应着Sigmoid函数的曲线形状。</p>
</blockquote>
<h3 id="假设函数-1"><a href="#假设函数-1" class="headerlink" title="假设函数"></a>假设函数</h3><script type="math/tex; mode=display">
\begin{align*} h_\theta (x) =  g ( \theta^T x ) \\ z = \theta^T x \\g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}</script><p>假设函数输出的分类目标的条件概率：</p>
<script type="math/tex; mode=display">
\begin{align*}& h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline& P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*}</script><h3 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h3><script type="math/tex; mode=display">
\begin{align*}& J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)})=-\frac{1}{m}(y^Tlog(h)+(1-y)^T(1-h)) \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; & \text{if y = 1} \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; & \text{if y = 0}\end{align*}</script><h3 id="梯度-1"><a href="#梯度-1" class="headerlink" title="梯度"></a>梯度</h3><script type="math/tex; mode=display">
\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})</script><script type="math/tex; mode=display">
\sigma(x)'=\sigma(x)(1 - \sigma(x))</script><blockquote>
<p>一些梯度下降算法之外的选择： 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：共轭梯度（Conjugate Gradient），局部优化法(Broyden fletcher goldfarb shann,BFGS)和有限内存局部优化法(LBFGS)<br>fminunc是 matlab和octave 中都带的一个最小值优化函数，使用时我们需要提供代价函数和每个参数的求导</p>
</blockquote>
<h3 id="多分类：-One-vs-all"><a href="#多分类：-One-vs-all" class="headerlink" title="多分类： One-vs-all"></a>多分类： One-vs-all</h3><p>每个分类目标对应一个分类器，计算分类过程中，取预测值最大的分类器输出作为最终分类结果：</p>
<script type="math/tex; mode=display">
prediction = \max_i( h_\theta ^{(i)}(x) )</script><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h3 id="正则化的线性回归"><a href="#正则化的线性回归" class="headerlink" title="正则化的线性回归"></a>正则化的线性回归</h3><script type="math/tex; mode=display">
\begin{align*}
& \text{Repeat}\ \lbrace \newline
& \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
& \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
& \rbrace
\end{align*}</script><h4 id="添加正则化的数值解"><a href="#添加正则化的数值解" class="headerlink" title="添加正则化的数值解"></a>添加正则化的数值解</h4><script type="math/tex; mode=display">
\begin{align*}& \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline& \text{where}\ \ L = \begin{bmatrix} 0 & & & & \newline & 1 & & & \newline & & 1 & & \newline & & & \ddots & \newline & & & & 1 \newline\end{bmatrix}\end{align*}</script><h3 id="正则化的逻辑回归"><a href="#正则化的逻辑回归" class="headerlink" title="正则化的逻辑回归"></a>正则化的逻辑回归</h3><script type="math/tex; mode=display">
J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2</script><script type="math/tex; mode=display">
\begin{array}& \text{Repeat}\ \lbrace \newline& \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline& \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline& \rbrace\end{array}</script><h2 id="Lecture-4-5-神经网络"><a href="#Lecture-4-5-神经网络" class="headerlink" title="Lecture 4-5: 神经网络"></a>Lecture 4-5: 神经网络</h2><ul>
<li>大脑学习的原理举例： 人类失去眼睛，可以学习通过超声来判断物体和方位，具备看的能力；</li>
</ul>
<p>If networkhas $s_j$ units in layer j and $s_j +1$ units in layer j+1, then Θ(j) willbe of dimension sj+1×(sj+1).</p>
<h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><script type="math/tex; mode=display">
\begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline\cdots \newline x_n\end{bmatrix} \rightarrow\begin{bmatrix}a_0^{(2)} \newline a_1^{(2)} \newline a_2^{(2)} \newline\cdots\end{bmatrix} \rightarrow\begin{bmatrix}a_0^{(3)} \newline a_1^{(3)} \newline a_2^{(3)} \newline\cdots\end{bmatrix} \rightarrow \cdots \rightarrow\begin{bmatrix}h_\Theta(x)_1 \newline h_\Theta(x)_2 \newline h_\Theta(x)_3 \newline h_\Theta(x)_4 \newline\end{bmatrix} \rightarrow\end{align*}</script><h3 id="代价函数-2"><a href="#代价函数-2" class="headerlink" title="代价函数"></a>代价函数</h3><script type="math/tex; mode=display">
\begin{gather*}\large J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}</script><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><blockquote>
<p>参见： <a href="Backpropagation-in-Neural-Network/">Backpropagation-in-Neural-Network</a></p>
</blockquote>
<h3 id="梯度校验"><a href="#梯度校验" class="headerlink" title="梯度校验"></a>梯度校验</h3><script type="math/tex; mode=display">
\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}</script><h2 id="Lecture-6：采用机器学习的建议"><a href="#Lecture-6：采用机器学习的建议" class="headerlink" title="Lecture 6：采用机器学习的建议"></a>Lecture 6：采用机器学习的建议</h2><h3 id="bias-与-variance"><a href="#bias-与-variance" class="headerlink" title="bias 与 variance"></a>bias 与 variance</h3><ul>
<li>高偏差==欠拟合</li>
<li>高方差==过拟合</li>
</ul>
<p><img src="/qnsource/images/2018-04-25-machine-learning-notes/Features-and-polynom-degree.png" alt="Features-and-polynom-degree"></p>
<p>正则化与偏差/方差：</p>
<ul>
<li>大的$\lambda$ :高偏差（欠拟合）；</li>
<li>小的$\lambda$ :高方差（过拟合）；</li>
</ul>
<p><img src="/qnsource/images/2018-04-25-machine-learning-notes/Features-and-polynom-degree-fix.png" alt="Features-and-polynom-degree-fix"></p>
<blockquote>
<p>选择合适的正则化参数</p>
</blockquote>
<h3 id="学习曲线（Learning-Curves）"><a href="#学习曲线（Learning-Curves）" class="headerlink" title="学习曲线（Learning Curves）"></a>学习曲线（Learning Curves）</h3><blockquote>
<p>错误率随样本数目变化的曲线；</p>
<p>如果算法过拟合，增大样本数有助于提升算法性能</p>
</blockquote>
<p><img src="/qnsource/images/2018-04-25-machine-learning-notes/Learning2.png" alt="Learning2"></p>
<p><img src="/qnsource/images/2018-04-25-machine-learning-notes/Learning1.png" alt="Learning1"></p>
<h2 id="Lecture-7：-SVM"><a href="#Lecture-7：-SVM" class="headerlink" title="Lecture 7： SVM"></a>Lecture 7： SVM</h2><script type="math/tex; mode=display">
z = \theta^Tx</script><script type="math/tex; mode=display">
\text{cost}_0(z) = \max(0, k(1+z))</script><script type="math/tex; mode=display">
\text{cost}_1(z) = \max(0, k(1-z))</script><blockquote>
<p>更多内容查看WiKi： <a href="https://en.wikipedia.org/wiki/Hinge_loss">Hingle loss</a></p>
</blockquote>
<h3 id="代价函数-3"><a href="#代价函数-3" class="headerlink" title="代价函数"></a>代价函数</h3><script type="math/tex; mode=display">
J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j</script><script type="math/tex; mode=display">
C = \frac{1}{\lambda}</script><blockquote>
<p>因为SVM的优化目标为最大化边界举例，所有SVM也叫做Large Margin Classifier</p>
</blockquote>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>实现SVM非线性分类的基础，将低维空间点投射到高维空间，定义近似函数：</p>
<p>高斯核函数(主要参数为 $\sigma$)</p>
<script type="math/tex; mode=display">
f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\sigma^2})</script><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ol>
<li><p>$C=\frac{1}{\lambda}$</p>
<p>如果$C$ 过大，高方差、低偏差；</p>
<p>如果$C$ 过小，高偏差、低方差；</p>
</li>
<li><p>$\sigma^2$</p>
<p>如果$\sigma^2$ 过大，特征更加平缓，导致高偏差，低方差；</p>
<p>如果$\sigma^2$ 过小，特征分布更集中，导致高方差，低偏差；</p>
</li>
</ol>
<h2 id="Lecture-8-K-Means-和-PCA"><a href="#Lecture-8-K-Means-和-PCA" class="headerlink" title="Lecture 8: K-Means 和 PCA"></a>Lecture 8: K-Means 和 PCA</h2><h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><p><img src="/qnsource/images/2018-04-25-machine-learning-notes/BILDt.png" alt="BILDt"></p>
<p>算法实现(<code>Cluster Assignment</code>+<code>Move Centroid</code>)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)</span><br><span class="line">Repeat:</span><br><span class="line">   <span class="keyword">for</span> i = 1 to m:</span><br><span class="line">      c(i):= index (from 1 to K) of cluster centroid closest to x(i)</span><br><span class="line">   <span class="keyword">for</span> k = 1 to K:</span><br><span class="line">      mu(k):= average (mean) of points assigned to cluster k</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
c^{(i)} = argmin_k\ ||x^{(i)} - \mu_k||^2</script><h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><script type="math/tex; mode=display">
J(c^{(i)},\dots,c^{(m)},\mu_1,\dots,\mu_K) = \dfrac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2</script><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ol>
<li>K个初始对象的选择应为随机选取k个训练样本；</li>
<li>为了尽量避免陷入局部极小值，需要进行多次随机初始化求解，选择代价函数最小的解；</li>
<li>K的选择，可以采用<code>elbow method</code>： Choose K at the point where the cost function starts to flatten out.</li>
</ol>
<h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p><strong>降维的目的：</strong> 1. 数据压缩；2. 可视化；</p>
<ul>
<li>The <strong>goal of PCA</strong> is to <strong>reduce</strong> the average of all the distances of every feature to the projection line. </li>
<li>PCA与线性回归不同之处在于，计算的各点到分割线的最短距离；</li>
</ul>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol>
<li><p>数据预处理：归一化；</p>
</li>
<li><p>计算协方差矩阵</p>
<script type="math/tex; mode=display">
\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T</script></li>
<li><p>计算$\Sigma$ 的特征矩阵</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[U,S,V]</span> = svd(Sigma);</span><br></pre></td></tr></table></figure>
</li>
<li><p>取$U$ 的前k列计算$z$</p>
<script type="math/tex; mode=display">
z^{(i)} = Ureduce^T \cdot x^{(i)}</script></li>
</ol>
<h4 id="恢复"><a href="#恢复" class="headerlink" title="恢复"></a>恢复</h4><script type="math/tex; mode=display">
x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}</script><h4 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h4><script type="math/tex; mode=display">
\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01</script><blockquote>
<p>描述了丢失特征的比例</p>
</blockquote>
<p>可以通过SVD分解之后的S值来做同样的事情：</p>
<script type="math/tex; mode=display">
\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99</script><h2 id="Lecture-9：-应用1-异常检测"><a href="#Lecture-9：-应用1-异常检测" class="headerlink" title="Lecture 9： 应用1-异常检测"></a>Lecture 9： 应用1-异常检测</h2><ul>
<li>x(i)= features of user i’s activities</li>
<li>Model p(x) from the data.</li>
<li>Identify unusual users by checking which have p(x)&lt;ϵ.</li>
</ul>
<h4 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h4><p> $x \sim \mathcal{N}(\mu, \sigma^2)$</p>
<script type="math/tex; mode=display">
p(x;\mu,\sigma^2) = \frac{1}{\sigma\sqrt{(2\pi)}}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2}</script><script type="math/tex; mode=display">
p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n)= \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2)</script><h4 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h4><ol>
<li><p>计算均值和方差</p>
</li>
<li><p>计算概率分布函数</p>
<script type="math/tex; mode=display">
p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2) = \prod\limits^n_{j=1} \dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j - \mu_j)^2}{2\sigma^2_j})</script></li>
<li><p>利用标记数据确定ϵ</p>
</li>
</ol>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>为了满足高斯分布，需要对原始数据进行转换：</p>
<ul>
<li>log(x)</li>
<li>log(x+1)</li>
<li>log(x+c) for some constant</li>
<li>$\sqrt x$</li>
<li>$x^{1/3}$</li>
</ul>
<h4 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h4><script type="math/tex; mode=display">
p(x;\mu,\Sigma) = \dfrac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(-1/2(x-\mu)^T\Sigma^{-1}(x-\mu))</script><blockquote>
<p>传统高斯模型乘积是多元多元高斯的一种特例</p>
</blockquote>
<p>$\Sigma$ 为协方差矩阵，如何假设$A$ 为非奇异矩阵，进行特征变换：$y=Ax$,则$\Sigma = AA^T$</p>
<p>多元高斯分布 vs 传统高斯模型</p>
<ul>
<li>多元 更能捕获特征之间的关联性；</li>
<li>多元 计算更耗时；</li>
<li>多元的前提，样本数 &gt; 特征数；m&gt;10n</li>
</ul>
<h2 id="Lecture-10-应用2-推荐系统"><a href="#Lecture-10-应用2-推荐系统" class="headerlink" title="Lecture 10: 应用2-推荐系统"></a>Lecture 10: 应用2-推荐系统</h2><h3 id="问题抽象"><a href="#问题抽象" class="headerlink" title="问题抽象"></a>问题抽象</h3><p>以电影推荐为例，定义如下变量：</p>
<ul>
<li>$n_\mu$ = 用户数目</li>
<li>$n_m$ = 电影数目</li>
<li>$r(i,j)=1$ = 1,如果用户j对电影i评分</li>
<li>$y(i,j)=rating_score$ </li>
</ul>
<h3 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h3><script type="math/tex; mode=display">
min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u}  \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2</script><h3 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h3><blockquote>
<p>既学习参数又学习特征表示</p>
</blockquote>
<script type="math/tex; mode=display">
J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>SVM：<a href="http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf">http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</a></li>
<li></li>
</ol>

        </div>
        
        <hr style="height:1px;margin:1rem 0"/>
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/机器学习/">机器学习</a>,&nbsp;<a class="has-link-grey -link" href="/tags/神经网络/">神经网络</a>,&nbsp;<a class="has-link-grey -link" href="/tags/笔记/">笔记</a>,&nbsp;<a class="has-link-grey -link" href="/tags/算法/">算法</a>,&nbsp;<a class="has-link-grey -link" href="/tags/线性回归/">线性回归</a>,&nbsp;<a class="has-link-grey -link" href="/tags/逻辑回归/">逻辑回归</a>
                </div>
            </div>
        </div>
        
        
        
        
<div class="sharethis-inline-share-buttons"></div>
<script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=5e762f775039a80012d346d9&amp;product=inline-share-buttons&amp;cms=sop' async='async'></script>

        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/site/alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/site/weixin.png" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2018/04/26/machinelearning-labs/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">《机器学习》课程实验回顾</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2018/04/21/机器学习三要素-模型-策略与算法/">
                <span class="level-item">机器学习三要素：模型,策略与算法</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: false,
        verify: false,
        app_id: 'JQMgg0zbFBNWwL0lLnq2s1G7-gzGzoHsz',
        app_key: 'm5FMVedFNxGutQCnMsVMAaXM',
        placeholder: 'Say Something ...'
    });
</script>

    </div>
</div>
</div>
                
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-3 column-right is-sticky">
    
        

    <div class="card widget" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Lecture-1：-机器学习的定义">
        <span class="has-mr-6">1</span>
        <span>Lecture 1： 机器学习的定义</span>
        </a></li><li>
        <a class="is-flex" href="#Lecture-2：-线性回归（多属性）">
        <span class="has-mr-6">2</span>
        <span>Lecture 2： 线性回归（多属性）</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#变量定义">
        <span class="has-mr-6">2.1</span>
        <span>变量定义</span>
        </a></li><li>
        <a class="is-flex" href="#多元线性规划模型">
        <span class="has-mr-6">2.2</span>
        <span>多元线性规划模型</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#假设函数">
        <span class="has-mr-6">2.2.1</span>
        <span>假设函数</span>
        </a></li><li>
        <a class="is-flex" href="#代价函数">
        <span class="has-mr-6">2.2.2</span>
        <span>代价函数</span>
        </a></li><li>
        <a class="is-flex" href="#梯度">
        <span class="has-mr-6">2.2.3</span>
        <span>梯度</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#特征正则化">
        <span class="has-mr-6">2.3</span>
        <span>特征正则化</span>
        </a></li><li>
        <a class="is-flex" href="#参数分析">
        <span class="has-mr-6">2.4</span>
        <span>参数分析</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Lecture-3：逻辑回归">
        <span class="has-mr-6">3</span>
        <span>Lecture 3：逻辑回归</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#假设函数-1">
        <span class="has-mr-6">3.1</span>
        <span>假设函数</span>
        </a></li><li>
        <a class="is-flex" href="#代价函数-1">
        <span class="has-mr-6">3.2</span>
        <span>代价函数</span>
        </a></li><li>
        <a class="is-flex" href="#梯度-1">
        <span class="has-mr-6">3.3</span>
        <span>梯度</span>
        </a></li><li>
        <a class="is-flex" href="#多分类：-One-vs-all">
        <span class="has-mr-6">3.4</span>
        <span>多分类： One-vs-all</span>
        </a></li><li>
        <a class="is-flex" href="#正则化">
        <span class="has-mr-6">3.5</span>
        <span>正则化</span>
        </a></li><li>
        <a class="is-flex" href="#正则化的线性回归">
        <span class="has-mr-6">3.6</span>
        <span>正则化的线性回归</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#添加正则化的数值解">
        <span class="has-mr-6">3.6.1</span>
        <span>添加正则化的数值解</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#正则化的逻辑回归">
        <span class="has-mr-6">3.7</span>
        <span>正则化的逻辑回归</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Lecture-4-5-神经网络">
        <span class="has-mr-6">4</span>
        <span>Lecture 4-5: 神经网络</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#多分类">
        <span class="has-mr-6">4.1</span>
        <span>多分类</span>
        </a></li><li>
        <a class="is-flex" href="#代价函数-2">
        <span class="has-mr-6">4.2</span>
        <span>代价函数</span>
        </a></li><li>
        <a class="is-flex" href="#反向传播">
        <span class="has-mr-6">4.3</span>
        <span>反向传播</span>
        </a></li><li>
        <a class="is-flex" href="#梯度校验">
        <span class="has-mr-6">4.4</span>
        <span>梯度校验</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Lecture-6：采用机器学习的建议">
        <span class="has-mr-6">5</span>
        <span>Lecture 6：采用机器学习的建议</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#bias-与-variance">
        <span class="has-mr-6">5.1</span>
        <span>bias 与 variance</span>
        </a></li><li>
        <a class="is-flex" href="#学习曲线（Learning-Curves）">
        <span class="has-mr-6">5.2</span>
        <span>学习曲线（Learning Curves）</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Lecture-7：-SVM">
        <span class="has-mr-6">6</span>
        <span>Lecture 7： SVM</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#代价函数-3">
        <span class="has-mr-6">6.1</span>
        <span>代价函数</span>
        </a></li><li>
        <a class="is-flex" href="#核函数">
        <span class="has-mr-6">6.2</span>
        <span>核函数</span>
        </a></li><li>
        <a class="is-flex" href="#参数">
        <span class="has-mr-6">6.3</span>
        <span>参数</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Lecture-8-K-Means-和-PCA">
        <span class="has-mr-6">7</span>
        <span>Lecture 8: K-Means 和 PCA</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#K-Means">
        <span class="has-mr-6">7.1</span>
        <span>K-Means</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#优化目标">
        <span class="has-mr-6">7.1.1</span>
        <span>优化目标</span>
        </a></li><li>
        <a class="is-flex" href="#注意事项">
        <span class="has-mr-6">7.1.2</span>
        <span>注意事项</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#PCA">
        <span class="has-mr-6">7.2</span>
        <span>PCA</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#算法流程">
        <span class="has-mr-6">7.2.1</span>
        <span>算法流程</span>
        </a></li><li>
        <a class="is-flex" href="#恢复">
        <span class="has-mr-6">7.2.2</span>
        <span>恢复</span>
        </a></li><li>
        <a class="is-flex" href="#k值的选择">
        <span class="has-mr-6">7.2.3</span>
        <span>k值的选择</span>
        </a></li></ul></li></ul></li><li>
        <a class="is-flex" href="#Lecture-9：-应用1-异常检测">
        <span class="has-mr-6">8</span>
        <span>Lecture 9： 应用1-异常检测</span>
        </a><ul class="menu-list"><ul class="menu-list"><li>
        <a class="is-flex" href="#高斯分布">
        <span class="has-mr-6">8.1.1</span>
        <span>高斯分布</span>
        </a></li><li>
        <a class="is-flex" href="#算法流程-1">
        <span class="has-mr-6">8.1.2</span>
        <span>算法流程</span>
        </a></li><li>
        <a class="is-flex" href="#数据预处理">
        <span class="has-mr-6">8.1.3</span>
        <span>数据预处理</span>
        </a></li><li>
        <a class="is-flex" href="#多元高斯分布">
        <span class="has-mr-6">8.1.4</span>
        <span>多元高斯分布</span>
        </a></li></ul></ul></li><li>
        <a class="is-flex" href="#Lecture-10-应用2-推荐系统">
        <span class="has-mr-6">9</span>
        <span>Lecture 10: 应用2-推荐系统</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#问题抽象">
        <span class="has-mr-6">9.1</span>
        <span>问题抽象</span>
        </a></li><li>
        <a class="is-flex" href="#基于内容的推荐">
        <span class="has-mr-6">9.2</span>
        <span>基于内容的推荐</span>
        </a></li><li>
        <a class="is-flex" href="#协同过滤">
        <span class="has-mr-6">9.3</span>
        <span>协同过滤</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#参考">
        <span class="has-mr-6">10</span>
        <span>参考</span>
        </a></li></ul>
            </div>
        </div>
    </div>

    
        <div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2021/02/05/那些效率提升的工具与方法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="那些效率提升的工具与方法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2021-02-05T14:10:00.000Z">2021-02-05</time></div>
                    <a href="/2021/02/05/那些效率提升的工具与方法/" class="title has-link-black-ter is-size-6 has-text-weight-normal">那些效率提升的工具与方法</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/总结/">总结</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/blog/rl.jpg" alt="Reinforcement Learning:Tips and Tricks">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2021-01-28T02:56:30.000Z">2021-01-28</time></div>
                    <a href="/2021/01/28/Reinforcement-Learning-Tips-and-Tricks/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Reinforcement Learning:Tips and Tricks</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/强化学习/">强化学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/11/19/pytorch模型的导出与部署/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="pytorch模型的导出与部署">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-11-19T05:00:18.000Z">2020-11-19</time></div>
                    <a href="/2020/11/19/pytorch模型的导出与部署/" class="title has-link-black-ter is-size-6 has-text-weight-normal">pytorch模型的导出与部署</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/05/04/读书笔记-《神经网络与深度学习》/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/blog/qxp.jpg" alt="读书笔记-《神经网络与深度学习》">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-05-04T13:50:21.000Z">2020-05-04</time></div>
                    <a href="/2020/05/04/读书笔记-《神经网络与深度学习》/" class="title has-link-black-ter is-size-6 has-text-weight-normal">读书笔记-《神经网络与深度学习》</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/04/19/计算机视觉目标检测研究札记/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/../qnsource/banner/02.jpg" alt="计算机视觉目标检测研究札记">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-04-19T07:10:45.000Z">2020-04-19</time></div>
                    <a href="/2020/04/19/计算机视觉目标检测研究札记/" class="title has-link-black-ter is-size-6 has-text-weight-normal">计算机视觉目标检测研究札记</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/深度学习/">深度学习</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo02.png" alt="Machine Learning Notes" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Ebby DD&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>  沪ICP备20005404号
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://blog.a-stack.com',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>