<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>cs231nè¯¾ç¨‹ç¬”è®°ï¼šï¼ˆlecture 4ï¼‰ç¥ç»ç½‘ç»œåŸºç¡€ | Ebby&#39;s Notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="äººå·¥æ™ºèƒ½,æ·±åº¦å­¦ä¹ ,æŠ€æœ¯,ç®—æ³•" />
  
  
  
  
  <meta name="description" content="æ‘˜è¦ï¼š è®¡åˆ’èŠ±ä¸€ä¸ªæœˆçš„æ—¶é—´åˆ·ä¸€éæ–¯å¦ç¦çš„æœºå™¨è§†è§‰è¯¾ç¨‹cs231nï¼Œå¹¶åšç¬”è®°è®°å½•æ¯å¤©å­¦ä¹ åˆ°çš„å†…å®¹ã€‚  æ¦‚è¿°cs231næ˜¯æ–¯å¦ç¦åœ¨æ·±åº¦å­¦ä¹ å’Œæœºå™¨è§†è§‰é¢†åŸŸçš„å…¥é—¨ç»å…¸è¯¾ç¨‹ï¼Œç›¸å…³èµ„æºå¦‚ä¸‹ï¼š  è¯¾ç¨‹ä¸»é¡µï¼š http://cs231n.stanford.edu/ è¯¾ç¨‹Notesï¼šhttp://cs231n.github.io/      Event Type Date Description Course Ma">
<meta name="keywords" content="äººå·¥æ™ºèƒ½,æ·±åº¦å­¦ä¹ ,æŠ€æœ¯,ç®—æ³•">
<meta property="og:type" content="article">
<meta property="og:title" content="cs231nè¯¾ç¨‹ç¬”è®°ï¼šï¼ˆLecture 4ï¼‰ç¥ç»ç½‘ç»œåŸºç¡€">
<meta property="og:url" content="http://blog.a-stack.com/2018/05/03/cs231n-lecture-4/index.html">
<meta property="og:site_name" content="Ebby&#39;s Notes">
<meta property="og:description" content="æ‘˜è¦ï¼š è®¡åˆ’èŠ±ä¸€ä¸ªæœˆçš„æ—¶é—´åˆ·ä¸€éæ–¯å¦ç¦çš„æœºå™¨è§†è§‰è¯¾ç¨‹cs231nï¼Œå¹¶åšç¬”è®°è®°å½•æ¯å¤©å­¦ä¹ åˆ°çš„å†…å®¹ã€‚  æ¦‚è¿°cs231næ˜¯æ–¯å¦ç¦åœ¨æ·±åº¦å­¦ä¹ å’Œæœºå™¨è§†è§‰é¢†åŸŸçš„å…¥é—¨ç»å…¸è¯¾ç¨‹ï¼Œç›¸å…³èµ„æºå¦‚ä¸‹ï¼š  è¯¾ç¨‹ä¸»é¡µï¼š http://cs231n.stanford.edu/ è¯¾ç¨‹Notesï¼šhttp://cs231n.github.io/      Event Type Date Description Course Ma">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/neuron_model.png">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/activation_functions.png">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/prepro1.jpeg">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/prepro2.jpeg">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/cifar10pca.jpeg">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/learningrates.jpeg">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/accuracies.jpeg">
<meta property="og:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/gridsearchbad.jpeg">
<meta property="og:updated_time" content="2018-05-16T13:59:45.204Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="cs231nè¯¾ç¨‹ç¬”è®°ï¼šï¼ˆLecture 4ï¼‰ç¥ç»ç½‘ç»œåŸºç¡€">
<meta name="twitter:description" content="æ‘˜è¦ï¼š è®¡åˆ’èŠ±ä¸€ä¸ªæœˆçš„æ—¶é—´åˆ·ä¸€éæ–¯å¦ç¦çš„æœºå™¨è§†è§‰è¯¾ç¨‹cs231nï¼Œå¹¶åšç¬”è®°è®°å½•æ¯å¤©å­¦ä¹ åˆ°çš„å†…å®¹ã€‚  æ¦‚è¿°cs231næ˜¯æ–¯å¦ç¦åœ¨æ·±åº¦å­¦ä¹ å’Œæœºå™¨è§†è§‰é¢†åŸŸçš„å…¥é—¨ç»å…¸è¯¾ç¨‹ï¼Œç›¸å…³èµ„æºå¦‚ä¸‹ï¼š  è¯¾ç¨‹ä¸»é¡µï¼š http://cs231n.stanford.edu/ è¯¾ç¨‹Notesï¼šhttp://cs231n.github.io/      Event Type Date Description Course Ma">
<meta name="twitter:image" content="http://blog.a-stack.com/qnsource/images/2018-04-28-cs231n-notes/neuron_model.png">
  
    <link rel="alternate" href="/atom.xml" title="Ebby&#39;s Notes" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/logo.png">
  <link rel="apple-touch-icon" href="/css/images/logo.png">
  
    <link href="http://p4ygzcmtw.bkt.clouddn.com/css/Source+Code+Pro.css" rel="stylesheet" type="text/css">
  
  <link href="http://p4ygzcmtw.bkt.clouddn.com/css/Open+Sans-Montserrat-700.css" rel="stylesheet" type="text/css">
  <link href="http://p4ygzcmtw.bkt.clouddn.com/css/Roboto-400-300.css" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>
  <script src="/js/bootstrap.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    <link rel="stylesheet" href="/css/dialog.css">
  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  
    <link rel="stylesheet" href="/css/vdonate.css" >
  

</head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 border-width: 0px;  margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="200px" height="100px" alt="Hike News" src="/css/images/logo.png">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">é¦–é¡µ</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">å½’æ¡£</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">åˆ†ç±»</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">æ ‡ç­¾</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">å…³äº</a> </li>
                
                  <li> <a class="main-nav-link" href="/reading">è¯»ä¹¦</a> </li>
                
                  <li> <a class="main-nav-link" href="/resources">èµ„æº</a> </li>
                
                  <li> <a class="main-nav-link" href="/notebooks">ğŸ“</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="è¯·è¾“å…¥å…³é”®è¯..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'æ–‡ç« ',
            PAGES: 'é¡µé¢',
            CATEGORIES: 'åˆ†ç±»',
            TAGS: 'æ ‡ç­¾',
            UNTITLED: '(æ— æ ‡é¢˜)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-cs231n-lecture-4" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      cs231nè¯¾ç¨‹ç¬”è®°ï¼šï¼ˆLecture 4ï¼‰ç¥ç»ç½‘ç»œåŸºç¡€
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2018/05/03/cs231n-lecture-4/" class="article-date">
	  <time datetime="2018-05-03T08:52:26.000Z" itemprop="datePublished">2018-05-03</time>
	</a>

      
    <a class="article-category-link" href="/categories/è¯»ä¹¦ç¬”è®°/">è¯»ä¹¦ç¬”è®°</a><a class="article-category-link" href="/categories/è¯»ä¹¦ç¬”è®°/è¯¾ç¨‹ç¬”è®°/">è¯¾ç¨‹ç¬”è®°</a>

      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		é˜…è¯»é‡<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>æ‘˜è¦ï¼š</strong> è®¡åˆ’èŠ±ä¸€ä¸ªæœˆçš„æ—¶é—´åˆ·ä¸€éæ–¯å¦ç¦çš„æœºå™¨è§†è§‰è¯¾ç¨‹cs231nï¼Œå¹¶åšç¬”è®°è®°å½•æ¯å¤©å­¦ä¹ åˆ°çš„å†…å®¹ã€‚</p>
<!-- excerpt -->
<h2 id="æ¦‚è¿°"><a href="#æ¦‚è¿°" class="headerlink" title="æ¦‚è¿°"></a>æ¦‚è¿°</h2><p>cs231næ˜¯æ–¯å¦ç¦åœ¨æ·±åº¦å­¦ä¹ å’Œæœºå™¨è§†è§‰é¢†åŸŸçš„å…¥é—¨ç»å…¸è¯¾ç¨‹ï¼Œç›¸å…³èµ„æºå¦‚ä¸‹ï¼š</p>
<ul>
<li>è¯¾ç¨‹ä¸»é¡µï¼š <a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">http://cs231n.stanford.edu/</a></li>
<li>è¯¾ç¨‹Notesï¼š<a href="http://cs231n.github.io/" target="_blank" rel="noopener">http://cs231n.github.io/</a></li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Event Type</th>
<th>Date</th>
<th>Description</th>
<th>Course Materials</th>
</tr>
</thead>
<tbody>
<tr>
<td>Lecture 4</td>
<td>Thursday April 12</td>
<td><strong>Introduction to Neural Networks</strong> BackpropagationMulti-layer PerceptronsThe neural viewpoint</td>
<td><a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture04.pdf" target="_blank" rel="noopener">[slides]</a> <a href="http://cs231n.github.io/optimization-2" target="_blank" rel="noopener">[backprop notes]</a><a href="http://cs231n.stanford.edu/handouts/linear-backprop.pdf" target="_blank" rel="noopener">[linear backprop example]</a><a href="http://cs231n.stanford.edu/handouts/derivatives.pdf" target="_blank" rel="noopener">[derivatives notes]</a> (optional) <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">[Efficient BackProp]</a> (optional)related: <a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">[1]</a>, <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">[2]</a>, <a href="https://www.youtube.com/watch?v=q0pm3BrIUFo" target="_blank" rel="noopener">[3]</a> (optional)</td>
</tr>
<tr>
<td>Discussion Section</td>
<td>Friday April 13</td>
<td><strong>Backpropagation</strong></td>
<td><a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds02.pdf" target="_blank" rel="noopener">[slides]</a></td>
</tr>
</tbody>
</table>
</div>
<h2 id="ç¥ç»ç½‘ç»œ"><a href="#ç¥ç»ç½‘ç»œ" class="headerlink" title="ç¥ç»ç½‘ç»œ"></a>ç¥ç»ç½‘ç»œ</h2><blockquote>
<p>æ›´å¤šå†…å®¹å‚è§<sup><a href="#fn_1" id="reffn_1">1</a></sup><sup><a href="#fn_2" id="reffn_2">2</a></sup><sup><a href="#fn_3" id="reffn_3">3</a></sup></p>
<p>è¿™è¯¾ç¨‹çš„å¤§å¤šæ•°å†…å®¹ä¹Ÿç»¼åˆäº†Lecture 6å’ŒLecutre 7çš„å†…å®¹ï¼Œæ‰€ä»¥é‚£ä¸¤èŠ‚è¯¾ä¸å•ç‹¬å†™åšå®¢äº†ã€‚</p>
</blockquote>
<ul>
<li>ä¸€ä¸ªäººçš„ç¥ç»ç³»ç»Ÿå¤§æ¦‚æœ‰860äº¿ä¸ªç¥ç»å…ƒï¼Œå½¢æˆ$10^{14}-10^{15}$ ä¸ªç¥ç»å…ƒé“¾æ¥ï¼›</li>
</ul>
<p><img src="/qnsource/images/2018-04-28-cs231n-notes/neuron_model.png" alt="neuron_model"></p>
<blockquote>
<p><strong>Coarse model.</strong> Itâ€™s important to stress that this model of a biological neuron is very coarse: For example, there are many different types of neurons, each with different properties. The dendrites in biological neurons perform complex nonlinear computations. The synapses are not just a single weight, theyâ€™re a complex non-linear dynamical system. The exact timing of the output spikes in many systems is known to be important, suggesting that the rate code approximation may not hold. Due to all these and many other simplifications, be prepared to hear groaning sounds from anyone with some neuroscience background if you draw analogies between Neural Networks and real brains. See this <a href="https://physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf" target="_blank" rel="noopener">review</a> (pdf), or more recently this <a href="http://www.sciencedirect.com/science/article/pii/S0959438814000130" target="_blank" rel="noopener">review</a> if you are interested.</p>
</blockquote>
<ul>
<li><p><strong>æ¿€æ´»å‡½æ•°</strong></p>
<p><img src="/qnsource/images/2018-04-28-cs231n-notes/activation_functions.png" alt="activation_functions"></p>
<ul>
<li>Sigmoidï¼š $\sigma(x) = 1 / (1 + e^{-x})$<ul>
<li>1)æ¢¯åº¦é¥±å’Œï¼›2)å‡½æ•°å½¢å¼ä¸æ˜¯ä¸­å¿ƒå¯¹ç§°çš„ï¼ˆè¿™ä¸ªç‰¹æ€§ä¼šå¯¼è‡´åé¢ç½‘ç»œå±‚çš„è¾“å…¥ä¹Ÿä¸æ˜¯é›¶ä¸­å¿ƒçš„ï¼Œè¿›è€Œå½±å“æ¢¯åº¦ä¸‹é™çš„è¿ä½œï¼‰ï¼›</li>
</ul>
</li>
<li>Tanh: $\tanh(x) = 2 \sigma(2x) -1$<ul>
<li>1)æ¢¯åº¦é¥±å’Œï¼›2ï¼‰ä¸­å¿ƒå¯¹ç§°[-1,1]ï¼›</li>
</ul>
</li>
<li>ReLU:  $f(x) = \max(0, x)$<ul>
<li>1)çº¿æ€§éé¥±å’Œçš„å½¢å¼ä½¿å¾—å…¶æ”¶æ•›é€Ÿåº¦æ˜¯Tanhçš„6å€ï¼› 2ï¼‰è®¡ç®—ä»£ä»·ä½ï¼›3ï¼‰éš¾è®­ç»ƒï¼Œå®¹æ˜“é™·å…¥â€æ­»åŒºâ€ï¼Œéœ€è¦ä¸¥æ ¼é…ç½®åˆé€‚çš„å­¦ä¹ ç‡ï¼›</li>
</ul>
</li>
<li>Leaky ReLUï¼š $f(x) = \mathbb{1}(x &lt; 0) (\alpha x) + \mathbb{1}(x&gt;=0) (x)$<ul>
<li>å¦ä¸€ç§æ³›åŒ–å½¢åŠ¿ä¸ºå‚æ•°ReLU</li>
<li>å¦ä¸€ç§ELU</li>
</ul>
</li>
<li><strong>Maxout</strong>: $\max(w_1^Tx+b_1, w_2^Tx + b_2)$<ul>
<li>ReLUå’ŒLeaky ReLUæ˜¯å…¶ç‰¹ä¾‹ï¼›</li>
</ul>
</li>
</ul>
</li>
<li><p>å…³äºç½‘ç»œè§„æ¨¡å¤§å°</p>
<ul>
<li>ä¸€èˆ¬æµ…å±‚ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­å®¹æ˜“é™·å…¥å±€éƒ¨æå°å€¼ï¼Œå¾ˆéš¾è®­ç»ƒï¼›</li>
<li>æ‰€ä»¥åŠæ—¶è¿‡æ‹Ÿåˆï¼Œä¹Ÿå°½é‡ä¸é‡‡ç”¨è¿‡å°çš„ç¥ç»ç½‘ç»œï¼ˆå¯ä»¥é‡‡ç”¨å…¶å®ƒæ‰‹æ®µå¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜ï¼‰</li>
</ul>
</li>
</ul>
<h3 id="æ•°æ®é¢„å¤„ç†"><a href="#æ•°æ®é¢„å¤„ç†" class="headerlink" title="æ•°æ®é¢„å¤„ç†"></a>æ•°æ®é¢„å¤„ç†</h3><ol>
<li>å‡å€¼å¤„ç†ï¼ˆä¸­å¿ƒåŒ–ï¼‰ï¼š<code>X -= np.mean(X, axis = 0)</code></li>
<li>å½’ä¸€åŒ–ï¼š<code>X /= np.std(X, axis = 0)</code></li>
</ol>
<blockquote>
<p>ä½¿ç”¨å½’ä¸€åŒ–éœ€è¦æ³¨æ„ï¼š åªæœ‰åœ¨ä¸åŒå˜é‡çš„å¤§å°å¯¹äºè¾“å‡ºåŒç­‰é‡è¦çš„æƒ…å†µä¸‹é‡‡ç”¨å½’ä¸€åŒ–ï¼›å›¾åƒä¸­ç”±äºæ‰€æœ‰å˜é‡éƒ½å·²ç»åœ¨åŒºé—´[0,255]èŒƒå›´ä¹‹ä¸­ï¼Œæ‰€ä»¥æ²¡æœ‰å¿…è¦é‡‡ç”¨å½’ä¸€åŒ–</p>
</blockquote>
<p><img src="/qnsource/images/2018-04-28-cs231n-notes/prepro1.jpeg" alt="prepro1"></p>
<ol>
<li>PCAå’Œç™½åŒ–</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume input data matrix X of size [N x D]</span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># zero-center the data (important)</span></span><br><span class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># get the data covariance matrix</span></span><br><span class="line"></span><br><span class="line">Uï¼ŒS, V = np.linalg.svd(cov)</span><br><span class="line">Xrot = np.dot(X, U) <span class="comment"># decorrelate the data</span></span><br><span class="line"><span class="comment"># 1. PCA</span></span><br><span class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># Xrot_reduced becomes [N x 100]</span></span><br><span class="line"><span class="comment">#2 . whiten the data:</span></span><br><span class="line"><span class="comment"># divide by the eigenvalues (which are square roots of the singular values)</span></span><br><span class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Uä¸ºæ­£äº¤ç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥åªé€‰æ‹©top Né‡è¦çš„ç‰¹å¾å‘é‡æ¥è¿›è¡Œé™ç»´å¤„ç†ï¼›</p>
<p>PCAï¼š ä¸‹å›¾æ‰€ç¤ºï¼Œæ•°æ®ä¸­å¿ƒå¯¹é½ï¼Œä»¥ç‰¹å¾å‘é‡æ–¹å‘æ—‹è½¬å¯¹é½ï¼›</p>
<p>ç™½åŒ–æ“ä½œï¼šå¦‚æœè¾“å…¥æ•°æ®ä¸ºå¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œç™½åŒ–ç»“æœä¸ºé«˜æ–¯é›¶å‡å€¼ç‹¬ç«‹åæ–¹å·®çŸ©é˜µ</p>
</blockquote>
<p><img src="/qnsource/images/2018-04-28-cs231n-notes/prepro2.jpeg" alt="prepro2"></p>
<p>ä»¥CIFAR-10ä¸ºä¾‹ï¼Œå¤„ç†ä¹‹åæ•ˆæœè§ä¸‹å›¾ï¼Œç¬¬2å¼ ä¸ºå–3072ä¸ªç‰¹å¾ä¸­å‰144ä¸ªç‰¹å¾ï¼š</p>
<p><img src="/qnsource/images/2018-04-28-cs231n-notes/cifar10pca.jpeg" alt="cifar10pca"></p>
<blockquote>
<p>æ‰€æœ‰é¢„å¤„ç†è¿‡ç¨‹ä»…ä½œç”¨åˆ°è®­ç»ƒæ•°æ®ä¸­ï¼Œå¹¶è®°å½•ç›¸å…³å‚æ•°ï¼ŒéªŒè¯æ•°æ®å’Œæµ‹è¯•æ•°æ®é‡‡ç”¨è®­ç»ƒæ•°æ®çš„ç»“æœè¿›è¡Œé¢„å¤„ç†</p>
</blockquote>
<h3 id="å‚æ•°åˆå§‹åŒ–"><a href="#å‚æ•°åˆå§‹åŒ–" class="headerlink" title="å‚æ•°åˆå§‹åŒ–"></a>å‚æ•°åˆå§‹åŒ–</h3><ul>
<li><code>W = 0.01* np.random.randn(D,H)</code></li>
<li><code>w = np.random.randn(n) / sqrt(2.0/n)</code></li>
</ul>
<blockquote>
<p><code>randn</code> äº§ç”Ÿé›¶å‡å€¼å•ä½æ–¹å·®é«˜æ–¯åˆ†å¸ƒ<br>wå‚æ•°ä¸èƒ½å…¨éƒ¨åˆå§‹åŒ–ä¸ºé›¶ï¼ˆå¯¹ç§°æ•ˆåº”å¯¼è‡´ç½‘ç»œæ¿€æ´»ä¸æ›´æ–°ï¼‰ï¼Œbiaså‚æ•°ä¸€èˆ¬åˆå§‹åŒ–ä¸ºé›¶<br>ç¬¬äºŒä¸ªå…¬å¼ä¸ºäº†è§£å†³è¾“å‡ºéšnçš„å¢åŠ è€Œæ¯”ä¾‹å¢åŠ çš„é—®é¢˜</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{align}
\text{Var}(s) &= \text{Var}(\sum_i^n w_ix_i) \\
&= \sum_i^n \text{Var}(w_ix_i) \\
&= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + E[(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \\
&= \sum_i^n \text{Var}(x_i)\text{Var}(w_i) \\
&= \left( n \text{Var}(w) \right) \text{Var}(x)
\end{align}</script><h3 id="æ­£åˆ™åŒ–"><a href="#æ­£åˆ™åŒ–" class="headerlink" title="æ­£åˆ™åŒ–"></a>æ­£åˆ™åŒ–</h3><ul>
<li>L2æ­£åˆ™åŒ–å¯¹äºå³°å€¼æƒé‡å¢åŠ æ¯”è¾ƒå¤§çš„æƒ©ç½šï¼Œå¯¹å¹³æ»‘çš„æƒé‡çŸ©é˜µæ›´åŠ å‹å¥½ï¼›</li>
<li>L1æ­£åˆ™åŒ–çš„ä¸€ä¸ªæ•ˆæœæ˜¯ä½¿å¾—è¾“å…¥å‚æ•°ç¨€ç–åŒ–ï¼Œä»è€Œè¿‡æ»¤è¾“å…¥ä¸­çš„å™ªå£°æ•°æ®ï¼›</li>
<li>æœ€å¤§åŸºå‡†é—¨é™ï¼š$\Vert \vec{w} \Vert_2 &lt; c$</li>
<li>Dropout: åœ¨é¢„æµ‹é˜¶æ®µä¸è¦ä½¿ç”¨<ul>
<li>inverted dropout</li>
<li>ä¸€èˆ¬ $p=0.5$</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" </span></span><br><span class="line"><span class="string">Inverted Dropout: Recommended implementation example.</span></span><br><span class="line"><span class="string">We drop and scale at train time and don't do anything at test time.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># probability of keeping a unit active. higher = less dropout</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># forward pass for example 3-layer neural network</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># first dropout mask. Notice /p!</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># second dropout mask. Notice /p!</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># backward pass: compute gradients... (not shown)</span></span><br><span class="line">  <span class="comment"># perform parameter update... (not shown)</span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># ensembled forward pass</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># no scaling necessary</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure>
<h3 id="ä»£ä»·å‡½æ•°ï¼ˆdata-lossï¼‰"><a href="#ä»£ä»·å‡½æ•°ï¼ˆdata-lossï¼‰" class="headerlink" title="ä»£ä»·å‡½æ•°ï¼ˆdata lossï¼‰"></a>ä»£ä»·å‡½æ•°ï¼ˆdata lossï¼‰</h3><ol>
<li>åˆ†ç±»é—®é¢˜</li>
</ol>
<ul>
<li><p>SVMä»£ä»·å‡½æ•°(åˆé¡µæŸå¤±å‡½æ•°)</p>
<script type="math/tex; mode=display">
L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)</script></li>
<li><p>äº¤å‰ç†µæŸå¤±å‡½æ•°</p>
<script type="math/tex; mode=display">
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)</script></li>
</ul>
<blockquote>
<p>å½“åˆ†ç±»æ•°ç›®ç‰¹åˆ«å¤§æ—¶ï¼Œå¯é‡‡ç”¨å±‚æ¬¡Softmax</p>
</blockquote>
<ol>
<li><p>å¤šå±æ€§åˆ†ç±»</p>
<p>å¦‚æœä¸€ä¸ªåˆ†ç±»ç›®æ ‡å¯¹åº”å¤šä¸ªæ­£ç¡®çš„ç»“æœï¼Œæ¯”å¦‚å¯¹ä¸€å¼ å›¾ç‰‡åˆ†ç±»ï¼Œå¯èƒ½åŒæ—¶æœ‰å¤šä¸ªæ­£ç¡®æ ‡ç­¾ã€‚å¯ä»¥å¯¹æ¯ä¸ªå±æ€§æ„å»ºä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨ï¼š</p>
</li>
</ol>
<script type="math/tex; mode=display">
L_i = \sum_j \max(0, 1 - y_{ij} f_j)</script><p>â€‹    å…¶ä¸­ï¼Œjä¸ºæŸä¸ªåˆ†ç±»ï¼Œ$y_{ij}$ ä¸º+1æˆ–-1ï¼Œè¡¨ç¤ºç¬¬iä¸ªæ ·æœ¬æ˜¯å¦å«æœ‰ç¬¬jä¸ªå±æ€§ï¼›$f_i$ ä¸ºæ­£ä»£è¡¨é¢„æµ‹æ­£ç¡®ï¼›</p>
<p>â€‹    ç¬¬äºŒç§æ–¹æ³•æ˜¯å¯¹æ¯ä¸ªå±æ€§åº”ç”¨é€»è¾‘å›å½’åˆ†ç±»å™¨ï¼š</p>
<script type="math/tex; mode=display">
L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))</script><p>â€‹    $\partial{L<em>i} / \partial{f_j} = y</em>{ij} - \sigma(f_j)$ã€‚</p>
<ol>
<li>å›å½’é—®é¢˜</li>
</ol>
<ul>
<li><p>L2: $L_i = \Vert f - y_i \Vert_2^2$</p>
<p>â€‹</p>
</li>
<li><p>L1: $L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - (y_i)_j \mid$</p>
<p>â€‹</p>
</li>
</ul>
<h3 id="è®­ç»ƒ3"><a href="#è®­ç»ƒ3" class="headerlink" title="è®­ç»ƒ3"></a>è®­ç»ƒ<sup><a href="#fn_3" id="reffn_3">3</a></sup></h3><h4 id="æ¢¯åº¦æ ¡éªŒ"><a href="#æ¢¯åº¦æ ¡éªŒ" class="headerlink" title="æ¢¯åº¦æ ¡éªŒ"></a>æ¢¯åº¦æ ¡éªŒ</h4><script type="math/tex; mode=display">
\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in}</script><blockquote>
<p>hå¯ä»¥è®¾ç½®ä¸º1e-5</p>
</blockquote>
<ol>
<li>å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å€¼æ¥åˆ¤æ–­æ ¡éªŒç»“æœï¼š</li>
</ol>
<script type="math/tex; mode=display">
\frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)}</script><blockquote>
<ul>
<li>relative error &gt; 1e-2 usually means the gradient is probably wrong</li>
<li>1e-2 &gt; relative error &gt; 1e-4 should make you feel uncomfortable</li>
<li>1e-4 &gt; relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high.</li>
<li>1e-7 and less you should be happy.</li>
<li>ç½‘ç»œå±‚æ•°è¶Šå¤šï¼Œç›¸å¯¹è¯¯å·®å€¼è¶Šå¤§</li>
</ul>
</blockquote>
<ol>
<li>è¿›è¡Œæ¢¯åº¦æ ¡éªŒï¼Œå…³é—­dropoutã€æ•°æ®æ”¾å¤§(æˆ–ä½¿ç”¨random seed)</li>
</ol>
<h4 id="Sanity-Checks"><a href="#Sanity-Checks" class="headerlink" title="Sanity Checks"></a>Sanity Checks</h4><ul>
<li>Losså‡½æ•°çš„åˆå§‹åŒ–è¾“å‡ºå€¼æ˜¯å¦åˆç†ï¼›<ul>
<li>å¢åŠ æ­£åˆ™åŒ–å‚æ•°ï¼Œlossæ˜¯å¦å¢åŠ </li>
</ul>
</li>
<li>ä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•æ˜¯å¦èƒ½å¤Ÿè¾¾åˆ°æ•°æ®è¿‡æ‹Ÿåˆï¼ˆ<strong>é›¶Losså€¼</strong>ï¼‰<ul>
<li>æ³¨æ„å…³é—­æ­£åˆ™åŒ–</li>
</ul>
</li>
</ul>
<h4 id="è®­ç»ƒè¿‡ç¨‹ç›‘æµ‹"><a href="#è®­ç»ƒè¿‡ç¨‹ç›‘æµ‹" class="headerlink" title="è®­ç»ƒè¿‡ç¨‹ç›‘æµ‹"></a>è®­ç»ƒè¿‡ç¨‹ç›‘æµ‹</h4><ol>
<li>ä»£ä»·å‡½æ•°</li>
</ol>
<p><img src="/qnsource/images/2018-04-28-cs231n-notes/learningrates.jpeg" alt="learningrates"></p>
<ol>
<li>è®­ç»ƒ/éªŒè¯é›†å‡†ç¡®ç‡</li>
</ol>
<p><img src="/qnsource/images/2018-04-28-cs231n-notes/accuracies.jpeg" alt="accuracies"></p>
<ol>
<li>æ›´æ–°å‚æ•°æ¯”ä¾‹</li>
</ol>
<blockquote>
<p>A rough heuristic is that this ratio should be somewhere around 1e-3. If it is lower than this then the learning rate might be too low. If it is higher then the learning rate is likely too high.</p>
</blockquote>
<p>â€‹    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># assume parameter vector W and its gradient vector dW</span></span><br><span class="line">param_scale = np.linalg.norm(W.ravel())</span><br><span class="line">update = -learning_rate*dW <span class="comment"># simple SGD update</span></span><br><span class="line">update_scale = np.linalg.norm(update.ravel())</span><br><span class="line">W += update <span class="comment"># the actual update</span></span><br><span class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># want ~1e-3</span></span><br></pre></td></tr></table></figure>
<h4 id="è¶…å‚ä¼˜åŒ–"><a href="#è¶…å‚ä¼˜åŒ–" class="headerlink" title="è¶…å‚ä¼˜åŒ–"></a>è¶…å‚ä¼˜åŒ–</h4><ul>
<li><code>learning_rate = 10 ** uniform(-6, 1)</code></li>
<li>random serach &gt; grid search<img src="/qnsource/images/2018-04-28-cs231n-notes/gridsearchbad.jpeg" alt="gridsearchbad"></li>
</ul>
<h4 id="æ¨¡å‹ç»„åˆ"><a href="#æ¨¡å‹ç»„åˆ" class="headerlink" title="æ¨¡å‹ç»„åˆ"></a>æ¨¡å‹ç»„åˆ</h4><ul>
<li><strong>Same model, different initializations</strong>. Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization.</li>
<li><strong>Top models discovered during cross-validation</strong>. Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesnâ€™t require additional retraining of models after cross-validation</li>
<li><strong>Different checkpoints of a single model</strong>. If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap.</li>
<li><strong>Running average of parameters during training</strong>. Related to the last point, a cheap way of almost always getting an extra percent or two of performance is to maintain a second copy of the networkâ€™s weights in memory that maintains an exponentially decaying sum of previous weights during training. This way youâ€™re averaging the state of the network over last several iterations. You will find that this â€œsmoothedâ€ version of the weights over last few steps almost always achieves better validation error. The rough intuition to have in mind is that the objective is bowl-shaped and your network is jumping around the mode, so the average has a higher chance of being somewhere nearer the mode.</li>
</ul>
<h3 id="åå‘ä¼ æ’­"><a href="#åå‘ä¼ æ’­" class="headerlink" title="åå‘ä¼ æ’­"></a>åå‘ä¼ æ’­</h3><ol>
<li><strong>Cache forward pass variables</strong>. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them.</li>
<li><strong>Gradients add up at forks</strong>. The forward expression involves the variables <strong>x,y</strong> multiple times, so when we perform backpropagation we must be careful to use <code>+=</code> instead of <code>=</code> to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the <em>multivariable chain rule</em> in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.</li>
</ol>
<h3 id="å®ç°-lt-ä»£ç -gt"><a href="#å®ç°-lt-ä»£ç -gt" class="headerlink" title="å®ç° &lt;ä»£ç &gt;"></a>å®ç° &lt;ä»£ç &gt;</h3><blockquote>
<p>é…åˆAssignment 1ï¼š Implement a Neural Networkæ•´ç†ä¸€ä¸ªä»å¤´è®­ç»ƒ2å±‚å…¨è”é€šç½‘ç»œçš„ä¾‹å­ã€‚</p>
</blockquote>
<ol>
<li>ç½‘ç»œç»“æ„ï¼ˆåªæœ‰ä¸€ä¸ªéšå±‚çš„å…¨è”åŒç¥ç»ç½‘ç»œï¼‰,å…¶ä¸­è¾“å…¥è®­ç»ƒæ ·æœ¬ä¸ºX (N x D), ç¬¬ä¸€ä¸ªéšå±‚ç”±Hä¸ªèŠ‚ç‚¹ç»„æˆï¼Œè¾“å‡ºå±‚ç”±Cä¸ªèŠ‚ç‚¹ç»„æˆï¼š</li>
</ol>
<figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input  - fully connected layer - ReLU - fully connected layer - softmax</span><br><span class="line">ï¼ˆ<span class="keyword">N</span>,<span class="keyword">D</span>ï¼‰           H                             <span class="keyword">C</span></span><br></pre></td></tr></table></figure>
<p>å„å‚æ•°çš„å¤§å°å¦‚ä¸‹ï¼š</p>
<ul>
<li>input: X (N x D)</li>
<li>W1 (H x D) ; b1 (H,)</li>
<li>W2 (C x H);  b2 (C,)</li>
</ul>
<ol>
<li><p>å‚æ•°åˆå§‹åŒ–</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">H = <span class="number">100</span></span><br><span class="line">C = <span class="number">10</span></span><br><span class="line">N,D = X.shape</span><br><span class="line"></span><br><span class="line">W1 = std * np.random.randn(input_size, hidden_size)</span><br><span class="line">b1 = np.zeros(hidden_size)</span><br><span class="line">W2 = std * np.random.randn(hidden_size, output_size)</span><br><span class="line">b2 = np.zeros(output_size)</span><br></pre></td></tr></table></figure>
</li>
<li><p>å‰å‘ä¼ æ’­ï¼Œè®¡ç®—å¾—åˆ†å‡½æ•°å’Œä»£ä»·å‡½æ•°</p>
<script type="math/tex; mode=display">
hiddenLayer = ReLU(X*W1 + b1) \\
ReLU(x) = \max(0, x)</script><script type="math/tex; mode=display">
scores = hidden\_layer * W2 + b2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hidden_layer = np.maximum(<span class="number">0</span>, np.dot(X, W1) + b1)   <span class="comment"># N x H</span></span><br><span class="line">socres = np.dot(hidden_layer, W2) + b2     <span class="comment"># N x C</span></span><br></pre></td></tr></table></figure>
<p>ä»£ä»·å‡½æ•°ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼š</p>
<script type="math/tex; mode=display">
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">exp_scores = np.exp(scores)</span><br><span class="line">probs = exp_scores / np.sum(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">correct_logprobs = -np.log(probs[range(N), y])</span><br><span class="line">loss = np.sum(correct_logprobs) / N</span><br><span class="line">loss += <span class="number">0.5</span>*reg*(np.sum(W1*W1) + np.sum(W2*W2))</span><br></pre></td></tr></table></figure>
</li>
<li><p>åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦</p>
<p>æ¢¯åº¦çš„è®¡ç®—åˆ©ç”¨åå‘ä¼ æ’­ï¼Œä»åå¾€å‰é€æ­¥æ¨å¯¼ï¼š</p>
<p>é¦–å…ˆè®¡ç®—<code>dscores</code></p>
<script type="math/tex; mode=display">
\begin{equation}  
\frac{\partial L}{\partial scores} =
\left\{  
\begin{array}{lr}  
-1 + \frac{e^{\hat f_{y_i}}}{\sum_j e^{f_j}}, &  j=i\\  
\frac{e^{\hat f_{y_i}}}{\sum_j e^{f_j}}, &   j \neq i 
\end{array}  
\right.  
\end{equation}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dscores = probs</span><br><span class="line">dscores[range(N), y] -=<span class="number">1</span></span><br><span class="line">dscores /= N</span><br></pre></td></tr></table></figure>
</li>
</ol>
<script type="math/tex; mode=display">
dW2 = H^T*dscores + 2\lambda*W2 \\
db2 = dscores</script>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dW2 = np.dot(h1.T, dscores) + <span class="number">2</span>*reg * W2</span><br><span class="line">db2 = np.sum(dscores, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>   æ¥ç€è®¡ç®—éšå±‚çš„åå¯¼,å…¶ä¸­æ¿€æ´»å‡½æ•°ç”±äºä½¿ç”¨ReLUï¼Œåœ¨$x&lt;0$æ—¶ï¼Œå¯¼æ•°ä¸º0</p>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dh1 = np.dot(dscores, W2.T)</span><br><span class="line">dh1[h1 &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">dW1 = np.dot(X.T, dh1) + <span class="number">2</span>*reg * W1</span><br><span class="line">db1 = np.sum(dh1, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="å…¶å®ƒ"><a href="#å…¶å®ƒ" class="headerlink" title="å…¶å®ƒ"></a>å…¶å®ƒ</h2><ul>
<li>å­¦ä¹ ç‡çš„è®¾è®¡ä¸€èˆ¬åœ¨<code>[1e-3, 1e-5]</code>,<code>10**uniform(-3,-6)</code></li>
</ul>
<h2 id="å‚è€ƒ"><a href="#å‚è€ƒ" class="headerlink" title="å‚è€ƒ"></a>å‚è€ƒ</h2><blockquote id="fn_1">
<sup>1</sup>. <a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="noopener">cs231n notes: Neural Network</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. <a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener">cs231n notes: Setting up the Data and Loss</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. <a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">cs231n notes: Learning and Evaluation</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>

      
    </div>
    <footer class="article-footer">

      
        <div id="donation_div"></div>

<script src="/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: 'å¦‚æœè§‰å¾—æˆ‘çš„æ–‡ç« å¯¹æ‚¨æœ‰ç”¨ï¼Œè¯·éšæ„æ‰“èµã€‚æ‚¨çš„æ”¯æŒå°†é¼“åŠ±æˆ‘ç»§ç»­åˆ›ä½œ!', // å¯é€‰å‚æ•°ï¼Œæ‰“èµæ ‡é¢˜
  btnText: 'æ‰“èµæ”¯æŒ', // å¯é€‰å‚æ•°ï¼Œæ‰“èµæŒ‰é’®æ–‡å­—
  el: document.getElementById('donation_div'),
  wechatImage: 'http://p4ygzcmtw.bkt.clouddn.com/site/weixin.png',
  alipayImage: 'http://p4ygzcmtw.bkt.clouddn.com/site/alipay.jpg'
});
</script>
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>æœ¬æ–‡ä½œè€…:  </strong>Ebby DD</a>
          </li>
          <li class="post-copyright-link">
          <strong>æœ¬æ–‡é“¾æ¥:  </strong>
          <a href="/2018/05/03/cs231n-lecture-4/" target="_blank" title="cs231nè¯¾ç¨‹ç¬”è®°ï¼šï¼ˆLecture 4ï¼‰ç¥ç»ç½‘ç»œåŸºç¡€">http://blog.a-stack.com/2018/05/03/cs231n-lecture-4/</a>
          </li>
          <li class="post-copyright-license">
            <strong>ç‰ˆæƒå£°æ˜:   </strong>
            æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„
          </li>
         
        </ul>
<div>

      

      
      <!--  -->
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/äººå·¥æ™ºèƒ½/">äººå·¥æ™ºèƒ½</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/æœºå™¨è§†è§‰/">æœºå™¨è§†è§‰</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ç¬”è®°/">ç¬”è®°</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/05/03/cs231n-lecture-5/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">ä¸Šä¸€ç¯‡</strong>
      <div class="article-nav-title">
        
          cs231nè¯¾ç¨‹ç¬”è®°:ï¼ˆLecture 5ï¼‰CNNåŸºç¡€
        
      </div>
    </a>
  
  
    <a href="/2018/05/03/cs231n-lecture-3/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">ä¸‹ä¸€ç¯‡</strong>
      <div class="article-nav-title">cs231nè¯¾ç¨‹ç¬”è®°:ï¼ˆLecture 3ï¼‰çº¿æ€§åˆ†ç±»å™¨å’Œä¼˜åŒ–æ–¹æ³•</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">æ–‡ç« ç›®å½•</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#æ¦‚è¿°"><span class="nav-number">1.</span> <span class="nav-text">æ¦‚è¿°</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ç¥ç»ç½‘ç»œ"><span class="nav-number">2.</span> <span class="nav-text">ç¥ç»ç½‘ç»œ</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#æ•°æ®é¢„å¤„ç†"><span class="nav-number">2.1.</span> <span class="nav-text">æ•°æ®é¢„å¤„ç†</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#å‚æ•°åˆå§‹åŒ–"><span class="nav-number">2.2.</span> <span class="nav-text">å‚æ•°åˆå§‹åŒ–</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#æ­£åˆ™åŒ–"><span class="nav-number">2.3.</span> <span class="nav-text">æ­£åˆ™åŒ–</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ä»£ä»·å‡½æ•°ï¼ˆdata-lossï¼‰"><span class="nav-number">2.4.</span> <span class="nav-text">ä»£ä»·å‡½æ•°ï¼ˆdata lossï¼‰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#è®­ç»ƒ3"><span class="nav-number">2.5.</span> <span class="nav-text">è®­ç»ƒ3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#æ¢¯åº¦æ ¡éªŒ"><span class="nav-number">2.5.1.</span> <span class="nav-text">æ¢¯åº¦æ ¡éªŒ</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sanity-Checks"><span class="nav-number">2.5.2.</span> <span class="nav-text">Sanity Checks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#è®­ç»ƒè¿‡ç¨‹ç›‘æµ‹"><span class="nav-number">2.5.3.</span> <span class="nav-text">è®­ç»ƒè¿‡ç¨‹ç›‘æµ‹</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#è¶…å‚ä¼˜åŒ–"><span class="nav-number">2.5.4.</span> <span class="nav-text">è¶…å‚ä¼˜åŒ–</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#æ¨¡å‹ç»„åˆ"><span class="nav-number">2.5.5.</span> <span class="nav-text">æ¨¡å‹ç»„åˆ</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#åå‘ä¼ æ’­"><span class="nav-number">2.6.</span> <span class="nav-text">åå‘ä¼ æ’­</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#å®ç°-lt-ä»£ç -gt"><span class="nav-number">2.7.</span> <span class="nav-text">å®ç° &lt;ä»£ç &gt;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#å…¶å®ƒ"><span class="nav-number">3.</span> <span class="nav-text">å…¶å®ƒ</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#å‚è€ƒ"><span class="nav-number">4.</span> <span class="nav-text">å‚è€ƒ</span></a></li></ol>
    
    </div>
  </aside>

</section>      
        
      </div>
      <!--  -->
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2017 - 2018 Ebby&#39;s Notes All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				è®¿å®¢æ•° : <span id="busuanzi_value_site_uv"></span> |  
				è®¿é—®é‡ : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>    
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/reading" class="mobile-nav-link">Reading</a>
  
    <a href="/resources" class="mobile-nav-link">Resources</a>
  
    <a href="/notebooks" class="mobile-nav-link">ğŸ“</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/scripts.js"></script>




  <script src="/js/dialog.js"></script>









	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>



	<script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?{{ theme.baidu_analytics }}";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">è®¾ç½®</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              æ­£æ–‡å­—å·å¤§å°
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            æ‚¨å·²è°ƒæ•´é¡µé¢å­—ä½“å¤§å°
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              å¤œé—´æŠ¤çœ¼æ¨¡å¼
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            å¤œé—´æ¨¡å¼å·²ç»å¼€å¯ï¼Œå†æ¬¡å•å‡»æŒ‰é’®å³å¯å…³é—­ 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;å…³ äº&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Ebby&#39;s Notes
          </div>
          <div class="panel-body">
            Copyright Â© 2018 Ebby DD All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
</body>
</html>