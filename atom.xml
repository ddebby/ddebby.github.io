<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ebby&#39;s Notes</title>
  
  <subtitle>=Blog for AI Learning=</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.a-stack.com/"/>
  <updated>2019-05-07T09:49:54.672Z</updated>
  <id>http://blog.a-stack.com/</id>
  
  <author>
    <name>Ebby DD</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pathlib--Python的路径管理库</title>
    <link href="http://blog.a-stack.com/2019/05/07/Pathlib-Python%E7%9A%84%E8%B7%AF%E5%BE%84%E7%AE%A1%E7%90%86%E5%BA%93/"/>
    <id>http://blog.a-stack.com/2019/05/07/Pathlib-Python的路径管理库/</id>
    <published>2019-05-07T08:53:05.000Z</published>
    <updated>2019-05-07T09:49:54.672Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/16.jpg" alt="Pathlib使用手册"></p><p><strong>摘要：</strong> 整理了Pathlib的使用方法，便于速查。</p><a id="more"></a><h2 id="Pathlib使用方法归纳"><a href="#Pathlib使用方法归纳" class="headerlink" title="Pathlib使用方法归纳"></a>Pathlib使用方法归纳</h2><h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p1 = Path()</span><br><span class="line">print(p1)</span><br></pre></td></tr></table></figure><pre><code>.</code></pre><h4 id="路径"><a href="#路径" class="headerlink" title="路径"></a>路径</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p1.cwd()</span><br></pre></td></tr></table></figure><pre><code>PosixPath(&#39;/data/home/gaoc/data/1.fast.ai/nbs/dl1&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回一个新的路径，这个新路径就是当前Path对象的绝对路径</span></span><br><span class="line">p1.resolve()</span><br></pre></td></tr></table></figure><pre><code>PosixPath(&#39;/data/home/gaoc/data/1.fast.ai/nbs/dl1&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以获取绝对路径，但是推荐使用resolve()</span></span><br><span class="line">p1.absolute()</span><br></pre></td></tr></table></figure><pre><code>PosixPath(&#39;/data/home/gaoc/data/1.fast.ai/nbs/dl1&#39;)</code></pre><h4 id="获取文件名，文件后缀，文件目录"><a href="#获取文件名，文件后缀，文件目录" class="headerlink" title="获取文件名，文件后缀，文件目录"></a>获取文件名，文件后缀，文件目录</h4><ul><li>name: 目录的最后一个部分</li><li>suffix:目录中最后一个部分的扩展名</li><li>stem：目录最后一个部分，没有后缀</li><li>suffixes：返回多个扩展名列表</li><li>with_suffix(suffix)：补充扩展名到尾部，扩展名存在无效</li><li>with_name(name)：替换目录最后一个部分并返回一个新的路径</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p2 = Path(<span class="string">'/data/home/gaoc/data/99.test/Untitled.ipynb.test'</span>)</span><br><span class="line">p2.name</span><br></pre></td></tr></table></figure><pre><code>&#39;Untitled.ipynb&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p2.stem</span><br></pre></td></tr></table></figure><pre><code>&#39;Untitled&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p2.suffix</span><br></pre></td></tr></table></figure><pre><code>&#39;.ipynb&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p2.suffixes</span><br></pre></td></tr></table></figure><pre><code>[&#39;.ipynb&#39;, &#39;.test&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p2.parent</span><br></pre></td></tr></table></figure><pre><code>PosixPath(&#39;/data/home/gaoc/data/99.test&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回一个iterable, 包含所有父目录</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> p2.parents:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><pre><code>/data/home/gaoc/data/99.test/data/home/gaoc/data/data/home/gaoc/data/home/data/</code></pre><h3 id="拼接、检查与分解"><a href="#拼接、检查与分解" class="headerlink" title="拼接、检查与分解"></a>拼接、检查与分解</h3><p>操作符：/<br>Path对象 / Path对象</p><p>Path对象 / 字符串 或者 字符串 / Path对象</p><p>分解：<br>parts 属性, 可以返回路径中的每一个部分</p><p>joinpath：</p><p>joinpath(*other) 连接多个字符串到Path对象中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p3 = Path(p2.parent, <span class="string">'test.txt'</span>)</span><br><span class="line">p3</span><br></pre></td></tr></table></figure><pre><code>PosixPath(&#39;/data/home/gaoc/data/99.test/test.txt&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p3 = p2.parent / <span class="string">'test2.txt'</span></span><br><span class="line">p3</span><br></pre></td></tr></table></figure><pre><code>PosixPath(&#39;/data/home/gaoc/data/99.test/test2.txt&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p3 = p2.parent.joinpath(<span class="string">'etc'</span>, <span class="string">'init.d'</span>, Path(<span class="string">'abc'</span>))</span><br><span class="line">p3</span><br></pre></td></tr></table></figure><pre><code>PosixPath(&#39;/data/home/gaoc/data/99.test/etc/init.d/abc&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p3.exists()</span><br></pre></td></tr></table></figure><pre><code>False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p3.is_file()</span><br></pre></td></tr></table></figure><pre><code>False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p3.is_dir()</span><br></pre></td></tr></table></figure><pre><code>False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将路径通过分隔符分割成一个元组</span></span><br><span class="line">p2.parts</span><br></pre></td></tr></table></figure><pre><code>(&#39;/&#39;, &#39;data&#39;, &#39;home&#39;, &#39;gaoc&#39;, &#39;data&#39;, &#39;99.test&#39;, &#39;Untitled.ipynb.test&#39;)</code></pre><h3 id="遍历文件夹"><a href="#遍历文件夹" class="headerlink" title="遍历文件夹"></a>遍历文件夹</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p4 = p2.parent</span><br><span class="line">p4.iterdir()</span><br></pre></td></tr></table></figure><pre><code>&lt;generator object Path.iterdir at 0x7f6dfc0fbeb8&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相当于os.listdir</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> p4.iterdir():</span><br><span class="line">    print(file)</span><br></pre></td></tr></table></figure><pre><code>/data/home/gaoc/data/99.test/.ipynb_checkpoints/data/home/gaoc/data/99.test/data-science/data/home/gaoc/data/99.test/Untitled.ipynb</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(p4.iterdir())</span><br></pre></td></tr></table></figure><pre><code>[PosixPath(&#39;/data/home/gaoc/data/99.test/.ipynb_checkpoints&#39;), PosixPath(&#39;/data/home/gaoc/data/99.test/test&#39;), PosixPath(&#39;/data/home/gaoc/data/99.test/data-science&#39;), PosixPath(&#39;/data/home/gaoc/data/99.test/Untitled.ipynb&#39;)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相当于os.listdir, 但是可以添加匹配条件</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> p4.glob(<span class="string">"*.ipynb"</span>):</span><br><span class="line">    print(file)</span><br></pre></td></tr></table></figure><pre><code>/data/home/gaoc/data/99.test/Untitled.ipynb</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(p4.glob(<span class="string">"*.ipynb"</span>))</span><br></pre></td></tr></table></figure><pre><code>[PosixPath(&#39;/data/home/gaoc/data/99.test/Untitled.ipynb&#39;)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相当于os.walk, 也可以添加匹配条件</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> p4.rglob(<span class="string">"*.ipynb"</span>):</span><br><span class="line">    print(file)</span><br></pre></td></tr></table></figure><pre><code>/data/home/gaoc/data/99.test/Untitled.ipynb/data/home/gaoc/data/99.test/.ipynb_checkpoints/Untitled-checkpoint.ipynb</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(p4.rglob(<span class="string">"*.ipynb"</span>))</span><br></pre></td></tr></table></figure><pre><code>[PosixPath(&#39;/data/home/gaoc/data/99.test/Untitled.ipynb&#39;), PosixPath(&#39;/data/home/gaoc/data/99.test/.ipynb_checkpoints/Untitled-checkpoint.ipynb&#39;)]</code></pre><h3 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p5 = Path(p4,<span class="string">'test'</span>)</span><br><span class="line">p5.mkdir(exist_ok=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p5</span><br></pre></td></tr></table></figure><pre><code>PosixPath(&#39;/data/home/gaoc/data/99.test/test&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p5.mkdir((exist_ok=<span class="keyword">True</span>, parents=<span class="keyword">True</span>) <span class="comment"># 递归创建文件目录</span></span><br></pre></td></tr></table></figure><h3 id="查看文件信息"><a href="#查看文件信息" class="headerlink" title="查看文件信息"></a>查看文件信息</h3><h4 id="获取详细信息"><a href="#获取详细信息" class="headerlink" title="获取详细信息"></a>获取详细信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p6 = Path(<span class="string">'/data/home/gaoc/data/99.test/Untitled.ipynb'</span>)</span><br><span class="line">p6.stat()</span><br></pre></td></tr></table></figure><pre><code>os.stat_result(st_mode=33204, st_ino=57147394, st_dev=2096, st_nlink=1, st_uid=1003, st_gid=1003, st_size=636, st_atime=1555927128, st_mtime=1555926338, st_ctime=1555926338)</code></pre><h4 id="文件大小"><a href="#文件大小" class="headerlink" title="文件大小"></a>文件大小</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p6.stat().st_size</span><br></pre></td></tr></table></figure><pre><code>636</code></pre><h4 id="创建时间"><a href="#创建时间" class="headerlink" title="创建时间"></a>创建时间</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p6.stat().st_atime</span><br></pre></td></tr></table></figure><pre><code>1555927128.4478571</code></pre><h4 id="修改时间"><a href="#修改时间" class="headerlink" title="修改时间"></a>修改时间</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p6.stat().st_mtime</span><br></pre></td></tr></table></figure><pre><code>1555926338.8676176</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.jianshu.com/p/a820038e65c3" target="_blank" rel="noopener">https://www.jianshu.com/p/a820038e65c3</a></li><li><a href="https://www.cnblogs.com/hkcs/p/7773484.html" target="_blank" rel="noopener">https://www.cnblogs.com/hkcs/p/7773484.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/16.jpg&quot; alt=&quot;Pathlib使用手册&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 整理了Pathlib的使用方法，便于速查。&lt;/p&gt;
    
    </summary>
    
      <category term="工具" scheme="http://blog.a-stack.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://blog.a-stack.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>从头训练一个图像分类器</title>
    <link href="http://blog.a-stack.com/2019/02/01/%E4%BB%8E%E5%A4%B4%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <id>http://blog.a-stack.com/2019/02/01/从头训练一个图像分类器/</id>
    <published>2019-02-01T02:16:46.000Z</published>
    <updated>2019-02-01T07:06:54.719Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/08.jpg" alt="Test Picture"></p><p><strong>摘要：</strong> 本文记录了利用fast.ai快速构建一个图像识别（图像分类器）的方法。</p><a id="more"></a><p>Fast.ai是一个基于Pytorch的深度学习计算框架，就像keras基于tensorflow，目的在于简化深度学习网络设计、训练过程中的复杂逻辑，简化设计时间。2019年第三个版的课程配合pytorch 1.0版本进行了全面更新，相关链接见文末参考。</p><h2 id="基本的环境配置"><a href="#基本的环境配置" class="headerlink" title="基本的环境配置"></a>基本的环境配置</h2><p>我是使用一台Azure的GPU云主机进行的实验，Azure的DSVM云主机提供了扩展包来迅速部署fast.ai所需的相关库，也可以参见 <a href="https://course.fast.ai/start_azure.html" target="_blank" rel="noopener">https://course.fast.ai/start_azure.html</a> 进行环境安装。安装的bash脚本如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#安装相关基本软件包</span></span></span><br><span class="line">/anaconda/bin/conda create -y -n fastai python=3.6</span><br><span class="line">source /anaconda/bin/activate fastai</span><br><span class="line">pip install dataclasses</span><br><span class="line">/anaconda/bin/conda install  -y -c pytorch pytorch torchvision</span><br><span class="line">/anaconda/bin/conda install  -y -c fastai fastai</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 在JupyterNotebook中增加fast.ai的kernel</span></span></span><br><span class="line">/anaconda/bin/conda install  -y ipykernel</span><br><span class="line">python -m ipykernel install --name 'fastai' --display-name 'Python (fastai)'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Script to update Notbook metadata</span></span><br><span class="line">cat &lt;&lt; EOF &gt; /tmp/changenbmeta.py</span><br><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/python</span></span><br><span class="line">import json,sys</span><br><span class="line"></span><br><span class="line">if len(sys.argv) &lt; 2 :</span><br><span class="line">        print "Usage: python changenbmeta.py &lt;filename&gt;"</span><br><span class="line">        exit()</span><br><span class="line">with open(sys.argv[1], "r") as jsonFile:</span><br><span class="line">    data = json.load(jsonFile)</span><br><span class="line"></span><br><span class="line">data["metadata"]["kernelspec"]["display_name"] = "Python (fastai)"</span><br><span class="line">data["metadata"]["kernelspec"]["name"] = "fastai"</span><br><span class="line"></span><br><span class="line">with open(sys.argv[1], "w") as jsonFile:</span><br><span class="line">    json.dump(data, jsonFile)</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 下载课程</span></span></span><br><span class="line">cd ~/home/courses</span><br><span class="line">git clone https://github.com/fastai/course-v3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Change metadata on notebook to match kernel name <span class="keyword">in</span> the fast.ai notebooks</span></span><br><span class="line">find . -name \*.ipynb -exec /usr/bin/python /tmp/changenbmeta.py &#123;&#125; \;</span><br></pre></td></tr></table></figure><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>Fast.ai课程给出了一种可以利用Google Image快速收集图片的流程，值得借鉴。在本例中，我想了半天该识别点啥呢？ 做个自行车、电瓶车、摩托车的识别分类器吧！</p><h3 id="准备图片"><a href="#准备图片" class="headerlink" title="准备图片"></a>准备图片</h3><ol><li><p>打开<a href="https://images.google.com/?gws_rd=ssl" target="_blank" rel="noopener">Google Image</a>,搜索并浏览需要的图片，往下滚动，直到有差不多数目的图像被加载（搜索越多，无关照片越多，尽量控制搜索条件，保证搜索质量）；</p></li><li><p>使用命令<code>Ctrl+Shift+J</code>打开控制台，填入如下命令并敲回车，将所有图片保持为<code>urls_xxx.txt</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">urls = <span class="built_in">Array</span>.from(<span class="built_in">document</span>.querySelectorAll(<span class="string">'.rg_di .rg_meta'</span>)).map(<span class="function"><span class="params">el</span>=&gt;</span><span class="built_in">JSON</span>.parse(el.textContent).ou);</span><br><span class="line"><span class="built_in">window</span>.open(<span class="string">'data:text/csv;charset=utf-8,'</span> + <span class="built_in">escape</span>(urls.join(<span class="string">'\n'</span>)));</span><br></pre></td></tr></table></figure></li><li><p>准备图片存储目录并下载图片，对于每个类别（注意改变类别名）分别使用如下命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai.vision <span class="keyword">import</span> *</span><br><span class="line">path = Path(<span class="string">'/home/gaoc/data/1.Courses/3.fastai/data/bicycles'</span>)</span><br><span class="line"></span><br><span class="line">file1 = <span class="string">'urls_bike.txt'</span></span><br><span class="line"></span><br><span class="line">dest1 = path/<span class="string">'bike'</span></span><br><span class="line">dest1.mkdir(parents=<span class="keyword">True</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line">dest2 = path/<span class="string">'battery'</span></span><br><span class="line">dest2.mkdir(parents=<span class="keyword">True</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line">dest3 = path/<span class="string">'motorcycle'</span></span><br><span class="line">dest3.mkdir(parents=<span class="keyword">True</span>, exist_ok=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></li><li><p>下载图片</p><p>将存储的<code>urls_xxx.txt</code>文件放于<code>path</code>目录下，执行如下命令：</p><blockquote><p>其中使用了fast.ai.vision.data的<a href="https://docs.fast.ai/vision.data.html#download_images" target="_blank" rel="noopener"><code>download_images</code></a>,指定url文件的位置，存放图片的目的文件夹，最大的图片数目，最大处理线程数目</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">download_images(path/file1, dest1, max_pics=<span class="number">200</span>)</span><br><span class="line">download_images(path/file2, dest2, max_pics=<span class="number">200</span>)</span><br><span class="line">download_images(path/file3, dest3, max_pics=<span class="number">200</span>)</span><br></pre></td></tr></table></figure></li></ol><h3 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h3><ol><li><p>下载下来的图片，格式（jpeg、png、gift等）、大小都不一致，需要进行一波清理，fast.ai提供了一个<code>verify_images</code>的工具，可以对图像进行基本的清理操作：</p><blockquote><p><a href="https://docs.fast.ai/vision.data.html#verify_images" target="_blank" rel="noopener">verify_images</a> 是一个很有用的函数，会查看该图片是否损坏、是否使用合适的channel数目，是否需要调整到指定大小或超过了限定大小</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">classes = [<span class="string">'bike'</span>,<span class="string">'battery'</span>,<span class="string">'motorcycle'</span>]</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> classes:</span><br><span class="line">    print(c)</span><br><span class="line">    verify_images(path/c, delete=<span class="keyword">True</span>, max_size=<span class="number">500</span>)</span><br></pre></td></tr></table></figure></li><li><p>虽然图片格式清理完毕， 但由于搜索引擎本身搜索质量的问题，还存在很多归类错误的照片，需要手动调整，fast.ai 提供了一个<code>fastai.widgets</code>的<code>ImageCleaner</code>工具，可以在JupyterNotebook中交互式的进行图片清理。一般而言，为了降低工作量都是从错分的数据中进行数据清理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai.widgets <span class="keyword">import</span> *</span><br><span class="line">ds, idxs = DatasetFormatter().from_toplosses(learn, ds_type=DatasetType.Valid)</span><br><span class="line">ImageCleaner(ds, idxs,path=path)</span><br></pre></td></tr></table></figure></li></ol><p><img src="/2019/02/01/从头训练一个图像分类器/check data.png" alt="check data"></p><p>查找并删除重复图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ds, idxs = DatasetFormatter().from_similars(learn, ds_type=DatasetType.Valid)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/01/从头训练一个图像分类器/similar data.png" alt="similar data"></p><h3 id="查看数据"><a href="#查看数据" class="headerlink" title="查看数据"></a>查看数据</h3><p>fast.ai通过类<code>ImageDataBunch</code>进行数据的组织，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">data = ImageDataBunch.from_folder(path, train=<span class="string">"."</span>, valid_pct=<span class="number">0.2</span>,</span><br><span class="line">        ds_tfms=get_transforms(), size=<span class="number">224</span>, num_workers=<span class="number">4</span>).normalize(imagenet_stats)</span><br></pre></td></tr></table></figure><p>如果从清理过的图像中导入数据，执行如下命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">data = ImageDataBunch.from_csv(<span class="string">"."</span>, folder=<span class="string">"."</span>, valid_pct=<span class="number">0.2</span>, csv_labels=path/<span class="string">'cleaned.csv'</span>,</span><br><span class="line">        ds_tfms=get_transforms(), size=<span class="number">224</span>, num_workers=<span class="number">4</span>).normalize(imagenet_stats)</span><br></pre></td></tr></table></figure><p>可以通过如下命令，查看相关类别情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.classes, data.c, len(data.train_ds), len(data.valid_ds)</span><br></pre></td></tr></table></figure><p>可以随机查看一组数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.show_batch(rows=<span class="number">3</span>, figsize=(<span class="number">7</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure><p><img src="/2019/02/01/从头训练一个图像分类器/data_show.png" alt="data_show"></p><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><ol><li><p>使用预训练模型resnet34进行迁移学习：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learn = create_cnn(data,arch=models.resnet34, metrics=error_rate)</span><br><span class="line">learn.fit_one_cycle(<span class="number">4</span>)</span><br><span class="line">learn.save(<span class="string">'stage-1'</span>)</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Total time: 00:11</span><br><span class="line">epochtrain_lossvalid_losserror_rate</span><br><span class="line">11.0837250.6011550.250000</span><br><span class="line">20.6813580.6146700.220000</span><br><span class="line">30.5183870.5018600.110000</span><br><span class="line">40.4154230.4752730.100000</span><br></pre></td></tr></table></figure></li><li><p>解冻模型，修改学习率，再训练几轮：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learn.unfreeze()</span><br><span class="line">learn.lr_find()</span><br><span class="line">learn.recorder.plot()</span><br></pre></td></tr></table></figure><p><img src="/2019/02/01/从头训练一个图像分类器/lr_find.png" alt="lr_find"></p></li><li><p>调整学习率继续学习</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.fit_one_cycle(<span class="number">2</span>,max_lr=slice(<span class="number">1e-4</span>,<span class="number">3e-4</span>))</span><br><span class="line">learn.save(<span class="string">'stage-2'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>这个步骤中学习率的控制很重要，后续更新</p></blockquote><p>结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Total time: <span class="number">00</span>:<span class="number">05</span></span><br><span class="line">epochtrain_lossvalid_losserror_rate</span><br><span class="line"><span class="number">1</span><span class="number">0.139484</span><span class="number">0.459167</span><span class="number">0.120000</span></span><br><span class="line"><span class="number">2</span><span class="number">0.118735</span><span class="number">0.434948</span><span class="number">0.080000</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="模型结果分析"><a href="#模型结果分析" class="headerlink" title="模型结果分析"></a>模型结果分析</h2><p>fast.ai提供了<code>ClassificationInterpretation</code>能够从结果中高效的进行分析</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">interp = ClassificationInterpretation.from_learner(learn)</span><br><span class="line">interp.plot_confusion_matrix()</span><br></pre></td></tr></table></figure><p><img src="/2019/02/01/从头训练一个图像分类器/confusion_matrix.png" alt="confusion_matrix"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp.plot_top_losses(<span class="number">9</span>)</span><br></pre></td></tr></table></figure><p><img src="/2019/02/01/从头训练一个图像分类器/top losses.png" alt="top losses"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">interp.most_confused(<span class="number">1</span>)</span><br><span class="line">&gt; [(<span class="string">'battery'</span>, <span class="string">'motorcycle'</span>, <span class="number">4</span>), (<span class="string">'battery'</span>, <span class="string">'bike'</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure><p>可以看处模型容易搞混电动车和摩托车。</p><h2 id="Next"><a href="#Next" class="headerlink" title="Next"></a>Next</h2><p>手动修正数据之后，再重复训练，错误率会降低至1%</p><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 执行如下命令，生成1个84M的export.pkl文件</span></span><br><span class="line">learn.export()</span><br><span class="line">learn = load_learner(path)</span><br><span class="line">pred_class,pred_idx,outputs = learn.predict(img)</span><br></pre></td></tr></table></figure><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><h3 id="关于Google-Image的图像下载"><a href="#关于Google-Image的图像下载" class="headerlink" title="关于Google Image的图像下载"></a>关于Google Image的图像下载</h3><p>GitHub项目 <a href="https://github.com/toffebjorkskog/ml-tools/blob/master/gi2ds.md" target="_blank" rel="noopener"><strong>ml-tools</strong></a> 提供了一种更加便捷的下载方案, 可以在搜索的时候选择是否存储相关照片，只需要把如下代码粘贴在控制台中：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">(<span class="function"><span class="keyword">function</span>(<span class="params">e, s</span>) </span>&#123;</span><br><span class="line">    e.src = s;</span><br><span class="line">    e.onload = <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        jQuery.noConflict();</span><br><span class="line">        jQuery(<span class="string">'&lt;style type="text/css"&gt; .remove &#123; opacity:0.3;&#125;\n .urlmodal &#123;padding: 10px; background-color: #eee; position: fixed; bottom: 0; right: 0; height: 100px; width: 300px; z-index: 1000;&#125; .urlmodal textarea &#123;width: 100%; height: 250px;&#125;&lt;/style&gt;'</span>).appendTo(<span class="string">'head'</span>);</span><br><span class="line">        jQuery(<span class="string">'&lt;div class="urlmodal"&gt;&lt;h3&gt;Let\'s create a dataset&lt;/h3&gt;&lt;textarea&gt;Scoll all the way down\nClick "Show more images"\nScroll more\nClick on the images you want to remove from the dataset\nThe urls will appear in this box for you to copy.&lt;/textarea&gt;&lt;/div&gt;'</span>).appendTo(<span class="string">'body'</span>);</span><br><span class="line"></span><br><span class="line">        jQuery(<span class="string">'#rg'</span>).on(<span class="string">'click'</span>, <span class="string">'.rg_di'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">            jQuery(<span class="keyword">this</span>).toggleClass(<span class="string">'remove'</span>);</span><br><span class="line">            updateUrls();</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;);</span><br><span class="line">        jQuery(<span class="built_in">window</span>).scroll(updateUrls);</span><br><span class="line">        jQuery(<span class="string">'.urlmodal textarea'</span>).focus(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;updateUrls(); setTimeout(selectText, <span class="number">100</span>)&#125;).mouseup(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;<span class="keyword">return</span> <span class="literal">false</span>;&#125;);</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">function</span> <span class="title">updateUrls</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">            <span class="keyword">var</span> urls = <span class="built_in">Array</span>.from(<span class="built_in">document</span>.querySelectorAll(<span class="string">'.rg_di:not(.remove) .rg_meta'</span>)).map(<span class="function"><span class="params">el</span>=&gt;</span><span class="built_in">JSON</span>.parse(el.textContent).ou);</span><br><span class="line">            <span class="keyword">var</span> search_term = jQuery(<span class="string">'.gsfi'</span>).val();</span><br><span class="line">            jQuery(<span class="string">'.urlmodal textarea'</span>).val(urls.join(<span class="string">"\n"</span>));</span><br><span class="line">            jQuery(<span class="string">'.urlmodal h3'</span>).html(search_term + <span class="string">": "</span> + urls.length);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">function</span> <span class="title">selectText</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">            jQuery(<span class="string">'.urlmodal textarea'</span>).select();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">document</span>.head.appendChild(e);</span><br><span class="line">&#125;)(<span class="built_in">document</span>.createElement(<span class="string">'script'</span>), <span class="string">'//code.jquery.com/jquery-latest.min.js'</span>);</span><br></pre></td></tr></table></figure><h3 id="数据清理工具-fastclass"><a href="#数据清理工具-fastclass" class="headerlink" title="数据清理工具 fastclass"></a>数据清理工具 fastclass</h3><blockquote><p><a href="https://github.com/cwerner/fastclass" target="_blank" rel="noopener">https://github.com/cwerner/fastclass</a></p><p><a href="https://www.christianwerner.net/tech/Build-your-image-dataset-faster/#" target="_blank" rel="noopener">https://www.christianwerner.net/tech/Build-your-image-dataset-faster/#</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install git+https://github.com/cwerner/fastclass.git<span class="comment">#egg=fastclass</span></span><br></pre></td></tr></table></figure><p>使用方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. 创建一个csv文件用于声明要下载的图片的关键字，比如：</span><br><span class="line">head -n 3 example/guitars.csv</span><br><span class="line">searchterm,exclude</span><br><span class="line">guitar gibson les paul,guitar</span><br><span class="line">guitar gibson SG,guitar</span><br><span class="line"></span><br><span class="line">2. 下载（ALL=Google+Bing+Baidu）</span><br><span class="line">fcd -c ALL -k -o guitars example/guitars.csv </span><br><span class="line"></span><br><span class="line">3. 清理（使用[1]-[9]分类，D删除，X退出）</span><br><span class="line">fcc guitars/gibson_les_paul</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://course.fast.ai/" target="_blank" rel="noopener">Fast.ai课程v3</a></li><li><a href="https://docs.fast.ai/" target="_blank" rel="noopener">Fast.ai文档</a></li><li><a href="https://40.73.66.75:8000/user/gaoc/notebooks/1.Courses/3.fastai/course-v3/nbs/dl-ebby/lesson2-download.ipynb" target="_blank" rel="noopener">JupyterNotebook</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/08.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 本文记录了利用fast.ai快速构建一个图像识别（图像分类器）的方法。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://blog.a-stack.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="Fast.ai" scheme="http://blog.a-stack.com/tags/Fast-ai/"/>
    
  </entry>
  
  <entry>
    <title>2018年总结：来自18年的工作感悟</title>
    <link href="http://blog.a-stack.com/2019/01/24/2018%E5%B9%B4%E6%80%BB%E7%BB%93%EF%BC%9A%E6%9D%A5%E8%87%AA18%E5%B9%B4%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%84%9F%E6%82%9F/"/>
    <id>http://blog.a-stack.com/2019/01/24/2018年总结：来自18年的工作感悟/</id>
    <published>2019-01-24T06:50:22.000Z</published>
    <updated>2019-01-24T07:12:33.424Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/08.jpg" alt="年度总结报告"></p><p><strong>摘要：</strong> 2018年在从事人工智能工作过程中，有着诸多的感悟，记录下来激励自己2019年不要放松要求。</p><a id="more"></a><blockquote><p>这是我在2019年年度工作会议上代表人工智能团队的讲话内容，总结了18年团队工作过程中的几点感悟。</p></blockquote><h2 id="安而不忘危，存而不忘亡，治而不忘乱"><a href="#安而不忘危，存而不忘亡，治而不忘乱" class="headerlink" title="安而不忘危，存而不忘亡，治而不忘乱"></a>安而不忘危，存而不忘亡，治而不忘乱</h2><p>第一个感悟，我引用了《周易》中的一段话：</p><blockquote><p>安而不忘危，存而不忘亡，治而不忘乱。——《周易》       </p><p> 原话内容是：“安而不忘危、存而不忘亡、治而不忘乱、富而不忘贫、乐而不忘忧、顺而不忘逆、甘而不忘苦、福而不忘祸“。</p></blockquote><p>其实三句话都在讲述同一个道理，要有忧患意识，懂得居安思危。我之所以有这样的感悟，来自于18年几次人工智能交流活动，让我对所从事的工作充满了紧迫感：</p><ol><li><p>今年下半年，我们去云智科技调研，这曾经是一家在传统视频图像分析领域很领先的公司，随着这波深度学习技术兴起，他们在技术转型上慢了一小步，便被抛下了，很难在智能安防这个市场下占有一席之地；逆境求生存，他们只能在一些边缘行业寻找机会，比如参与了航天员的训练监控项目、音频分析等特质化的业务场景；云智是众多拥有一定技术的人工智能小公司的真实写照；这也是人工智能对传统行业的冲击；</p></li><li><p>10月份，我们去塞嘉电子正在驻场的一家客户——杭州卷烟厂进行实地调研，塞嘉在烟厂落地了一个烟包检测的人工智能的小项目，项目虽小，但为了推动后续合作，专门在现场设了驻场人员，一点一点的从客户身上挖需求，这让我真切感受到了人工智能外部项目落地的艰辛。跟赛嘉驻场人员沟通时，我问：你们觉得在实现客户需求有什么技术困难么？他说其实我们最终技术实现也没用什么深度学习技术，更多是在用传统的机器视觉分析，与技术比起来更头痛的是，烟厂更看重人工智能项目的学术价值，让我们帮他们发几篇SCI论文；</p></li><li><p>同样在10月，微软亚洲研究院副院长，自然语言处理领域的权威周明博士来公司做了一次交流，会上我问道了如何能够在人工智能领域跟的上？ 周院长笑了笑，说这是个开放的问题，我是做理论研究的，光我所从事的领域每天就更新几十篇重要论文，我休个假回来就需要补好久。我只能给你提供一些高效的工具。</p></li></ol><p>这些只是18年发生在身边众多感触的一小部分，在外部环境风起云涌，技术飞速发展的今天，一方面请珍惜xx给予了我们相对稳定、宽松的研发环境，但更重要的是安而不忘危，存而不忘亡，治而不忘乱。需要提升紧迫感和危机意识。没有压力就没有动力，去年这个时候我曾经开玩笑说唯一让我睡不着觉的恐怕是牛总在熬夜学习人工智能技术给我的压迫感，今年让我紧迫的可不止牛总一个了。</p><h2 id="学如逆水行舟，不进则退；心似平原纵马，易放难收"><a href="#学如逆水行舟，不进则退；心似平原纵马，易放难收" class="headerlink" title="学如逆水行舟，不进则退；心似平原纵马，易放难收"></a>学如逆水行舟，不进则退；心似平原纵马，易放难收</h2><p>2018年给我们的第二个感悟： </p><blockquote><p>学如逆水行舟，不进则退；心似平原纵马，易放难收。——《增广贤文》</p></blockquote><p>在过去的一年里，团队在技术跟进和项目研发过程中面临了很多全新的技术内容，在人工智能领域，技术上跟的上对我们团队是个很大的无形压力。我们一起啃论文、扣算法、业余充电。</p><p>在此，我要感谢团队这过去一年的辛勤付出，让技能时刻更新，让项目如期完成：</p><ul><li><p>…</p></li><li><p>对我而言，18年是我整体规划、系统学习的一年，迫于压力，在过去的一年来我读了二十四本书；拿了30门课程认证证书，花了400个学时来补技术短板；</p><p><img src="/2019/01/24/2018年总结：来自18年的工作感悟/感悟二.png" alt="感悟二"></p></li><li><p>在人工智能技术影响越来越多行业领域今天，它作为一项技能不再单是人工智能工程师一个角色的事情，我们即将迎来的是个全民AI时代，就像程序员都需要学习并使用一门编程语言一样，大家也应该尝试了解一两个机器学习算法，说不定对你手头工作大有裨益。</p><h2 id="路逢剑客须呈剑，不是诗人莫献诗"><a href="#路逢剑客须呈剑，不是诗人莫献诗" class="headerlink" title="路逢剑客须呈剑，不是诗人莫献诗"></a>路逢剑客须呈剑，不是诗人莫献诗</h2></li></ul><p>我的第三个感悟：</p><blockquote><p>“路逢剑客须呈剑，不是诗人莫献诗。”———《庄子》</p></blockquote><p>在过去的一年里，我们技术上对接过来自集团内外不同背景、不同需求、不用业务条线的很多人。每次我都怀着一个技术人员的执念，跟他们介绍什么是人工智能，什么是这个技术能做的，人工智能不是万能，没有什么是100%准确的。但每次都感觉收效甚微。</p><p>经历了几次碰壁，我去好好思考了这个问题，结论见图：</p><blockquote><p>你以为你给别人呈现的未必是别人以为你能给别人呈现的内容。</p></blockquote><p><img src="/2019/01/24/2018年总结：来自18年的工作感悟/不同人眼中的人工智能工程师2.jpg" alt="不同人眼中的人工智能工程师2"></p><p>我们人工智能的工作是每天都在想如何把这些数学模型做好，再通过计算机程序实现。当然在这期间我们也做了不少艺术化的工作，比如一直尝试把饼图画得更圆一点、哪种配色展示效果更好，这似乎是我们唯一的艺术追求，但实际上我们的追求还是要更大一点的。我觉得一个技术人员除了内修技术之外，还要在艺术上有所作为，做好一个演员。</p><p><img src="/2019/01/24/2018年总结：来自18年的工作感悟/感悟三.png" alt="感悟三"></p><p>我一直觉得，在人工智能领域奇缺一类人才，我姑且叫他们“人工智能需求转化工程师”。他们一方面具备快速了解客户业务场景的能力，另一方面需要明白人工智能技术的边界，当然更为关键的是懂得如何与客户沟通，要能迅速入戏，成为他们期待你成为的样子，知道该呈剑，还是献诗。只有如此才能将业务需求很好的和对应的人工智能技术衔接。</p><p>在18年，为了更好的让别人了解我们在做什么，更好的展现研发的技术，团队成员一起利用空余时间搭建了研究院的人工智能AI实验室平台，将一些可视化的技术成果和培训课程移值了上去，同时一起为集团进行了人工智能的技术分享，（我们在演员的道路上走出了第一步）；台上一分钟，台下十年功，18年只是一个开始，19年人工智能大戏，我们已经准备好粉墨登场。</p><p>2018年一代武侠大师金庸先生驾鹤西去，但19年技术武学仍要刻苦修炼，留本武林秘籍，供大家参悟：</p><blockquote><p>“知识是内功，执行是外功，经验是心法，领悟是贯通。”</p></blockquote><p>祝大家19年工作顺利！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/08.jpg&quot; alt=&quot;年度总结报告&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 2018年在从事人工智能工作过程中，有着诸多的感悟，记录下来激励自己2019年不要放松要求。&lt;/p&gt;
    
    </summary>
    
      <category term="工作总结" scheme="http://blog.a-stack.com/categories/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="总结" scheme="http://blog.a-stack.com/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="感悟" scheme="http://blog.a-stack.com/tags/%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>Batch-Normalization(批量归一化)</title>
    <link href="http://blog.a-stack.com/2019/01/12/Batch-Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <id>http://blog.a-stack.com/2019/01/12/Batch-Normalization-批量归一化/</id>
    <published>2019-01-12T14:45:29.000Z</published>
    <updated>2019-01-16T09:43:14.913Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/07.jpg" alt="BN不得不了解的深度学习技术~"></p><p><strong>摘要：</strong> 借重读论文的机会，重新整理一下Batch Normalization的关键技术。</p><a id="more"></a><blockquote><p>近期在重新做CS231n作业的时候，再次被Batch Normalization给困住，无奈从论文和资料重新开始查阅，这次尽量把相关技术研究透，整理以备速查。</p><p>CS231n中关于Batch Normalization的作业出在Assignment 2的第二个作业，可通过该<a href="https://github.com/ddebby/cs231n/blob/master/assignment2/BatchNormalization.ipynb" target="_blank" rel="noopener">链接</a>访问。</p></blockquote><h2 id="为什么需要Batch-Normalization"><a href="#为什么需要Batch-Normalization" class="headerlink" title="为什么需要Batch Normalization"></a>为什么需要Batch Normalization</h2><p>BN是由Google于2015年提出，这是一个深度神经网络训练的技巧，它不仅可以加快了模型的收敛速度，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。所以目前BN已经成为几乎所有卷积神经网络的标配技巧了。</p><p>在机器学习算法中，一般都假设所有输入特征符合独立同分布的特性，所以将输入数据归一化为单位高斯分布有利于提升模型的训练效果。这也是白化对于提升模型训练起的良好效果的原因。在深度神经网络中，我们通过对输入数据的约束，已经可以实现在输入层的激活可以保证数据的独立同分布，但随着网络传播过程，非线性激活函数的作用，导致数据的分布不断变化，数据的独立性（层的数据之间存在不同程度的相关性）被破坏。数据不再是零均值或者单位方差。同时随着梯度的更新，每层数据分布也在产生不同程度的偏移。</p><h2 id="Batch-Normalization的思想及实现"><a href="#Batch-Normalization的思想及实现" class="headerlink" title="Batch Normalization的思想及实现"></a>Batch Normalization的思想及实现</h2><p>模型训练阶段，BN层通过估计mini-batch中数据的均值和方差，将数据进行归一化。同时这些估计结果的滑动均值被记录下来，用于推理阶段。</p><p>Batch Normalization的计算</p><script type="math/tex; mode=display">\begin{align}\mu_i = \frac{1}{m} \sum_{k \in S_i} x_k\\\sigma_i = \sqrt{\frac{1}{m}\sum_{k\in S_i} (x_k-\mu_i)^2 + \epsilon}\\\hat x_i = \frac{x_i - \mu_i}{\sigma_i}\\y_i = \lambda \hat x_i + \beta\end{align}</script><blockquote><p>公式（4）是为了保证被BN层处理之后的数据仍具备一定的非线性特征而进行的恢复操作，不然网络所有层都是线性变换，无法进行模型参数更新。</p></blockquote><h3 id="前向传播的实现"><a href="#前向传播的实现" class="headerlink" title="前向传播的实现"></a>前向传播的实现</h3><p>参考BN作者论文的思路如下：</p><p><img src="/2019/01/12/Batch-Normalization-批量归一化/BN_Forward.PNG" alt="BN_Forward"></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the training-time forward pass for batch norm.      #</span></span><br><span class="line">        <span class="comment"># Use minibatch statistics to compute the mean and variance, use      #</span></span><br><span class="line">        <span class="comment"># these statistics to normalize the incoming data, and scale and      #</span></span><br><span class="line">        <span class="comment"># shift the normalized data using gamma and beta.                     #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should store the output in the variable out. Any intermediates  #</span></span><br><span class="line">        <span class="comment"># that you need for the backward pass should be stored in the cache   #</span></span><br><span class="line">        <span class="comment"># variable.                                                           #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should also use your computed sample mean and variance together #</span></span><br><span class="line">        <span class="comment"># with the momentum variable to update the running mean and running   #</span></span><br><span class="line">        <span class="comment"># variance, storing your result in the running_mean and running_var   #</span></span><br><span class="line">        <span class="comment"># variables.                                                          #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># Note that though you should be keeping track of the running         #</span></span><br><span class="line">        <span class="comment"># variance, you should normalize the data based on the standard       #</span></span><br><span class="line">        <span class="comment"># deviation (square root of variance) instead!                        # </span></span><br><span class="line">        <span class="comment"># Referencing the original paper (https://arxiv.org/abs/1502.03167)   #</span></span><br><span class="line">        <span class="comment"># might prove to be helpful.                                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#pass</span></span><br><span class="line">        sample_mean = np.mean(x,axis=<span class="number">0</span>)</span><br><span class="line">        sample_var = np.var(x,axis=<span class="number">0</span>)</span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line">        x_hat = (x - sample_mean)/np.sqrt(sample_var + eps)</span><br><span class="line">        out = gamma * x_hat  + beta</span><br><span class="line"></span><br><span class="line">        cache =(x, x_hat, gamma, beta,eps,sample_mean, sample_var)</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test-time forward pass for batch normalization. #</span></span><br><span class="line">        <span class="comment"># Use the running mean and variance to normalize the incoming data,   #</span></span><br><span class="line">        <span class="comment"># then scale and shift the normalized data using gamma and beta.      #</span></span><br><span class="line">        <span class="comment"># Store the result in the out variable.                               #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#pass</span></span><br><span class="line">        x_hat = (x - running_mean)/np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma * x_hat  + beta</span><br><span class="line">        cache =(x,x_hat, gamma, beta, eps, running_mean, running_var)</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><h3 id="反向传播的实现"><a href="#反向传播的实现" class="headerlink" title="反向传播的实现"></a>反向传播的实现</h3><p><img src="/2019/01/12/Batch-Normalization-批量归一化/BN_Backward.PNG" alt="BN_Backward"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Alternative backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation you should work out the derivatives for the batch</span></span><br><span class="line"><span class="string">    normalizaton backward pass on paper and simplify as much as possible. You</span></span><br><span class="line"><span class="string">    should be able to derive a simple expression for the backward pass. </span></span><br><span class="line"><span class="string">    See the jupyter notebook for more hints.</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">    Note: This implementation should expect to receive the same cache variable</span></span><br><span class="line"><span class="string">    as batchnorm_backward, but might not use all of the values in the cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs / outputs: Same as batchnorm_backward</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></span><br><span class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># After computing the gradient with respect to the centered inputs, you   #</span></span><br><span class="line">    <span class="comment"># should be able to compute gradients with respect to the inputs in a     #</span></span><br><span class="line">    <span class="comment"># single statement; our implementation fits on a single 80-character line.#</span></span><br><span class="line">    <span class="comment">###########################################################################    </span></span><br><span class="line">    x, x_hat, gamma, beta, eps, mean, var = cache</span><br><span class="line"></span><br><span class="line">    dgamma = np.sum(dout*x_hat,axis=<span class="number">0</span>)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dx_hat = dout * gamma</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># temp vars</span></span><br><span class="line">    xmu = x - mean</span><br><span class="line">    istd = <span class="number">1</span>/np.sqrt(var + eps)</span><br><span class="line">    m = x.shape[<span class="number">0</span>]   </span><br><span class="line">    </span><br><span class="line">    dvar = np.sum(dx_hat*xmu*(<span class="number">-0.5</span>)*np.power((var+eps),<span class="number">-1.5</span>),axis=<span class="number">0</span>)</span><br><span class="line">    dmean = np.sum(dx_hat*(<span class="number">-1</span>)*istd,axis=<span class="number">0</span>) + np.sum(<span class="number">-2</span>*xmu*dvar,axis=<span class="number">0</span>)/m</span><br><span class="line">    dx = dx_hat*istd + dvar*<span class="number">2</span>*xmu/m + dmean/m    </span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h3 id="BN层参数的学习过程中"><a href="#BN层参数的学习过程中" class="headerlink" title="BN层参数的学习过程中"></a>BN层参数的学习过程中</h3><p>在每个BN层，除了w和b之外，还需要学习<code>gamma</code>和<code>beta</code>两个额外的参数。</p><h2 id="Batch-Normalization带来的好处"><a href="#Batch-Normalization带来的好处" class="headerlink" title="Batch Normalization带来的好处"></a>Batch Normalization带来的好处</h2><ol><li>不仅极大的提升了训练速度，收敛过程也大大的加快了；</li><li>增强分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；</li><li>使用了BN层之后，进一步减少了在处理过拟合过程中对Dropout的依赖；</li><li>对于网络的参数初始化要求降低，不再担心网络初始化没控制好导致网络难以训练的问题（尤其是深层次网络）</li></ol><h2 id="其它的归一化方法"><a href="#其它的归一化方法" class="headerlink" title="其它的归一化方法"></a>其它的归一化方法</h2><p>由于BN会受到batch size大小的影响，如果batch size太小，算出的均值和方差就会不准确，太大存储可能不够用。所以衍生出了几种优化表达。</p><p><img src="/2019/01/12/Batch-Normalization-批量归一化/BN_error_with_small_batch_size.png" alt="BN_error_with_small_batch_size"></p><p><img src="/qnsource/images/2018-04-28-cs231n-notes/normalization_methods.png" alt="normalization_methods"></p><ul><li>BatchNorm： batch方向做归一化，计算 <code>H*W*C</code> 的均值；</li><li>LayerNorm： channel方向做归一化，计算<code>N*H*W</code> 的均值；</li><li>InstanceNorm： 一个channel内做归一化，计算<code>H*W</code>的均值；</li><li>GroupNorm： 将Channel方向分为Group，然后每个Group内做归一化，计算<code>(C//G)*H*W</code>的均值</li></ul><blockquote><p>当G=C时，GroupNorm为LayerNorm，当G=1时，GroupNorm为InstanceNorm</p></blockquote><p>其中，GourpNorm的启发来源于图像的手工特征提取器（比如HOG，SIFT）发现很多不同类型的特征都是成组出现的，利用成组封装的方式进行特征均值、方差提取可能更能反映特征的真实情况，尤其在小批量BN效果下降的情况下作为BN的一个替代品。</p><p>但实际上，在使用过程中还是优先选用BN，只有在批量实在很小，如视频分析、图像分割等场景，每个批次只有一两张图像的情况下，选用GroupNorm作为替代。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”, ICML 2015.</a></li><li><a href="https://arxiv.org/abs/1803.08494" target="_blank" rel="noopener">Wu, Yuxin, and Kaiming He. “Group Normalization.” arXiv preprint arXiv:1803.08494 (2018).</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/07.jpg&quot; alt=&quot;BN不得不了解的深度学习技术~&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 借重读论文的机会，重新整理一下Batch Normalization的关键技术。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://blog.a-stack.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>数据科学工具——Pandas</title>
    <link href="http://blog.a-stack.com/2019/01/02/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%B7%A5%E5%85%B7%E2%80%94%E2%80%94Pandas/"/>
    <id>http://blog.a-stack.com/2019/01/02/数据科学工具——Pandas/</id>
    <published>2019-01-02T08:54:58.000Z</published>
    <updated>2019-02-25T06:24:12.385Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2019/01/02/数据科学工具——Pandas/pandas.jpg" alt="pandas"></p><p><strong>摘要：</strong>工欲善其事必先利其器，数据科学工具的整理是个循序渐进的过程，开个头立个Flag🏁，慢慢积累工具的使用方法，便于以后速查。主要涉及工具包括：Ipython，Numpy，Pandas，Scikit-Learn等。</p><a id="more"></a><h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><p><strong>Index + Values</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pd.Series([<span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">0.75</span>, <span class="number">1.0</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])</span><br><span class="line"></span><br><span class="line">repeat_constent_value = pd.Series(<span class="number">5</span>, index=[<span class="number">100</span>,<span class="number">200</span>,<span class="number">300</span>])</span><br></pre></td></tr></table></figure><blockquote><p>Series是Python字典和Numpy的有机结合，集成了字典引用的便捷性，同时也实现了narray结构的灵活性。</p></blockquote><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><ul><li><p>可以通过Series生成；</p></li><li><p>通过一组python字典变量生成；</p></li><li><p>通过二维的Numpy Array生成</p></li></ul><h2 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h2><ul><li><code>loc</code>: 实际的index值: <code>data.loc[data.density &gt; 100, [&#39;pop&#39;, &#39;density&#39;]]</code></li><li><code>iloc</code>: index的编号值： <code>data.iloc[:3, :2]</code></li><li><code>ix</code>: 以上两者的组合： `data.ix[:3, :’column2’]</li></ul><blockquote><ol><li>First, while indexing refers to columns, slicing refers to rows.</li><li>Similarly, direct masking operations are also interpreted row-wise rather than column-wise.</li></ol></blockquote><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><p>`</p><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><p><code>NaN</code>是浮点标记，可参与运算；结果都是<code>NaN</code></p><ul><li><p><code>isunll()</code>与<code>notnull</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[data.notnull()]</span><br></pre></td></tr></table></figure></li><li><p><code>dropna()</code></p><ul><li>将丢弃存在至少一个null数值的行；</li><li>也可以通过命令<code>dropna(axis=&#39;columns&#39;)</code>丢弃列;</li><li>也可以通过参数控制丢弃的门限(至少有几个非空元素)：<code>df.dropna(axis=&#39;rows&#39;, thresh=3)</code></li></ul></li><li><p><code>fillna()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data.fillna(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># forward-fill</span></span><br><span class="line">data.fillna(method=<span class="string">'ffill'</span>)</span><br><span class="line"><span class="comment"># back-fill </span></span><br><span class="line">data.fillna(method=<span class="string">'bfill'</span>)</span><br></pre></td></tr></table></figure></li></ul><h2 id="时序数据处理"><a href="#时序数据处理" class="headerlink" title="时序数据处理"></a>时序数据处理</h2><h3 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h3><ul><li>Time Stamps：Pandas中提供<code>Timestamp</code>类型</li><li>Time Intervals/Periods: Pandas中提供<code>Period</code>类型</li><li>Time delta/Durations: Pandas中提供<code>Timedelta</code>类型</li><li>python中处理时间的库有<code>datetime</code>和<code>dateutil</code></li><li>Numpy中处理时间数据的类型：<code>datetime64</code></li></ul><h3 id="Pandas中的主要相关函数"><a href="#Pandas中的主要相关函数" class="headerlink" title="Pandas中的主要相关函数"></a>Pandas中的主要相关函数</h3><p><code>pd.to_datetime()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">date = pd.to_datetime(<span class="string">"4th of July, 2015"</span>)</span><br><span class="line"></span><br><span class="line">date + pd.to_timedelta(np.arange(<span class="number">12</span>), <span class="string">'D'</span>)</span><br><span class="line">date2 = pd.to_datetime([datetime(<span class="number">2015</span>, <span class="number">7</span>, <span class="number">3</span>), <span class="string">'4th of July, 2015'</span>,</span><br><span class="line">                       <span class="string">'2015-Jul-6'</span>, <span class="string">'07-07-2015'</span>, <span class="string">'20150708'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DatetimeIndex([<span class="string">'2015-07-04'</span>, <span class="string">'2015-07-05'</span>, <span class="string">'2015-07-06'</span>, <span class="string">'2015-07-07'</span>,</span><br><span class="line">               <span class="string">'2015-07-08'</span>, <span class="string">'2015-07-09'</span>, <span class="string">'2015-07-10'</span>, <span class="string">'2015-07-11'</span>,</span><br><span class="line">               <span class="string">'2015-07-12'</span>, <span class="string">'2015-07-13'</span>, <span class="string">'2015-07-14'</span>, <span class="string">'2015-07-15'</span>],</span><br><span class="line">              dtype=<span class="string">'datetime64[ns]'</span>, freq=None)</span><br></pre></td></tr></table></figure><p>Pandas中时间序列分析的强大之处在于当时间变量作为Index时，通过对Index的操作来体现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">index = pd.DatetimeIndex([<span class="string">'2014-07-04'</span>, <span class="string">'2014-08-04'</span>,</span><br><span class="line">                          <span class="string">'2015-07-04'</span>, <span class="string">'2015-08-04'</span>])</span><br><span class="line">data = pd.Series([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=index)</span><br><span class="line">data[<span class="string">'2015'</span>]</span><br><span class="line">data[<span class="string">'2014'</span>:<span class="string">'2015'</span>]</span><br></pre></td></tr></table></figure><p><code>pd.date_range</code>,产生从起始时间开始的一段时间信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pd.date_range(<span class="string">'2015-07-03'</span>, <span class="string">'2015-07-10'</span>)</span><br><span class="line">pd.date_range(<span class="string">'2015-07-03'</span>, periods=<span class="number">8</span>)</span><br><span class="line">pd.date_range(<span class="string">'2015-07-03'</span>, periods=<span class="number">8</span>, freq=<span class="string">'H'</span>)</span><br></pre></td></tr></table></figure><h3 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h3><ul><li><code>resample()</code>: 聚合采样；</li><li><code>asfreq()</code>: 选择抽样；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#降采样</span></span><br><span class="line">data.resample(<span class="string">'BA'</span>).mean()</span><br><span class="line">data.asfreq(<span class="string">'BA'</span>)</span><br><span class="line"><span class="comment">#升采样</span></span><br><span class="line">data.asfreq(<span class="string">'D'</span>, method=<span class="string">'bfill'</span>)</span><br><span class="line">data.asfreq(<span class="string">'D'</span>, method=<span class="string">'ffill'</span>)</span><br></pre></td></tr></table></figure><h3 id="数据偏移"><a href="#数据偏移" class="headerlink" title="数据偏移"></a>数据偏移</h3><blockquote><p>用于比较不同时间数据差异</p></blockquote><ul><li><code>tshift()</code></li><li><code>shift()</code></li></ul><h3 id="滑动平均：Rolling"><a href="#滑动平均：Rolling" class="headerlink" title="滑动平均：Rolling"></a>滑动平均：<code>Rolling</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = data.rolling(<span class="number">365</span>, center=<span class="keyword">True</span>)</span><br><span class="line">data.rolling.mean()</span><br><span class="line">data.rolling.std()</span><br></pre></td></tr></table></figure><h3 id="※时序数据分析举例"><a href="#※时序数据分析举例" class="headerlink" title="※时序数据分析举例"></a>※时序数据分析举例</h3><blockquote><p>数据：西雅图自行车计数，<a href="https://data.seattle.gov/Transportation/Fremont-Bridge-Hourly-Bicycle-Counts-by-Month-Octo/65db-xm6k" target="_blank" rel="noopener">Link</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">'FremontBridge.csv'</span>, index_col=<span class="string">'Date'</span>, parse_dates=<span class="keyword">True</span>)</span><br><span class="line">data.columns = [<span class="string">'West'</span>, <span class="string">'East'</span>]</span><br><span class="line">data[<span class="string">'Total'</span>] = data.eval(<span class="string">'West + East'</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th></th><th>West</th><th>East</th><th>Total</th></tr></thead><tbody><tr><td>Date</td><td></td><td></td><td></td></tr><tr><td>2012-10-03 00:00:00</td><td>9.0</td><td>4.0</td><td>13.0</td></tr><tr><td>2012-10-03 01:00:00</td><td>6.0</td><td>4.0</td><td>10.0</td></tr><tr><td>2012-10-03 02:00:00</td><td>1.0</td><td>1.0</td><td>2.0</td></tr><tr><td>2012-10-03 03:00:00</td><td>3.0</td><td>2.0</td><td>5.0</td></tr><tr><td>2012-10-03 04:00:00</td><td>1.0</td><td>6.0</td><td>7.0</td></tr></tbody></table></div><p>由于数据每小时记录一个数值，直接画图，数据点过密：</p><p><img src="/2019/01/02/数据科学工具——Pandas/timeseries_1.png" alt="timeseries_1"></p><ol><li><p>通过降采样，方便显示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weekly = data.resample(<span class="string">'W'</span>).sum()</span><br><span class="line">weekly.plot(style=[<span class="string">':'</span>, <span class="string">'--'</span>, <span class="string">'-'</span>])</span><br><span class="line">plt.ylabel(<span class="string">'Weekly bicycle count'</span>);</span><br></pre></td></tr></table></figure><p><img src="/2019/01/02/数据科学工具——Pandas/timeseries_weekly.png" alt="timeseries_weekly"></p></li><li><p>通过滑动平均处理每天为单位的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">daily = data.resample(<span class="string">'D'</span>).sum()</span><br><span class="line">daily.rolling(<span class="number">30</span>, center=<span class="keyword">True</span>).sum().plot(style=[<span class="string">':'</span>, <span class="string">'--'</span>, <span class="string">'-'</span>])</span><br><span class="line">plt.ylabel(<span class="string">'mean hourly count'</span>);</span><br></pre></td></tr></table></figure><p><img src="/2019/01/02/数据科学工具——Pandas/timeseries_daily.png" alt="timeseries_daily"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">daily.rolling(<span class="number">50</span>, center=<span class="keyword">True</span>,</span><br><span class="line">              win_type=<span class="string">'gaussian'</span>).sum(std=<span class="number">10</span>).plot(style=[<span class="string">':'</span>, <span class="string">'--'</span>, <span class="string">'-'</span>]);</span><br></pre></td></tr></table></figure><p><img src="/2019/01/02/数据科学工具——Pandas/timeseries_daily2.png" alt="timeseries_daily2"></p></li><li><p>通过数据分组查看每年、每周或每天的数据规律</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">by_time = data.groupby(data.index.time).mean()</span><br><span class="line">hourly_ticks = <span class="number">4</span> * <span class="number">60</span> * <span class="number">60</span> * np.arange(<span class="number">6</span>)</span><br><span class="line">by_time.plot(xticks=hourly_ticks, style=[<span class="string">':'</span>, <span class="string">'--'</span>, <span class="string">'-'</span>]);</span><br></pre></td></tr></table></figure><p><img src="/2019/01/02/数据科学工具——Pandas/data_in_one_day.png" alt="data_in_one_day"></p></li><li><p>进一步查看周末与周中数据区别</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">by_weekday = data.groupby(data.index.dayofweek).mean()</span><br><span class="line">by_weekday.index = [<span class="string">'Mon'</span>, <span class="string">'Tues'</span>, <span class="string">'Wed'</span>, <span class="string">'Thurs'</span>, <span class="string">'Fri'</span>, <span class="string">'Sat'</span>, <span class="string">'Sun'</span>]</span><br><span class="line">by_weekday.plot(style=[<span class="string">':'</span>, <span class="string">'--'</span>, <span class="string">'-'</span>]);</span><br></pre></td></tr></table></figure><p><img src="/2019/01/02/数据科学工具——Pandas/data_in_one_week.png" alt="data_in_one_week"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">weekend = np.where(data.index.weekday &lt; <span class="number">5</span>, <span class="string">'Weekday'</span>, <span class="string">'Weekend'</span>)</span><br><span class="line">by_time = data.groupby([weekend, data.index.time]).mean()</span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">5</span>))</span><br><span class="line">by_time.ix[<span class="string">'Weekday'</span>].plot(ax=ax[<span class="number">0</span>], title=<span class="string">'Weekdays'</span>,</span><br><span class="line">                           xticks=hourly_ticks, style=[<span class="string">':'</span>, <span class="string">'--'</span>, <span class="string">'-'</span>])</span><br><span class="line">by_time.ix[<span class="string">'Weekend'</span>].plot(ax=ax[<span class="number">1</span>], title=<span class="string">'Weekends'</span>,</span><br><span class="line">                           xticks=hourly_ticks, style=[<span class="string">':'</span>, <span class="string">'--'</span>, <span class="string">'-'</span>]);</span><br></pre></td></tr></table></figure><p><img src="/2019/01/02/数据科学工具——Pandas/week+OneDay.png" alt="week+OneDay"></p></li></ol><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><div class="table-container"><table><thead><tr><th>Code</th><th>Description</th><th>Code</th><th>Description</th></tr></thead><tbody><tr><td><code>D</code></td><td>Calendar day</td><td><code>B</code></td><td>Business day</td></tr><tr><td><code>W</code></td><td>Weekly</td><td>\</td><td></td><td></td></tr><tr><td><code>M</code></td><td>Month end</td><td><code>BM</code></td><td>Business month end</td></tr><tr><td><code>Q</code></td><td>Quarter end</td><td><code>BQ</code></td><td>Business quarter end</td></tr><tr><td><code>A</code></td><td>Year end</td><td><code>BA</code></td><td>Business year end</td></tr><tr><td><code>H</code></td><td>Hours</td><td><code>BH</code></td><td>Business hours</td></tr><tr><td><code>T</code></td><td>Minutes</td><td>\</td><td></td><td></td></tr><tr><td><code>S</code></td><td>Seconds</td><td>\</td><td></td><td></td></tr><tr><td><code>L</code></td><td>Milliseonds</td><td>\</td><td></td><td></td></tr><tr><td><code>U</code></td><td>Microseconds</td><td>\</td><td></td><td></td></tr><tr><td><code>N</code></td><td>nanoseconds</td><td>\</td><td></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>Directive</th><th>Meaning</th><th>Example</th><th>Notes</th></tr></thead><tbody><tr><td><code>%a</code></td><td>Weekday as locale’s abbreviated name.</td><td>Sun, Mon, …, Sat (en_US);So, Mo, …, Sa (de_DE)</td><td>(1)</td></tr><tr><td><code>%A</code></td><td>Weekday as locale’s full name.</td><td>Sunday, Monday, …, Saturday (en_US);Sonntag, Montag, …, Samstag (de_DE)</td><td>(1)</td></tr><tr><td><code>%w</code></td><td>Weekday as a decimal number, where 0 is Sunday and 6 is Saturday.</td><td>0, 1, …, 6</td><td></td></tr><tr><td><code>%d</code></td><td>Day of the month as a zero-padded decimal number.</td><td>01, 02, …, 31</td><td></td></tr><tr><td><code>%b</code></td><td>Month as locale’s abbreviated name.</td><td>Jan, Feb, …, Dec (en_US);Jan, Feb, …, Dez (de_DE)</td><td>(1)</td></tr><tr><td><code>%B</code></td><td>Month as locale’s full name.</td><td>January, February, …, December (en_US);Januar, Februar, …, Dezember (de_DE)</td><td>(1)</td></tr><tr><td><code>%m</code></td><td>Month as a zero-padded decimal number.</td><td>01, 02, …, 12</td><td></td></tr><tr><td><code>%y</code></td><td>Year without century as a zero-padded decimal number.</td><td>00, 01, …, 99</td><td></td></tr><tr><td><code>%Y</code></td><td>Year with century as a decimal number.</td><td>0001, 0002, …, 2013, 2014, …, 9998, 9999</td><td>(2)</td></tr><tr><td><code>%H</code></td><td>Hour (24-hour clock) as a zero-padded decimal number.</td><td>00, 01, …, 23</td><td></td></tr><tr><td><code>%I</code></td><td>Hour (12-hour clock) as a zero-padded decimal number.</td><td>01, 02, …, 12</td><td></td></tr><tr><td><code>%p</code></td><td>Locale’s equivalent of either AM or PM.</td><td>AM, PM (en_US);am, pm (de_DE)</td><td>(1), (3)</td></tr><tr><td><code>%M</code></td><td>Minute as a zero-padded decimal number.</td><td>00, 01, …, 59</td><td></td></tr><tr><td><code>%S</code></td><td>Second as a zero-padded decimal number.</td><td>00, 01, …, 59</td><td>(4)</td></tr><tr><td><code>%f</code></td><td>Microsecond as a decimal number, zero-padded on the left.</td><td>000000, 000001, …, 999999</td><td>(5)</td></tr><tr><td><code>%z</code></td><td>UTC offset in the form ±HHMM[SS[.ffffff]] (empty string if the object is naive).</td><td>(empty), +0000, -0400, +1030, +063415, -030712.345216</td><td>(6)</td></tr><tr><td><code>%Z</code></td><td>Time zone name (empty string if the object is naive).</td><td>(empty), UTC, EST, CST</td><td></td></tr><tr><td><code>%j</code></td><td>Day of the year as a zero-padded decimal number.</td><td>001, 002, …, 366</td><td></td></tr><tr><td><code>%U</code></td><td>Week number of the year (Sunday as the first day of the week) as a zero padded decimal number. All days in a new year preceding the first Sunday are considered to be in week 0.</td><td>00, 01, …, 53</td><td>(7)</td></tr><tr><td><code>%W</code></td><td>Week number of the year (Monday as the first day of the week) as a decimal number. All days in a new year preceding the first Monday are considered to be in week 0.</td><td>00, 01, …, 53</td><td>(7)</td></tr><tr><td><code>%c</code></td><td>Locale’s appropriate date and time representation.</td><td>Tue Aug 16 21:30:00 1988 (en_US);Di 16 Aug 21:30:00 1988 (de_DE)</td><td>(1)</td></tr><tr><td><code>%x</code></td><td>Locale’s appropriate date representation.</td><td>08/16/88 (None);08/16/1988 (en_US);16.08.1988 (de_DE)</td><td>(1)</td></tr><tr><td><code>%X</code></td><td>Locale’s appropriate time representation.</td><td>21:30:00 (en_US);21:30:00 (de_DE)</td><td>(1)</td></tr><tr><td><code>%%</code></td><td>A literal <code>&#39;%&#39;</code> character.</td><td>%</td></tr></tbody></table></div><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>Book《Python Data Science Handbook》](<a href="https://jakevdp.github.io/PythonDataScienceHandbook/" target="_blank" rel="noopener">https://jakevdp.github.io/PythonDataScienceHandbook/</a>)</li><li><a href="https://github.com/jakevdp/PythonDataScienceHandbook" target="_blank" rel="noopener">https://github.com/jakevdp/PythonDataScienceHandbook</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2019/01/02/数据科学工具——Pandas/pandas.jpg&quot; alt=&quot;pandas&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;工欲善其事必先利其器，数据科学工具的整理是个循序渐进的过程，开个头立个Flag🏁，慢慢积累工具的使用方法，便于以后速查。主要涉及工具包括：Ipython，Numpy，Pandas，Scikit-Learn等。&lt;/p&gt;
    
    </summary>
    
      <category term="工具" scheme="http://blog.a-stack.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="时序数据" scheme="http://blog.a-stack.com/tags/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE/"/>
    
      <category term="工具使用" scheme="http://blog.a-stack.com/tags/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"/>
    
      <category term="Pandas" scheme="http://blog.a-stack.com/tags/Pandas/"/>
    
  </entry>
  
  <entry>
    <title>数据科学工具——Numpy</title>
    <link href="http://blog.a-stack.com/2018/12/30/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%B7%A5%E5%85%B7%E2%80%94%E2%80%94Numpy/"/>
    <id>http://blog.a-stack.com/2018/12/30/数据科学工具——Numpy/</id>
    <published>2018-12-30T08:54:48.000Z</published>
    <updated>2018-12-30T14:11:41.226Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/12/30/数据科学工具——Numpy/Numpy.png" alt="Numpy"></p><p><strong>摘要：</strong>工欲善其事必先利其器，数据科学工具的整理是个循序渐进的过程，开个头立个Flag🏁，慢慢积累工具的使用方法，便于以后速查。主要涉及工具包括：Ipython，Numpy，Pandas，Scikit-Learn等。</p><a id="more"></a><h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><ul><li>Python List and Numpy</li><li>一个Numpy矩阵中的所有元素类型必须相同；</li><li>矩阵slicing操作，返回的是对原矩阵的引用而非复制，所以对其操作将影响原矩阵；</li></ul><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a 3x5 array filled with 3.14 </span></span><br><span class="line">np.full((<span class="number">3</span>, <span class="number">5</span>), <span class="number">3.14</span>)</span><br><span class="line"><span class="comment"># Create an array filled with a linear sequence # Starting at 0, ending at 20, stepping by 2 # (this is similar to the built-in range() function) </span></span><br><span class="line">np.arange(<span class="number">0</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment">#&gt;&gt; array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])</span></span><br><span class="line"><span class="comment"># Create an array of five values evenly spaced between 0 and 1 </span></span><br><span class="line">np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment">#&gt;&gt; array([0.  , 0.25, 0.5 , 0.75, 1.  ])</span></span><br></pre></td></tr></table></figure><h2 id="矩阵组合：-np-concatenate-np-vstack-np-hstack"><a href="#矩阵组合：-np-concatenate-np-vstack-np-hstack" class="headerlink" title="矩阵组合： np.concatenate,np.vstack,np.hstack"></a>矩阵组合： <code>np.concatenate</code>,<code>np.vstack</code>,<code>np.hstack</code></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># concatenate along the second axis (zero-indexed) </span></span><br><span class="line">np.concatenate([grid, grid], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="np-random"><a href="#np-random" class="headerlink" title="np.random"></a><code>np.random</code></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a 3x3 array of uniformly distributed # random values between 0 and 1  </span></span><br><span class="line">  np.random.random((<span class="number">3</span>, <span class="number">3</span>)) </span><br><span class="line">Out[<span class="number">17</span>]: array([[ <span class="number">0.99844933</span>, <span class="number">0.52183819</span>, <span class="number">0.22421193</span>], [ <span class="number">0.08007488</span>, <span class="number">0.45429293</span>, <span class="number">0.20941444</span>], [ <span class="number">0.14360941</span>, <span class="number">0.96910973</span>, <span class="number">0.946117</span> ]])</span><br><span class="line">In[<span class="number">18</span>]: <span class="comment"># Create a 3x3 array of normally distributed random values # with mean 0 and standard deviation 1 </span></span><br><span class="line">  np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">Out[<span class="number">18</span>]: array([[ <span class="number">1.51772646</span>, <span class="number">0.39614948</span>, <span class="number">-0.10634696</span>], [ <span class="number">0.25671348</span>, <span class="number">0.00732722</span>, <span class="number">0.37783601</span>], [ <span class="number">0.68446945</span>, <span class="number">0.15926039</span>, <span class="number">-0.70744073</span>]])</span><br><span class="line">In[<span class="number">19</span>]: <span class="comment"># Create a 3x3 array of random integers in the interval [0, 10) </span></span><br><span class="line">  np.random.randint(<span class="number">0</span>, <span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">Out[<span class="number">19</span>]: array([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><ol><li><p>矩阵元素反置</p><p>x是一维矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[::<span class="number">-1</span>]</span><br></pre></td></tr></table></figure></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>Book《Python Data Science Handbook》](<a href="https://jakevdp.github.io/PythonDataScienceHandbook/" target="_blank" rel="noopener">https://jakevdp.github.io/PythonDataScienceHandbook/</a>)</li><li><a href="https://github.com/jakevdp/PythonDataScienceHandbook" target="_blank" rel="noopener">https://github.com/jakevdp/PythonDataScienceHandbook</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2018/12/30/数据科学工具——Numpy/Numpy.png&quot; alt=&quot;Numpy&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;工欲善其事必先利其器，数据科学工具的整理是个循序渐进的过程，开个头立个Flag🏁，慢慢积累工具的使用方法，便于以后速查。主要涉及工具包括：Ipython，Numpy，Pandas，Scikit-Learn等。&lt;/p&gt;
    
    </summary>
    
      <category term="工具" scheme="http://blog.a-stack.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="教学" scheme="http://blog.a-stack.com/tags/%E6%95%99%E5%AD%A6/"/>
    
      <category term="Numpy" scheme="http://blog.a-stack.com/tags/Numpy/"/>
    
      <category term="工具使用" scheme="http://blog.a-stack.com/tags/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>数据科学工具——Ipython</title>
    <link href="http://blog.a-stack.com/2018/12/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%B7%A5%E5%85%B7%E2%80%94%E2%80%94Ipython/"/>
    <id>http://blog.a-stack.com/2018/12/29/数据科学工具——Ipython/</id>
    <published>2018-12-29T08:13:36.000Z</published>
    <updated>2018-12-30T08:54:09.979Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/12/29/数据科学工具——Ipython/Jupyter Notebook.png" alt="Jupyter Notebook"></p><p><strong>摘要：</strong> 工欲善其事必先利其器，数据科学工具的整理是个循序渐进的过程，开个头立个Flag🏁，慢慢积累工具的使用方法，便于以后速查。主要涉及工具包括：Ipython，Numpy，Pandas，Scikit-Learn等。</p><a id="more"></a><h2 id="数据科学的定义"><a href="#数据科学的定义" class="headerlink" title="数据科学的定义"></a>数据科学的定义</h2><blockquote><p>Data science comprises three distinct and overlapping areas: the skills of a <em>statistician</em> who knows how to model and summarize datasets (which are growing ever larger); the skills of a <em>computer scientist</em> who can design and use algorithms to efficiently store, process, and visualize this data; and the <em>domain expertise</em>—what we might think of as “classical” training in a subject—necessary both to formulate the right questions and to put their answers in context.</p></blockquote><h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><ol><li><p>Accessing Documentation with <code>?</code> | Accessing Source Code with <code>??</code></p></li><li><p><code>+ &lt;TAB&gt;</code> 的使用</p></li><li><p>查找命令的通配符匹配策略： <code>*</code> + <code>?</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">*Warning?</span><br><span class="line"></span><br><span class="line">str.*find*?</span><br><span class="line">  str.find</span><br><span class="line">  str.rfind</span><br></pre></td></tr></table></figure></li><li><p>在Notebook中可以使用<code>automagic</code>,直接通过<code>cd</code>命令切换目录；</p><ol><li>其它还有<code>%cat,%cp,%env,%ls,%man,%mkdir,%more,%mv, %pwd, %rm, and %rmdir</code></li></ol></li><li></li></ol><h2 id="魔法字符"><a href="#魔法字符" class="headerlink" title="魔法字符%"></a>魔法字符<code>%</code></h2><ol><li><p><code>%run</code></p><p>运行<code>.py</code>文件,同时把文件中的上下文环境加载到当前环境中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run xxx.py</span><br></pre></td></tr></table></figure></li><li><p><code>%timeit</code>，<code>%time</code>,<code>%%time</code></p></li><li><p><code>%history -n 1-4</code></p></li></ol><h2 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h2><p><code>ipdb</code>:Ipython Debugger</p><p><code>%debug</code> 进入调试模式，<code>quit</code>命令退出；</p><ul><li><code>up</code>和<code>down</code>可以控制代码执行位置；</li></ul><p>命令<code>%pdb on</code>可以开启自动调试模式，每次出异常自动触发调试；</p><p>其它相关调试命令见下表：</p><div class="table-container"><table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody><tr><td><code>list</code></td><td>显示当前异常在文件中的位置</td></tr><tr><td><code>h(elp)</code></td><td>查看帮助文件</td></tr><tr><td><code>q(uit)</code></td><td>退出调试模式</td></tr><tr><td><code>c(ontinue)</code></td><td>中断调试，继续执行程序</td></tr><tr><td><code>n(ext)</code></td><td>继续程序下一步</td></tr><tr><td><code>&lt;enter&gt;</code></td><td>重复上一个命令</td></tr><tr><td><code>p(rint)</code></td><td>打印变量</td></tr><tr><td><code>s(tep)</code></td><td>进入子程序</td></tr><tr><td><code>r(etrun)</code></td><td>返回子程序</td></tr></tbody></table></div><h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><h4 id="命令行模式-按-Esc-生效-编辑快捷键"><a href="#命令行模式-按-Esc-生效-编辑快捷键" class="headerlink" title="命令行模式(按 Esc 生效)编辑快捷键"></a>命令行模式(按 Esc 生效)编辑快捷键</h4><p>F: 查找并且替换</p><p>Enter: 进入编辑模式</p><p>P: 打开命令配置</p><p>Shift-Enter: 运行代码块, 选择下面的代码块</p><p>Ctrl-Enter: 运行选中的代码块</p><p>Alt-Enter: 运行代码块并且插入下面</p><p><strong>Y: 把代码块变成代码</strong></p><p><strong>M: 把代码块变成标签</strong></p><p><strong>R: 清除代码块格式</strong></p><p>1: 把代码块变成heading 1</p><p>2: 把代码块变成heading 2</p><p>3: 把代码块变成heading 3</p><p>4: 把代码块变成heading 4</p><p>5: 把代码块变成heading 5</p><p>6: 把代码块变成heading 6</p><p>K: 选择上面的代码块</p><p>J: 选择下面的代码块</p><p>Shift-K: 扩展上面选择的代码块</p><p>Shift-上: 扩展上面选择的代码块</p><p>Shift-下: 扩展下面选择的代码块</p><p>Shift-J: 扩展下面选择的代码块</p><p><strong>A: 在上面插入代码块</strong></p><p><strong>B: 在下面插入代码块</strong></p><p>X: 剪切选择的代码块</p><p>C: 复制选择的代码块</p><p>Shift-V: 粘贴到上面</p><p>V: 粘贴到下面</p><p>Z: 撤销删除</p><p>D,D: 删除选中单元格</p><p><strong>Shift-M: 合并选中单元格, 如果只有一个单元格被选中</strong></p><p>Ctrl-S: 保存并检查</p><p>L: 切换行号</p><p>O: 选择单元格的输出</p><p>Shift-O: 切换选定单元的输出滚动</p><p>H: 显示快捷键</p><p>I,I: 中断服务</p><p>0,0: 重启服务(带窗口)</p><p>Esc: 关闭页面</p><p>Q: 关闭页面</p><p>Shift-L: 在所有单元格中切换行号，并保持设置</p><p>Shift-空格: 向上滚动</p><p>空格: 向下滚动</p><h4 id="编辑模式-按-Enter-生效"><a href="#编辑模式-按-Enter-生效" class="headerlink" title="编辑模式(按 Enter 生效)"></a>编辑模式(按 Enter 生效)</h4><p>Tab: 代码完成或缩进</p><p><strong>Shift-Tab: 工具提示</strong></p><p>Ctrl-]: 缩进</p><p>Ctrl-[: 取消缩进</p><p>Ctrl-A: 全选</p><p>Ctrl-Z: 撤销</p><p><strong>Ctrl-/: 注释代码</strong></p><p>Ctrl-D: 删除整行</p><p>Ctrl-U: 撤销选择</p><p>Insert: 切换 重写标志</p><p>Ctrl-Home: 跳到单元格起始处</p><p>Ctrl-上: 跳到单元格起始处</p><p>Ctrl-End: 跳到单元格最后</p><p>Ctrl-下: 跳到单元格最后</p><p>Ctrl-左: 跳到单词左边</p><p>Ctrl-右: 跳到单词右边</p><p>Ctrl-删除: 删除前面的单词</p><p>Ctrl-Delete: 删除后面的单词</p><p>Ctrl-Y: 重做</p><p>Alt-U: 重新选择</p><p>Ctrl-M: 进入命令行模式</p><p>Ctrl-Shift-F: 打开命令配置</p><p>Ctrl-Shift-P: 打开命令配置</p><p>Esc: 进入命令行模式</p><p>Shift-Enter: 运行代码块, 选择下面的代码块</p><p>Ctrl-Enter: 运行选中的代码块</p><p>Alt-Enter: 运行代码块并且插入下面</p><p>Ctrl-Shift-Minus: 在鼠标出分割代码块</p><p>Ctrl-S: 保存并检查</p><p>下: 光标下移</p><p>上: 光标上移</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/" target="_blank" rel="noopener">Book《Python Data Science Handbook》</a></li><li><a href="https://github.com/jakevdp/PythonDataScienceHandbook" target="_blank" rel="noopener">https://github.com/jakevdp/PythonDataScienceHandbook</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2018/12/29/数据科学工具——Ipython/Jupyter Notebook.png&quot; alt=&quot;Jupyter Notebook&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 工欲善其事必先利其器，数据科学工具的整理是个循序渐进的过程，开个头立个Flag🏁，慢慢积累工具的使用方法，便于以后速查。主要涉及工具包括：Ipython，Numpy，Pandas，Scikit-Learn等。&lt;/p&gt;
    
    </summary>
    
      <category term="工具" scheme="http://blog.a-stack.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="教学" scheme="http://blog.a-stack.com/tags/%E6%95%99%E5%AD%A6/"/>
    
      <category term="JupyterNotebook" scheme="http://blog.a-stack.com/tags/JupyterNotebook/"/>
    
      <category term="Ipython" scheme="http://blog.a-stack.com/tags/Ipython/"/>
    
  </entry>
  
  <entry>
    <title>深度学习资源调度平台OpenPAI部署</title>
    <link href="http://blog.a-stack.com/2018/12/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0OpenPAI%E9%83%A8%E7%BD%B2/"/>
    <id>http://blog.a-stack.com/2018/12/26/深度学习资源调度平台OpenPAI部署/</id>
    <published>2018-12-26T10:42:53.000Z</published>
    <updated>2019-03-14T04:53:43.184Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/12/26/深度学习资源调度平台OpenPAI部署/sysarch.png" alt="OpenPAI"></p><a id="more"></a><blockquote><p>date: 2018-12-26</p><p>Version: 0.8.3</p><p>操作系统：Ubuntu 16.04</p><p>GitHub: <a href="https://github.com/Microsoft/pai" target="_blank" rel="noopener">https://github.com/Microsoft/pai</a></p><p>All-in-One部署</p></blockquote><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="1-环境要求"><a href="#1-环境要求" class="headerlink" title="1. 环境要求"></a>1. 环境要求</h3><ul><li>一组集群主机，PC或服务器均可；</li><li>所有节点全新安装<strong>Ubuntu Server 16.04</strong>，不需安装GPU驱动及CUDA；</li><li>各节点具有统一的登录账户及密码，此账户不需要root账户角色，但<strong>必须具有sudo权限</strong>；</li><li>各节点具有静态IP地址；</li><li>各节点均可访问互联网；</li><li>各节点间可互访；</li><li>各节点时间一致，即<strong>ntp</strong>功能可用；</li></ul><h3 id="2-依赖包安装"><a href="#2-依赖包安装" class="headerlink" title="2. 依赖包安装"></a>2. 依赖包安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0.1</span></span><br><span class="line">gaoc@openpaiTest03:~$ sudo apt-get -y update</span><br><span class="line"><span class="comment"># 0.2</span></span><br><span class="line">gaoc@openpaiTest03:~$ sudo apt-get -y install \</span><br><span class="line">      nano \</span><br><span class="line">      vim \</span><br><span class="line">      joe \</span><br><span class="line">      wget \</span><br><span class="line">      curl \</span><br><span class="line">      jq \</span><br><span class="line">      gawk \</span><br><span class="line">      psmisc \</span><br><span class="line">      python \</span><br><span class="line">      python-yaml \</span><br><span class="line">      python-jinja2 \</span><br><span class="line">      python-paramiko \</span><br><span class="line">      python-urllib3 \</span><br><span class="line">      python-tz \</span><br><span class="line">      python-nose \</span><br><span class="line">      python-prettytable \</span><br><span class="line">      python-netifaces \</span><br><span class="line">      python-dev \</span><br><span class="line">      python-pip \</span><br><span class="line">      python-mysqldb \</span><br><span class="line">      openjdk-8-jre \</span><br><span class="line">      openjdk-8-jdk \</span><br><span class="line">      openssh-server \</span><br><span class="line">      openssh-client \</span><br><span class="line">      git \</span><br><span class="line">      bash-completion \</span><br><span class="line">      inotify-tools \</span><br><span class="line">      rsync \</span><br><span class="line">      realpath \</span><br><span class="line">      net-tools</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 0.3</span></span><br><span class="line">gaoc@openpaiTest03:~$ pip install python-etcd docker kubernetes GitPython</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.4 </span></span><br><span class="line">gaoc@openpaiTest03:~$ git <span class="built_in">clone</span> https://github.com/Microsoft/pai.git</span><br></pre></td></tr></table></figure><blockquote><ul><li><p>0.3 pip包安装失败报错如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;   ...</span><br><span class="line">&gt;     File <span class="string">"/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py"</span>, line 118, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">&gt;       SSL_ST_INIT = _lib.SSL_ST_INIT</span><br><span class="line">&gt;   AttributeError: <span class="string">'module'</span> object has no attribute <span class="string">'SSL_ST_INIT'</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><p>  修复如下：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;   <span class="comment">#1.1</span></span><br><span class="line">&gt;   sudo python -m easy_install --upgrade pyOpenSSL</span><br><span class="line">&gt;   <span class="comment">#1.2 更新pip版本</span></span><br><span class="line">&gt;   gaoc@openpaiTest03:~$ pip install --upgrade pip</span><br><span class="line">&gt;   <span class="comment">#1.3 修复pip新版本bug </span></span><br><span class="line">&gt;   sudo vim /usr/bin/pip</span><br><span class="line">&gt;   </span><br><span class="line">&gt;   from pip import __main__</span><br><span class="line">&gt;   <span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">&gt;       sys.exit(__main__._main())</span><br><span class="line">&gt;   <span class="comment">#1.4 重新安装包</span></span><br><span class="line">&gt;   gaoc@openpaiTest03:~$ pip install python-etcd docker kubernetes GitPython</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="配置文件准备"><a href="#配置文件准备" class="headerlink" title="配置文件准备"></a>配置文件准备</h2><ol><li>使用快速部署文件<code>quick-start.yaml</code></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gaoc@openpaiTest03:~$ <span class="built_in">cd</span> pai/deployment/quick-start/</span><br><span class="line">gaoc@openpaiTest03:~/pai/deployment/quick-start$ mv quick-start-example.yaml quick-start.yaml</span><br></pre></td></tr></table></figure><ol><li><p>修改<code>quick-start.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.</span></span><br><span class="line">machines:</span><br><span class="line">  - &lt;ip-of-master&gt;</span><br><span class="line">  - &lt;ip-of-worker1&gt;</span><br><span class="line">  - &lt;ip-of-worder2&gt;</span><br><span class="line"><span class="comment">#--------------------</span></span><br><span class="line"><span class="comment">#修改为本机内网IP地址，因为All-in-One,所以配置一个master IP即可：</span></span><br><span class="line">machines:</span><br><span class="line">  - 10.0.7.4</span><br><span class="line"><span class="comment">#2.</span></span><br><span class="line">ssh-username: &lt;username&gt;</span><br><span class="line">ssh-password: &lt;password&gt;</span><br><span class="line"><span class="comment">#--------------------</span></span><br><span class="line"><span class="comment">#修改为openPAI用户的用户名/密码</span></span><br></pre></td></tr></table></figure></li><li><p>生成配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /pai</span><br><span class="line"></span><br><span class="line"><span class="comment"># cmd should be executed under pai directory</span></span><br><span class="line"></span><br><span class="line">python paictl.py config generate -i ~/pai/deployment/quick-start/quick-start.yaml -o ~/pai-config -f</span><br></pre></td></tr></table></figure></li><li><p>修改生成的配置文件</p><blockquote><p>目录 <code>~/pai-config</code></p></blockquote><p><code>cluster-configuration.yaml  k8s-role-definition.yaml  kubernetes-configuration.yaml  services-configuration.yaml</code></p></li></ol><ul><li><p><code>`cluster-configuration.yaml</code></p><p>修改<code>machine-sku</code>,如果有需要进一步修改<code>machine-list</code></p><p>因此部署电脑没有GPU，将GPU的<code>count</code>调整为0，同时按照服务器内存和cpu虚拟核心数目调整参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">machine-sku:</span><br><span class="line">  GENERIC:</span><br><span class="line">    mem: 56</span><br><span class="line">    gpu:</span><br><span class="line">      <span class="built_in">type</span>: generic</span><br><span class="line">      count: 0</span><br><span class="line">    cpu:</span><br><span class="line">      vcore: 16</span><br><span class="line">    os: ubuntu16.04</span><br><span class="line"></span><br><span class="line">machine-list:</span><br><span class="line">  - hostname: openpaiTest03</span><br><span class="line">    hostip: 10.0.7.4</span><br><span class="line">    machine-type: GENERIC</span><br><span class="line">    k8s-role: master</span><br><span class="line">    etcdid: etcdid1</span><br><span class="line">    zkid: <span class="string">"1"</span></span><br><span class="line">    dashboard: <span class="string">"true"</span></span><br><span class="line">    pai-master: <span class="string">"true"</span></span><br><span class="line">    pai-worker: <span class="string">"true"</span></span><br><span class="line">    docker-data: /var/lib/docker</span><br></pre></td></tr></table></figure></li><li><p><code>kubernetes-configuration.yaml</code></p><p>修改docker registry为国内源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-registry: docker.io/openpai</span><br></pre></td></tr></table></figure></li><li><p><code>services-configuration.yaml</code></p><p>修改<code>docker-registry</code>的<code>tag</code>值为openPAI版本号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tag: 0.8.3</span><br></pre></td></tr></table></figure></li></ul><p>更多配置修改详见： <a href="https://github.com/Microsoft/pai/blob/master/docs/pai-management/doc/how-to-generate-cluster-config.md" target="_blank" rel="noopener">https://github.com/Microsoft/pai/blob/master/docs/pai-management/doc/how-to-generate-cluster-config.md</a></p><h2 id="部署Kubernetes-集群"><a href="#部署Kubernetes-集群" class="headerlink" title="部署Kubernetes 集群"></a>部署Kubernetes 集群</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/pai</span><br><span class="line"></span><br><span class="line">sudo python paictl.py cluster k8s-bootup -p ~/pai-config</span><br></pre></td></tr></table></figure><p>集群部署完成可通过如下地址访问集群管理系统：</p><p><code>http://&lt;master&gt;:9090</code></p><p>部署过程中遇到问题重新部署前先执行如下命令删掉<code>kubelet</code>的容器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo docker ps</span><br><span class="line">sudo docker stop kubelet</span><br><span class="line">sudo docker rm kubelet</span><br></pre></td></tr></table></figure><h2 id="更新集群配置"><a href="#更新集群配置" class="headerlink" title="更新集群配置"></a>更新集群配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/pai</span><br><span class="line">python paictl.py config push -p ~/pai-config</span><br></pre></td></tr></table></figure><p>执行完成之后会提示输入一个cluster id：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Please input the cluster-id <span class="keyword">for</span> your cluster: 123456</span><br></pre></td></tr></table></figure><h2 id="启动OpenPAI服务"><a href="#启动OpenPAI服务" class="headerlink" title="启动OpenPAI服务"></a>启动OpenPAI服务</h2><blockquote><p>此步骤将依次安装如下服务：</p><p><code>[&#39;watchdog&#39;, &#39;node-exporter&#39;, &#39;yarn-frameworklauncher&#39;, &#39;hadoop-name-node&#39;, &#39;cleaner&#39;, &#39;rest-server&#39;, &#39;grafana&#39;, &#39;hadoop-resource-manager&#39;, &#39;hadoop-batch-job&#39;, &#39;drivers&#39;, &#39;cluster-configuration&#39;, &#39;alert-manager&#39;, &#39;pylon&#39;, &#39;hadoop-jobhistory&#39;, &#39;hadoop-node-manager&#39;, &#39;end-to-end-test&#39;, &#39;job-exporter&#39;, &#39;webportal&#39;, &#39;zookeeper&#39;, &#39;prometheus&#39;, &#39;hadoop-data-node&#39;]</code></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/pai</span><br><span class="line">python paictl.py service start</span><br></pre></td></tr></table></figure><p>此时可以通过kebernetes的管理界面查看服务所在容器的部署情况：<code>http://127.0.0.1:9090/#!/job?namespace=default</code></p><p><img src="/2018/12/26/深度学习资源调度平台OpenPAI部署/k8s-management.png" alt="k8s-management"></p><p>在此步骤中遇到了<code>watchdog</code>无法部署成功的问题，指定的配置文件版本无法找到对应0.8.3的watchdog：</p><p><img src="/2018/12/26/深度学习资源调度平台OpenPAI部署/watchdog-error.png" alt="watchdog-error"></p><ul><li><p>解决办法</p><ul><li><p>通过如下命令删除已经部署的watchdog容器</p></li><li><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python paictl.py service delete -n watchdog</span><br></pre></td></tr></table></figure><p>更新<code>pai-config</code>中的配置文件<code>services-configuration.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">namespace: xudifsd</span><br><span class="line">tag: latest</span><br></pre></td></tr></table></figure></li><li><p>重新执行配置更新命令</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/pai</span><br><span class="line">python paictl.py<span class="built_in"> config </span>push -p ~/pai-config</span><br></pre></td></tr></table></figure></li><li><p>重启启动服务部署命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/pai</span><br><span class="line">python paictl.py service start</span><br></pre></td></tr></table></figure></li></ul></li></ul><blockquote><p><code>end-to-end-test</code>服务可以通过如下命令删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;python paictl.py service delete -n end-to-end-test</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>最后，可以通过如下链接的方法验证部署成功与否：</p><p><a href="https://github.com/Microsoft/pai/blob/master/docs/pai-management/doc/validate-deployment.md" target="_blank" rel="noopener">https://github.com/Microsoft/pai/blob/master/docs/pai-management/doc/validate-deployment.md</a></p><h4 id="删掉整个集群"><a href="#删掉整个集群" class="headerlink" title="删掉整个集群"></a>删掉整个集群</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python paictl.py service delete</span><br><span class="line">sudo python paictl.py cluster k8s-clean -p ~/pai-config/</span><br><span class="line"></span><br><span class="line"><span class="comment">#optional</span></span><br><span class="line">sudo rm –rf /var/etcd/data</span><br></pre></td></tr></table></figure><h2 id="其它更新"><a href="#其它更新" class="headerlink" title="其它更新"></a>其它更新</h2><h3 id="2019-03-14-更新"><a href="#2019-03-14-更新" class="headerlink" title="2019/03/14 更新"></a>2019/03/14 更新</h3><blockquote><p>最近在一台新的虚拟机环境部署v0.9.1版本，遇到了 <code>pylon</code>hung住的问题，详见github提交的issue: <a href="https://github.com/Microsoft/pai/issues/2282" target="_blank" rel="noopener">https://github.com/Microsoft/pai/issues/2282</a></p><p>后来发现是Linux默认配置的apache服务占用了80端口，导致docker-proxy抢占端口失败，pylon无法启动，停掉apache服务或修改pylon的端口(<code>services-configuration.yaml</code>)可以解决问题。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2018/12/26/深度学习资源调度平台OpenPAI部署/sysarch.png&quot; alt=&quot;OpenPAI&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手实践营" scheme="http://blog.a-stack.com/categories/%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E8%90%A5/"/>
    
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="部署" scheme="http://blog.a-stack.com/tags/%E9%83%A8%E7%BD%B2/"/>
    
      <category term="资源调度" scheme="http://blog.a-stack.com/tags/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/"/>
    
      <category term="开源平台" scheme="http://blog.a-stack.com/tags/%E5%BC%80%E6%BA%90%E5%B9%B3%E5%8F%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习——EDA</title>
    <link href="http://blog.a-stack.com/2018/12/11/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94EDA/"/>
    <id>http://blog.a-stack.com/2018/12/11/机器在学习——EDA/</id>
    <published>2018-12-11T06:46:47.000Z</published>
    <updated>2018-12-12T12:38:55.539Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/12/11/机器在学习——EDA/pairplot.png" alt="EDA"></p><a id="more"></a><hr><h1 id="Topic-1：-EDA-with-pandas"><a href="#Topic-1：-EDA-with-pandas" class="headerlink" title="Topic 1： EDA with pandas"></a>Topic 1： EDA with pandas</h1><ol><li><p>调整column类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'Churn'</span>] = df[<span class="string">'Churn'</span>].astype(<span class="string">'int64'</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>df.describe()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.describe(include=[<span class="string">'object'</span>, <span class="string">'bool'</span>])</span><br></pre></td></tr></table></figure></li><li><p><code>df.value_counts(normalize=True)</code></p><blockquote><p>对于离散量数据（<code>object</code>或<code>bool</code>）统计不同类型的数目</p></blockquote><p>另外可以通过如下命令来统计一个离散属性的所有类别数目：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flights_df[<span class="string">'UniqueCarrier'</span>].nunique()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flights_df.groupby(<span class="string">'UniqueCarrier'</span>).size().plot(kind=<span class="string">'bar'</span>);</span><br><span class="line"><span class="comment">#等价于</span></span><br><span class="line">flights_df[<span class="string">'UniqueCarrier'</span>].value_counts().plot(kind=<span class="string">'bar'</span>)</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>排序方法：<code>sort_values(by=&#39;...&#39;,ascending=False)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.sort_values(by=<span class="string">'Total day charge'</span>, ascending=<span class="keyword">False</span>).head()</span><br><span class="line"></span><br><span class="line">df.sort_values(by=[<span class="string">'Churn'</span>, <span class="string">'Total day charge'</span>],</span><br><span class="line">        ascending=[<span class="keyword">True</span>, <span class="keyword">False</span>])</span><br></pre></td></tr></table></figure></li><li><p>Boolean indexing：<code>df[P(df[&#39;Name&#39;])]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[(df[<span class="string">'Churn'</span>] == <span class="number">0</span>) &amp; (df[<span class="string">'International plan'</span>] == <span class="string">'No'</span>)][<span class="string">'Total intl minutes'</span>].max()</span><br></pre></td></tr></table></figure></li><li><p><code>.loc</code>与<code>.iloc</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.loc[<span class="number">0</span>:<span class="number">5</span>, <span class="string">'State'</span>:<span class="string">'Area code'</span>]</span><br><span class="line">df.iloc[<span class="number">0</span>:<span class="number">5</span>, <span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">df[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure></li><li><p><code>apply</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将apply应用到每一行，注意添加 axis=1</span></span><br><span class="line">df[<span class="string">'State'</span>].apply(<span class="keyword">lambda</span> state: state[<span class="number">0</span>] == <span class="string">'W'</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>map</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'No'</span> : <span class="keyword">False</span>, <span class="string">'Yes'</span> : <span class="keyword">True</span>&#125;</span><br><span class="line">df[<span class="string">'International plan'</span>] = df[<span class="string">'International plan'</span>].map(d)</span><br></pre></td></tr></table></figure></li><li><p><code>Grouping</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.groupby(by=grouping_columns)[columns_to_show].function()</span><br><span class="line"></span><br><span class="line">columns_to_show = [<span class="string">'Total day minutes'</span>, <span class="string">'Total eve minutes'</span>, </span><br><span class="line">                   <span class="string">'Total night minutes'</span>]</span><br><span class="line"></span><br><span class="line">df.groupby([<span class="string">'Churn'</span>])[columns_to_show].agg([np.mean, np.std, np.min, </span><br><span class="line">                                            np.max])</span><br></pre></td></tr></table></figure></li><li><p>Summary Tables</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.crosstab(df[<span class="string">'Churn'</span>], df[<span class="string">'International plan'</span>])</span><br></pre></td></tr></table></figure></li></ol><h1 id="Topic-2：-EDA-with-plot"><a href="#Topic-2：-EDA-with-plot" class="headerlink" title="Topic 2： EDA with plot"></a>Topic 2： EDA with plot</h1><blockquote><p>收集 ML toolbox： <code>pandas</code>,<code>matplotlib</code>,<code>seaborn</code></p></blockquote><h2 id="1-看数据分布"><a href="#1-看数据分布" class="headerlink" title="1. 看数据分布"></a>1. 看数据分布</h2><h3 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h3><blockquote><p>使用<code>pandas</code> DataFrame的<code>hist()</code>方法直接绘制：将数据划分至等长区间(bins)。</p><p>用于统计数据分布，在不同区间的计数值；</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[features].hist(figsize=(<span class="number">10</span>, <span class="number">4</span>));</span><br></pre></td></tr></table></figure><h3 id="Box图"><a href="#Box图" class="headerlink" title="Box图"></a>Box图</h3><p>箱形图（Box-plot）又称为盒须图、盒式图或箱线图，是一种用作显示一组数据分散情况资料的统计图。它能显示出一组数据的<strong>最大值</strong>、<strong>最小值</strong>、<strong>中位数</strong>及<strong>上下四分位数</strong>。因形状如箱子而得名。在各种领域也经常被使用，常见于品质管理。</p><p>接下来我们介绍Seaborn中的箱型图的具体实现方法，这是boxplot的API：</p><blockquote><p>seaborn.boxplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, dodge=True, fliersize=5, linewidth=None, whis=1.5, notch=False, ax=None, **kwargs)</p></blockquote><h3 id="Kernel-Density-Plots"><a href="#Kernel-Density-Plots" class="headerlink" title="Kernel Density Plots"></a>Kernel Density Plots</h3><blockquote><p>看作为直方图的平滑版本</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[features].plot(kind=<span class="string">'density'</span>, subplots=<span class="keyword">True</span>, layout=(<span class="number">1</span>, <span class="number">2</span>), </span><br><span class="line">                  sharex=<span class="keyword">False</span>, figsize=(<span class="number">10</span>, <span class="number">4</span>));</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/kernel-density-plots.png" alt="kernel-density-plots"></p><h3 id="Seaborn的displot方法"><a href="#Seaborn的displot方法" class="headerlink" title="Seaborn的displot方法"></a>Seaborn的<code>displot</code>方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(df[<span class="string">'Total intl calls'</span>]);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/sns.distplot.png" alt="sns.distplot"></p><h3 id="pandas的describe-方法"><a href="#pandas的describe-方法" class="headerlink" title="pandas的describe()方法"></a><code>pandas</code>的<code>describe()</code>方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[features].describe()</span><br></pre></td></tr></table></figure><blockquote><p>也可以通过seaborn的box方法</p></blockquote><h2 id="2-离散变量"><a href="#2-离散变量" class="headerlink" title="2. 离散变量"></a>2. 离散变量</h2><h3 id="pandas的value-counts"><a href="#pandas的value-counts" class="headerlink" title="pandas的value_counts"></a><code>pandas</code>的<code>value_counts</code></h3><blockquote><p> <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html" target="_blank" rel="noopener"><code>value_counts()</code></a> </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'feature_name'</span>].value_counts(normalize=Fase)</span><br></pre></td></tr></table></figure><h3 id="seaborn的countplot"><a href="#seaborn的countplot" class="headerlink" title="seaborn的countplot"></a><code>seaborn</code>的<code>countplot</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, axes = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">sns.countplot(x=<span class="string">'Churn'</span>, data=df, ax=axes[<span class="number">0</span>]);</span><br><span class="line">sns.countplot(x=<span class="string">'Customer service calls'</span>, data=df, ax=axes[<span class="number">1</span>]);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/barplot.png" alt="barplot"></p><h3 id="lmplot"><a href="#lmplot" class="headerlink" title="lmplot"></a><code>lmplot</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.lmplot(<span class="string">'Total day minutes'</span>, <span class="string">'Total night minutes'</span>, data=df, hue=<span class="string">'Churn'</span>, fit_reg=<span class="keyword">False</span>);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/lmplot.png" alt="lmplot"></p><h2 id="3-关联分析"><a href="#3-关联分析" class="headerlink" title="3. 关联分析"></a>3. 关联分析</h2><h3 id="相关性分析"><a href="#相关性分析" class="headerlink" title="相关性分析"></a>相关性分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Drop non-numerical variables</span></span><br><span class="line">numerical = list(set(df.columns) - </span><br><span class="line">                 set([<span class="string">'State'</span>, <span class="string">'International plan'</span>, <span class="string">'Voice mail plan'</span>, </span><br><span class="line">                      <span class="string">'Area code'</span>, <span class="string">'Churn'</span>, <span class="string">'Customer service calls'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate and plot</span></span><br><span class="line">corr_matrix = df[numerical].corr()</span><br><span class="line">sns.heatmap(corr_matrix);</span><br></pre></td></tr></table></figure><h3 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h3><blockquote><p><a href="https://seaborn.pydata.org/generated/seaborn.jointplot.html" target="_blank" rel="noopener"><code>jointplot()</code></a> </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.jointplot(x=<span class="string">'Total day minutes'</span>, y=<span class="string">'Total night minutes'</span>, </span><br><span class="line">              data=df, kind=<span class="string">'scatter'</span>);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/scatter.png" alt="scatter"></p><h4 id="散点图的相关性矩阵：-pairplot"><a href="#散点图的相关性矩阵：-pairplot" class="headerlink" title="散点图的相关性矩阵： pairplot"></a>散点图的相关性矩阵： <code>pairplot</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `pairplot()` may become very slow with the SVG format</span></span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">sns.pairplot(df[numerical]);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/pairplot.png" alt="pairplot"></p><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><h2 id="技巧总结"><a href="#技巧总结" class="headerlink" title="技巧总结"></a>技巧总结</h2><ol><li><p><code>seaborn</code>的<code>hue</code>属性是一个可视化很直接，比较有用的配置属性；</p><p>比如通过下图来看某个特征对于结果的不同分布：</p><p><img src="/2018/12/11/机器在学习——EDA/count-plot-hue.png" alt="count-plot-hue"></p></li><li><p>在量化离散变量之前，将数值变量和类别变量分别进行分析</p></li><li><p>环境准备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Matplotlib forms basis for visualization in Python</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># We will use the Seaborn library</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Graphics in SVG format are more sharp and legible</span></span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Increase the default plot size and set the color scheme</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = <span class="number">8</span>, <span class="number">5</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'viridis'</span></span><br></pre></td></tr></table></figure></li><li><p>熟练使用特征选择命令，限定画图范围</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top_features = df[<span class="string">'features'</span>].value_counts().sort_values(ascending=<span class="keyword">False</span>).head(<span class="number">5</span>).index.values</span><br></pre></td></tr></table></figure></li><li><p>善用<code>pandas.crosstab</code>进行分析</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2018/12/11/机器在学习——EDA/pairplot.png&quot; alt=&quot;EDA&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-时序数据分析</title>
    <link href="http://blog.a-stack.com/2018/12/04/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0-%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://blog.a-stack.com/2018/12/04/机器在学习-时序数据分析/</id>
    <published>2018-12-04T08:50:11.000Z</published>
    <updated>2018-12-11T04:00:45.956Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="Test Picture"></p><a id="more"></a><h2 id="算法理论"><a href="#算法理论" class="headerlink" title="算法理论"></a>算法理论</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><blockquote><p><strong>Time series</strong> is a series of data points indexed (or listed or graphed) in time order. </p></blockquote><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination" target="_blank" rel="noopener">R squared</a>: coefficient of determination (in econometrics, this can be interpreted as the percentage of variance explained by the model), $(-\infty, 1]$</li></ul><script type="math/tex; mode=display">R^2 = 1 - \frac{SS_{res}}{SS_{tot}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.r2_score</span><br></pre></td></tr></table></figure><hr><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error" target="_blank" rel="noopener">Mean Absolute Error</a>: this is an interpretable metric because it has the same unit of measurment as the initial series, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MAE = \frac{\sum\limits_{i=1}^{n} |y_i - \hat{y}_i|}{n}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_absolute_error</span><br></pre></td></tr></table></figure><hr><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error" target="_blank" rel="noopener">Median Absolute Error</a>: again, an interpretable metric that is particularly interesting because it is robust to outliers, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MedAE = median(|y_1 - \hat{y}_1|, ... , |y_n - \hat{y}_n|)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.median_absolute_error</span><br></pre></td></tr></table></figure><hr><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error" target="_blank" rel="noopener">Mean Squared Error</a>: the most commonly used metric that gives a higher penalty to large errors and vice versa, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MSE = \frac{1}{n}\sum\limits_{i=1}^{n} (y_i - \hat{y}_i)^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error</span><br></pre></td></tr></table></figure><hr><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error" target="_blank" rel="noopener">Mean Squared Logarithmic Error</a>: practically, this is the same as MSE, but we take the logarithm of the series. As a result, we give more weight to small mistakes as well. This is usually used when the data has exponential trends, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MSLE = \frac{1}{n}\sum\limits_{i=1}^{n} (log(1+y_i) - log(1+\hat{y}_i))^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_log_error</span><br></pre></td></tr></table></figure><hr><ul><li>Mean Absolute Percentage Error: this is the same as MAE but is computed as a percentage, which is very convenient when you want to explain the quality of the model to management, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MAPE = \frac{100}{n}\sum\limits_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{y_i}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_absolute_percentage_error</span><span class="params">(y_true, y_pred)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> np.mean(np.abs((y_true - y_pred) / y_true)) * <span class="number">100</span></span><br></pre></td></tr></table></figure><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="使用相关模型库"><a href="#使用相关模型库" class="headerlink" title="使用相关模型库"></a>使用相关模型库</h3><ol><li><p><a href="http://statsmodels.sourceforge.net/stable/" target="_blank" rel="noopener">statsmodels</a> library </p><blockquote><p>R的统计学习库，被移植到了python中，包含对于时序数据分析的各种函数</p></blockquote></li><li></li></ol><h3 id="滑动平均"><a href="#滑动平均" class="headerlink" title="滑动平均"></a>滑动平均</h3><p><a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html" target="_blank" rel="noopener"><code>DataFrame.rolling(window).mean()</code></a> </p><h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><h3 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h3><p>随机森林支持树的并行构建和预测结果的并行计算，这可以通过 <code>n_jobs</code> 参数实现。 如果设置 <code>n_jobs = k</code> ，则计算被划分为 <code>k</code> 个作业，并运行在机器的 <code>k</code> 个核上。 如果设置 <code>n_jobs = -1</code> ，则使用机器的所有核。 注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 <code>k</code> 个作业不会快 <code>k</code> 倍）。 当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>《机器学习》——周志华</p></li><li><p><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</p></li></ol><p>3.<a href="http://sklearn.apachecn.org/cn/0.19.0/" target="_blank" rel="noopener">Sklearn文档</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="时序数据" scheme="http://blog.a-stack.com/tags/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Time Series" scheme="http://blog.a-stack.com/tags/Time-Series/"/>
    
      <category term="SARIMA" scheme="http://blog.a-stack.com/tags/SARIMA/"/>
    
      <category term="Prophet" scheme="http://blog.a-stack.com/tags/Prophet/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-线性回归与分类</title>
    <link href="http://blog.a-stack.com/2018/11/29/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://blog.a-stack.com/2018/11/29/机器在学习-随机森林/</id>
    <published>2018-11-29T08:50:11.000Z</published>
    <updated>2018-12-11T04:04:47.431Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="Test Picture"></p><h2 id="算法理论"><a href="#算法理论" class="headerlink" title="算法理论"></a>算法理论</h2><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p><strong>集成方法</strong> 的目标是把多个使用给定学习算法构建的基估计器的预测结果结合起来，从而获得比单个估计器更好的泛化能力/鲁棒性。</p><p>集成方法通常分为两种:</p><ul><li><p><strong>平均方法</strong>，该方法的原理是构建多个独立的估计器，然后取它们的预测结果的平均。一般来说组合之后的估计器是会比单个估计器要好的，因为它的方差减小了。</p><p><strong>示例:</strong> <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#bagging" target="_blank" rel="noopener">Bagging 方法</a> , <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#forest" target="_blank" rel="noopener">随机森林</a> , …</p></li><li><p>相比之下，在 <strong>boosting 方法</strong> 中，基估计器是依次构建的，并且每一个基估计器都尝试去减少组合估计器的偏差。这种方法主要目的是为了结合多个弱模型，使集成的模型更加强大。</p><p><strong>示例:</strong> <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#adaboost" target="_blank" rel="noopener">AdaBoost</a> , <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#gradient-boosting" target="_blank" rel="noopener">梯度提升树</a> , …</p><a id="more"></a></li></ul><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>在随机森林中（参见 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" target="_blank" rel="noopener"><code>RandomForestClassifier</code></a> 和 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" target="_blank" rel="noopener"><code>RandomForestRegressor</code></a> 类）， 集成模型中的每棵树构建时的样本都是由训练集经过有放回抽样得来的（例如，自助采样法-bootstrap sample）。 另外，在构建树的过程中进行结点分割时，选择的分割点不再是所有特征中最佳分割点，而是特征的一个随机子集中的最佳分割点。 由于这种随机性，森林的偏差通常会有略微的增大（相对于单个非随机树的偏差），但是由于取了平均，其方差也会减小，通常能够补偿偏差的增加，从而产生一个总体上更好的模型。 </p><script type="math/tex; mode=display">\large p_{+} = \frac{OR_{+}}{1 + OR_{+}} = \frac{\exp^{\textbf{w}^\text{T}\textbf{x}}}{1 + \exp^{\textbf{w}^\text{T}\textbf{x}}} = \frac{1}{1 + \exp^{-\textbf{w}^\text{T}\textbf{x}}} = \sigma(\textbf{w}^\text{T}\textbf{x})</script><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>要调整的参数主要是 <code>n_estimators</code> 和 <code>max_features</code>。 前者（n_estimators）是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好。 后者（max_features）是分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多。 根据经验，回归问题中使用 <code>max_features = n_features</code>， 分类问题使用 <code>max_features = sqrt（n_features）</code> （其中 <code>n_features</code> 是特征的个数）是比较好的默认值。 <code>max_depth = None</code> 和 <code>min_samples_split = 2</code> 结合通常会有不错的效果（即生成完全的树）。 请记住，这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由交叉验证获得。 另外，请注意，在随机森林中，默认使用自助采样法（<code>bootstrap = True</code>）， 然而 extra-trees 的默认策略是使用整个数据集（<code>bootstrap = False</code>）。 当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外的样本来估算的，设置 <code>oob_score = True</code> 即可实现。</p><blockquote><p><strong>提示:</strong></p><p>默认参数下模型复杂度是：<code>O(M*N*log(N))</code> ， 其中 <code>M</code> 是树的数目， <code>N</code> 是样本数。 可以通过设置以下参数来降低模型复杂度： <code>min_samples_split</code> , <code>min_samples_leaf</code> , <code>max_leaf_nodes`` 和 ``max_depth</code> 。</p></blockquote><h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><h3 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h3><p>随机森林支持树的并行构建和预测结果的并行计算，这可以通过 <code>n_jobs</code> 参数实现。 如果设置 <code>n_jobs = k</code> ，则计算被划分为 <code>k</code> 个作业，并运行在机器的 <code>k</code> 个核上。 如果设置 <code>n_jobs = -1</code> ，则使用机器的所有核。 注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 <code>k</code> 个作业不会快 <code>k</code> 倍）。 当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>《机器学习》——周志华</p></li><li><p><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</p></li></ol><p>3.<a href="http://sklearn.apachecn.org/cn/0.19.0/" target="_blank" rel="noopener">Sklearn文档</a></p><p>- </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;算法理论&quot;&gt;&lt;a href=&quot;#算法理论&quot; class=&quot;headerlink&quot; title=&quot;算法理论&quot;&gt;&lt;/a&gt;算法理论&lt;/h2&gt;&lt;h3 id=&quot;集成学习&quot;&gt;&lt;a href=&quot;#集成学习&quot; class=&quot;headerlink&quot; title=&quot;集成学习&quot;&gt;&lt;/a&gt;集成学习&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;集成方法&lt;/strong&gt; 的目标是把多个使用给定学习算法构建的基估计器的预测结果结合起来，从而获得比单个估计器更好的泛化能力/鲁棒性。&lt;/p&gt;
&lt;p&gt;集成方法通常分为两种:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;平均方法&lt;/strong&gt;，该方法的原理是构建多个独立的估计器，然后取它们的预测结果的平均。一般来说组合之后的估计器是会比单个估计器要好的，因为它的方差减小了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例:&lt;/strong&gt; &lt;a href=&quot;http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#bagging&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bagging 方法&lt;/a&gt; , &lt;a href=&quot;http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#forest&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;随机森林&lt;/a&gt; , …&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;相比之下，在 &lt;strong&gt;boosting 方法&lt;/strong&gt; 中，基估计器是依次构建的，并且每一个基估计器都尝试去减少组合估计器的偏差。这种方法主要目的是为了结合多个弱模型，使集成的模型更加强大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例:&lt;/strong&gt; &lt;a href=&quot;http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#adaboost&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AdaBoost&lt;/a&gt; , &lt;a href=&quot;http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#gradient-boosting&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;梯度提升树&lt;/a&gt; , …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-KNN</title>
    <link href="http://blog.a-stack.com/2018/11/23/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0%E2%80%94-KNN/"/>
    <id>http://blog.a-stack.com/2018/11/23/机器在学习—-KNN/</id>
    <published>2018-11-23T08:50:37.000Z</published>
    <updated>2018-12-11T10:30:24.216Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="KNN"></p><a id="more"></a><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/classes.html#module-sklearn.neighbors" target="_blank" rel="noopener"><code>sklearn.neighbors</code></a> 提供了 neighbors-based (基于邻居的) 无监督学习以及监督学习方法的功能。 无监督的最近邻是许多其它学习方法的基础，尤其是 manifold learning (流行学习) 和 spectral clustering (谱聚类)。 neighbors-based (基于邻居的) 监督学习分为两种： <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html#classification" target="_blank" rel="noopener">classification</a> （分类）针对的是具有离散标签的数据，<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html#regression" target="_blank" rel="noopener">regression</a> （回归）针对的是具有连续标签的数据。 </p><ol><li>Calculate the distance to each of the samples in the training set.</li><li>Select 𝑘k samples from the training set with the minimal distance to them.</li><li>The class of the test sample will be the most frequent class among those 𝑘k nearest neighbors.</li></ol><blockquote><p>在Kaggle比赛中，KNN经常作为前期特征生成的算法；</p><p>算法处理有两个关键：</p><ul><li>K的选取；</li><li>距离的评价准则（Hamming，Euclidean，cosine, Minkowski distance）</li><li>所有属性必须进行归一化操作，保证距离计算的一致性</li></ul></blockquote><h3 id="K-D-树¶"><a href="#K-D-树¶" class="headerlink" title="K-D 树¶"></a>K-D 树<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html#k-d" target="_blank" rel="noopener">¶</a></h3><p>为了解决效率低下的暴力计算方法，已经发明了大量的基于树的数据结构。总的来说， 这些结构试图通过有效地编码样本的 aggregate distance (聚合距离) 信息来减少所需的距离计算量。 基本思想是，若A点距离B点非常远，B距离C点非常近， 可知 A点与 C点很遥远，<em>不需要明确计算它们的距离</em>。 通过这样的方式，近邻搜索的计算成本可以降低为 $O[DNlog(N)]$或更低。 这是对于暴力搜索在大样本数 N中表现的显著改善。</p><p>利用这种聚合信息的早期方法是 <em>KD tree</em> 数据结构（<em> K-dimensional tree</em> 的简写）, 它将二维 <em>Quad-trees</em> 和三维 <em>Oct-trees</em>推广到任意数量的维度. KD 树是一个二叉树结构，它沿着数据轴递归地划分参数空间，将其划分为嵌入数据点的嵌套的各向异性区域。 KD 树的构造非常快：因为只需沿数据轴执行分区, 无需计算 D-dimensional 距离。 一旦构建完成, 查询点的最近邻距离计算复杂度仅为 $O[log(N)]$。 虽然 KD 树的方法对于低维度 （D&lt;20）近邻搜索非常快, 当D增长到很大时, 效率变低: 这就是所谓的 “维度灾难” 的一种体现。 在 scikit-learn 中, KD 树近邻搜索可以使用关键字 <code>algorithm = &#39;kd_tree&#39;</code> 来指定, 并且使用类 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" target="_blank" rel="noopener"><code>KDTree</code></a> 来计算。</p><h2 id="工程实现"><a href="#工程实现" class="headerlink" title="工程实现"></a>工程实现</h2><p>The main parameters of the class <code>sklearn.neighbors.KNeighborsClassifier</code> are:</p><ul><li>weights: <code>uniform</code> (all weights are equal), <code>distance</code> (the weight is inversely proportional to the distance from the test sample), or any other user-defined function;</li><li>algorithm (optional): <code>brute</code>, <code>ball_tree</code>, <code>KD_tree</code>, or <code>auto</code>. In the first case, the nearest neighbors for each test case are computed by a grid search over the training set. In the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. If you set this parameter to <code>auto</code>, the right way to find the neighbors will be automatically chosen based on the training set.</li><li>leaf_size (optional): threshold for switching to grid search if the algorithm for finding neighbors is BallTree or KDTree;</li><li>metric: <code>minkowski</code>, <code>manhattan</code>, <code>euclidean</code>, <code>chebyshev</code>, or other.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">knn_pipe = Pipeline([(<span class="string">'scaler'</span>, StandardScaler()), (<span class="string">'knn'</span>, KNeighborsClassifier(n_jobs=<span class="number">-1</span>))])</span><br><span class="line"></span><br><span class="line">knn_params = &#123;<span class="string">'knn__n_neighbors'</span>: range(<span class="number">1</span>, <span class="number">10</span>)&#125;</span><br><span class="line"></span><br><span class="line">knn_grid = GridSearchCV(knn_pipe, knn_params,</span><br><span class="line">                        cv=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">knn_grid.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">knn_grid.best_params_, knn_grid.best_score_</span><br><span class="line"></span><br><span class="line">accuracy_score(y_holdout, knn_grid.predict(X_holdout))</span><br></pre></td></tr></table></figure><h2 id="KNN的优缺点"><a href="#KNN的优缺点" class="headerlink" title="KNN的优缺点"></a>KNN的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>实现简单，理论完备；</li><li>一般作为机器学习使用的第一个测试算法；</li><li>可解释性强；</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>处理大数据训练样本效率低；</li><li>难以处理特征数目多的数据集；</li><li>对距离测量算法依赖高；</li><li>K的选择也需要反复尝试；</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</li></ol><p>2.<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html" target="_blank" rel="noopener">Sklearn 最近邻</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;KNN&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-线性回归与分类</title>
    <link href="http://blog.a-stack.com/2018/11/23/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://blog.a-stack.com/2018/11/23/机器在学习-线性回归/</id>
    <published>2018-11-23T08:50:11.000Z</published>
    <updated>2018-11-26T10:08:04.170Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="Test Picture"></p><h2 id="算法理论"><a href="#算法理论" class="headerlink" title="算法理论"></a>算法理论</h2><script type="math/tex; mode=display">y = W^TX + b</script><p>更一般地，</p><script type="math/tex; mode=display">\large \textbf y = \textbf X \textbf w + \epsilon</script><p>其中 $\epsilon $ 为噪声，均值为0，方差服从正态分布。</p><ul><li>expectation of random errors is zero:  $\forall i: \mathbb{E}\left[\epsilon_i\right] = 0 $;</li><li>the random error has the same finite variance, this property is called <a href="https://en.wikipedia.org/wiki/Homoscedasticity" target="_blank" rel="noopener">homoscedasticity</a>:  $\forall i: \text{Var}\left(\epsilon_i\right) = \sigma^2 &lt; \infty $;</li><li>random errors are uncorrelated:  $\forall i \neq j: \text{Cov}\left(\epsilon_i, \epsilon_j\right) = 0 $.</li></ul><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><blockquote><p>最小化均方差误差值</p></blockquote><script type="math/tex; mode=display">\Large \begin{array}{rcl}\mathcal{L}\left(\textbf X, \textbf{y}, \textbf{w} \right) &=& \frac{1}{2n} \sum_{i=1}^n \left(y_i - \textbf{w}^\text{T} \textbf{x}_i\right)^2 \\&=& \frac{1}{2n} \left\| \textbf{y} - \textbf X \textbf{w} \right\|_2^2 \\&=& \frac{1}{2n} \left(\textbf{y} - \textbf X \textbf{w}\right)^\text{T} \left(\textbf{y} - \textbf X \textbf{w}\right)\end{array}</script><p>求解</p><script type="math/tex; mode=display">\Large \begin{array}{rcl} \frac{\partial \mathcal{L}}{\partial \textbf{w}} = 0 &\Leftrightarrow& \frac{1}{2n} \left(-2 \textbf{X}^{\text{T}} \textbf{y} + 2\textbf{X}^{\text{T}} \textbf{X} \textbf{w}\right) = 0 \\&\Leftrightarrow& -\textbf{X}^{\text{T}} \textbf{y} + \textbf{X}^{\text{T}} \textbf{X} \textbf{w} = 0 \\&\Leftrightarrow& \textbf{X}^{\text{T}} \textbf{X} \textbf{w} = \textbf{X}^{\text{T}} \textbf{y} \\&\Leftrightarrow& \textbf{w} = \left(\textbf{X}^{\text{T}} \textbf{X}\right)^{-1} \textbf{X}^{\text{T}} \textbf{y}\end{array}</script><blockquote><p>然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵$X$的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性（multicollinearity）的情况可能真的会出现。 </p></blockquote><h4 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h4><blockquote><p>在损失函数中增加对于权重的l2正则化；l1正则化叫做Lasso回归；</p></blockquote><h4 id="极大似然"><a href="#极大似然" class="headerlink" title="极大似然"></a>极大似然</h4><blockquote><p>最大化似然估计值</p></blockquote><script type="math/tex; mode=display">\Large \begin{array}{rcl} y_i &=& \sum_{j=1}^m w_j X_{ij} + \epsilon_i \\&\sim& \sum_{j=1}^m w_j X_{ij} + \mathcal{N}\left(0, \sigma^2\right) \\p\left(y_i \mid \textbf X; \textbf{w}\right) &=& \mathcal{N}\left(\sum_{j=1}^m w_j X_{ij}, \sigma^2\right)\end{array}</script><h3 id="偏差-方差理论"><a href="#偏差-方差理论" class="headerlink" title="偏差-方差理论"></a>偏差-方差理论</h3><h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><ul><li>true value of the target variable is the sum of a deterministic function $f\left(\textbf{x}\right)$ and random error $\epsilon$: $y = f\left(\textbf{x}\right) + \epsilon$;</li><li>error is normally distributed with zero mean and some variance: $\epsilon \sim \mathcal{N}\left(0, \sigma^2\right)$;</li><li>true value of the target variable is also normally distributed: $y \sim \mathcal{N}\left(f\left(\textbf{x}\right), \sigma^2\right)$;</li><li>we try to approximate a deterministic but unknown function $f\left(\textbf{x}\right)$ using a linear function of the covariates $\widehat{f}\left(\textbf{x}\right)$, which, in turn, is a point estimate of the function $f$ in function space (specifically, the family of linear functions that we have limited our space to), i.e. a random variable that has mean and variance.</li></ul><h4 id="误差公式推导"><a href="#误差公式推导" class="headerlink" title="误差公式推导"></a>误差公式推导</h4><script type="math/tex; mode=display">\Large \begin{array}{rcl} \text{Err}\left(\textbf{x}\right) &=& \mathbb{E}\left[\left(y - \widehat{f}\left(\textbf{x}\right)\right)^2\right] \\&=& \mathbb{E}\left[y^2\right] + \mathbb{E}\left[\left(\widehat{f}\left(\textbf{x}\right)\right)^2\right] - 2\mathbb{E}\left[y\widehat{f}\left(\textbf{x}\right)\right] \\&=& \mathbb{E}\left[y^2\right] + \mathbb{E}\left[\widehat{f}^2\right] - 2\mathbb{E}\left[y\widehat{f}\right] \\\end{array}</script><p>由$\text{Var}\left(z\right) = \mathbb{E}\left[z^2\right] - \mathbb{E}\left[z\right]^2$得，</p><script type="math/tex; mode=display">\Large \begin{array}{rcl} \mathbb{E}\left[y^2\right] &=& \text{Var}\left(y\right) + \mathbb{E}\left[y\right]^2 = \sigma^2 + f^2\\\mathbb{E}\left[\widehat{f}^2\right] &=& \text{Var}\left(\widehat{f}\right) + \mathbb{E}\left[\widehat{f}\right]^2 \\\end{array}</script><p>其中，</p><script type="math/tex; mode=display">\Large \begin{array}{rcl} \text{Var}\left(y\right) &=& \mathbb{E}\left[\left(y - \mathbb{E}\left[y\right]\right)^2\right] \\&=& \mathbb{E}\left[\left(y - f\right)^2\right] \\&=& \mathbb{E}\left[\left(f + \epsilon - f\right)^2\right] \\&=& \mathbb{E}\left[\epsilon^2\right] = \sigma^2\end{array}</script><script type="math/tex; mode=display">\Large \mathbb{E}[y] = \mathbb{E}[f + \epsilon] = \mathbb{E}[f] + \mathbb{E}[\epsilon] = f</script><script type="math/tex; mode=display">\Large \begin{array}{rcl} \mathbb{E}\left[y\widehat{f}\right] &=& \mathbb{E}\left[\left(f + \epsilon\right)\widehat{f}\right] \\&=& \mathbb{E}\left[f\widehat{f}\right] + \mathbb{E}\left[\epsilon\widehat{f}\right] \\&=& f\mathbb{E}\left[\widehat{f}\right] + \mathbb{E}\left[\epsilon\right] \mathbb{E}\left[\widehat{f}\right]  = f\mathbb{E}\left[\widehat{f}\right]\end{array}</script><p>最终，</p><script type="math/tex; mode=display">\Large \begin{array}{rcl} \text{Err}\left(\textbf{x}\right) &=& \mathbb{E}\left[\left(y - \widehat{f}\left(\textbf{x}\right)\right)^2\right] \\&=& \sigma^2 + f^2 + \text{Var}\left(\widehat{f}\right) + \mathbb{E}\left[\widehat{f}\right]^2 - 2f\mathbb{E}\left[\widehat{f}\right] \\&=& \left(f - \mathbb{E}\left[\widehat{f}\right]\right)^2 + \text{Var}\left(\widehat{f}\right) + \sigma^2 \\&=& \text{Bias}\left(\widehat{f}\right)^2 + \text{Var}\left(\widehat{f}\right) + \sigma^2\end{array}</script><h3 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h3><p>$OR(X)=\frac{P(X)}{1-P(X)}$</p><script type="math/tex; mode=display">\large p_{+} = \frac{OR_{+}}{1 + OR_{+}} = \frac{\exp^{\textbf{w}^\text{T}\textbf{x}}}{1 + \exp^{\textbf{w}^\text{T}\textbf{x}}} = \frac{1}{1 + \exp^{-\textbf{w}^\text{T}\textbf{x}}} = \sigma(\textbf{w}^\text{T}\textbf{x})</script><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="超参优化：正则化参数C"><a href="#超参优化：正则化参数C" class="headerlink" title="超参优化：正则化参数C"></a>超参优化：正则化参数C</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression, LogisticRegressionCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">skf = StratifiedKFold(n_splits=<span class="number">5</span>, shuffle=<span class="keyword">True</span>, random_state=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line">c_values = np.logspace(<span class="number">-2</span>, <span class="number">3</span>, <span class="number">500</span>)</span><br><span class="line"></span><br><span class="line">logit_searcher = LogisticRegressionCV(Cs=c_values, cv=skf, verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">logit_searcher.fit(X_poly, y)</span><br></pre></td></tr></table></figure><h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><h3 id="1-BoW构造稀疏矩阵"><a href="#1-BoW构造稀疏矩阵" class="headerlink" title="1. BoW构造稀疏矩阵"></a>1. BoW构造稀疏矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">cv.fit(text_train)</span><br><span class="line">X_train = cv.transform(text_train)</span><br><span class="line">X_test = cv.transform(text_test)</span><br></pre></td></tr></table></figure><h3 id="2-构造多项式特征"><a href="#2-构造多项式特征" class="headerlink" title="2. 构造多项式特征"></a>2. 构造多项式特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">7</span>)</span><br><span class="line">X_poly = poly.fit_transform(X)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>《机器学习》——周志华</p></li><li><p><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</p></li></ol><p>3.<a href="http://sklearn.apachecn.org/cn/0.19.0/" target="_blank" rel="noopener">Sklearn文档</a></p><ul><li>Course materials as a <a href="https://www.kaggle.com/kashnitsky/mlcourse" target="_blank" rel="noopener">Kaggle Dataset</a></li><li>A nice and concise overview of linear models is given in the book <a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener">“Deep Learning”</a> (I. Goodfellow, Y. Bengio, and A. Courville).</li><li>Linear models are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).</li><li>If you prefer a thorough overview of linear model from a statistician’s viewpoint, then look at “The elements of statistical learning” (T. Hastie, R. Tibshirani, and J. Friedman).</li><li>The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.</li><li><a href="http://scikit-learn.org/stable/documentation.html" target="_blank" rel="noopener">Scikit-learn</a> library. These guys work hard on writing really clear documentation.</li><li>Scipy 2017 <a href="https://github.com/amueller/scipy-2017-sklearn" target="_blank" rel="noopener">scikit-learn tutorial</a> by Alex Gramfort and Andreas Mueller.</li><li>One more <a href="https://github.com/diefimov/MTH594_MachineLearning" target="_blank" rel="noopener">ML course</a> with very good materials.</li><li><a href="https://github.com/rushter/MLAlgorithms" target="_blank" rel="noopener">Implementations</a> of many ML algorithms. Search for linear regression and logistic regression.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;算法理论&quot;&gt;&lt;a href=&quot;#算法理论&quot; class=&quot;headerlink&quot; title=&quot;算法理论&quot;&gt;&lt;/a&gt;算法理论&lt;/h2&gt;&lt;sc
      
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-决策树(Dicision Tree)</title>
    <link href="http://blog.a-stack.com/2018/11/23/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-Dicision-Tree/"/>
    <id>http://blog.a-stack.com/2018/11/23/机器在学习-决策树-Dicision-Tree/</id>
    <published>2018-11-23T08:50:11.000Z</published>
    <updated>2018-11-25T07:22:23.785Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="决策树的算法"><a href="#决策树的算法" class="headerlink" title="决策树的算法"></a>决策树的算法</h2><blockquote><p>ID3  C4.5  CART</p><p>ID3（Iterative Dichotomiser 3）由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛华能力。</p><p>C4.5 是 ID3 的后继者，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。</p><p>C5.0 是 Quinlan 根据专有许可证发布的最新版本。它使用更少的内存，并建立比 C4.5 更小的规则集，同时更准确。</p><p>CART（Classification and Regression Trees （分类和回归树））与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。</p><p>scikit-learn 使用 CART 算法的优化版本。</p></blockquote><p><strong>Decision Trees (DTs)</strong> 是一种用来 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree-classification" target="_blank" rel="noopener">classification</a> 和 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree-regression" target="_blank" rel="noopener">regression</a> 的无参监督学习方法。其目的是创建一种模型从数据特征中学习简单的决策规则来预测一个目标变量的值。 </p><p><img src="/2018/11/23/机器在学习-决策树-Dicision-Tree/credit_scoring_toy_tree_english.png" alt="credit_scoring_toy_tree_english"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(L)</span>:</span></span><br><span class="line">    create node t</span><br><span class="line">    <span class="keyword">if</span> the stopping criterion <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">        assign a predictive model to t</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Find the best binary split L = L_left + L_right</span><br><span class="line">        t.left = build(L_left)</span><br><span class="line">        t.right = build(L_right)</span><br><span class="line">    <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure><h3 id="决策树的递归返回"><a href="#决策树的递归返回" class="headerlink" title="决策树的递归返回"></a>决策树的递归返回</h3><ul><li>(1) 当前结点包含的样本全属于同一类别，无需划分; </li><li>(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分; </li><li>(3) 当前结点包含的样本集合为空，不能划分.</li></ul><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><ul><li><p>信息熵（香农）： $E(D) = -\sum_k p_klog_2p_k$</p></li><li><p>增益： $IG(D,a) = E(D) - \sum_v \frac{|D^v|}{|D|}E(D^v)$</p><p><img src="/2018/11/23/机器在学习-决策树-Dicision-Tree/topic3_credit_scoring_entropy.png" alt="scoring_entropy"></p><blockquote><ol><li>信息增益可以衡量使用某个属性划分的好坏；</li><li>信息增益准则对可能取值数目较多的属性有偏好，为减少这种偏好带来的不利影响，C4.5决策树算法使用增益率来选择最优划分属性。</li></ol></blockquote></li><li><p>增益率： $G_r(D,a)=\frac{IG(D,a)}{IV(a)}$</p><script type="math/tex; mode=display">IV(a) =\sum_v \frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}</script></li></ul><h3 id="Gini"><a href="#Gini" class="headerlink" title="Gini"></a>Gini</h3><script type="math/tex; mode=display">Gini(D)=1-\sum_k p^2_k</script><blockquote><p>CART决策树使用的算法,Gini数值约小，数据集D的纯度越高。</p></blockquote><p><img src="/2018/11/23/机器在学习-决策树-Dicision-Tree/Criteria of quality as a function of binary classification.png" alt="Criteria of quality as a function of binary classification"></p><h3 id="剪枝（pruning）"><a href="#剪枝（pruning）" class="headerlink" title="剪枝（pruning）"></a>剪枝（pruning）</h3><ul><li><strong>预剪枝：</strong> （自顶向下） 预剪枝是指在决策树生成过程中，对每个结点在划 分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;</li><li><strong>后剪枝：</strong>（自底向上）后剪枝则是先从训练集生成一棵完整的决策树， 然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.</li></ul><blockquote><p>预剪枝减少了计算量，减少了过拟合，但存在欠拟合的风险；</p><p>后剪枝是自底向上的对树中所有非叶节点进行逐一考察，训练开销比预剪枝要大很多。</p></blockquote><h3 id="一些处理技巧"><a href="#一些处理技巧" class="headerlink" title="一些处理技巧"></a>一些处理技巧</h3><h4 id="处理连续值"><a href="#处理连续值" class="headerlink" title="处理连续值"></a>处理连续值</h4><ul><li>连续属性离散化技术（二分法）；</li><li>对训练样本值排序，查找最大信息增益的分割点（对结果影响大的点），划分为多个连续区间；</li><li>如果训练样本某个属性不同样本值特别多，可以选择top-N增益最大的点作为划分分类的点；</li></ul><h4 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h4><ul><li>根据缺失值赋权；</li></ul><h4 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h4><ul><li>组合多个变量，可以形成非与坐标轴平行的分类边界；</li></ul><h4 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h4><script type="math/tex; mode=display">\Large D = \frac{1}{\ell} \sum\limits_{i =1}^{\ell} (y_i - \frac{1}{\ell} \sum\limits_{j=1}^{\ell} y_j)^2</script><p>where $\ell$ is the number of samples in a leaf, $y_i$ is the value of the target variable.</p><h2 id="决策树算法的优缺点"><a href="#决策树算法的优缺点" class="headerlink" title="决策树算法的优缺点"></a>决策树算法的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>可解释性强，决策树的计算原理和人做决策的过程十分类似，是解释性最强的机器学习模型；</li><li>可视化方便，计算过程可以在图中直观的反映出来；</li><li>训练和预测都十分迅速，计算代价小；</li><li>训练需要的数据少。其他机器学习模型通常需要数据规范化，比如构建虚拟变量和移除缺失值,不过请注意，这种模型不支持缺失值。</li><li>超参十分少，常用的只有层数、叶子节点中元素个数、最大特征数等少数几个超参；</li><li>对于连续变量、离散变量、分类问题和回归问题都可以处理。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>对噪声数据敏感，训练数据发生轻微变动，可能导致整个模型参数发生很大变化；</li><li>分割界面只能平行或垂直坐标轴方向，处理逻辑太过简单；</li><li>需要特别注意过拟合的发生；决策树模型容易产生一个过于复杂的模型,这样的模型对数据的泛化性能会很差。这就是所谓的过拟合.一些策略像剪枝、设置叶节点所需的最小样本数或设置数的最大深度是避免出现 该问题最为有效地方法。</li><li>稳定性差；因为数据中的微小变化可能会导致完全不同的树生成。这个问题可以通过决策树的集成来得到缓解 </li><li>最优决策树的选择是一个NP-完备问题。需要使用一些启发式算法寻找最优的信息增益，但未必能找到最优值；这个问题可以通过集成学习来训练多棵决策树来缓解,这多棵决策树一般通过对特征和样本有放回的随机采样来生成。 </li><li>缺失数据的处理比较麻烦；</li><li>数据集没有覆盖的区域，没有区分能力</li><li>有些概念很难被决策树学习到,因为决策树很难清楚的表述这些概念。例如XOR，奇偶或者复用器的问题。</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><blockquote><p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" target="_blank" rel="noopener"><code>sklearn.tree.DecisionTreeClassifier</code></a></p><ul><li><code>max_depth</code> – the maximum depth of the tree;</li><li><code>max_features</code> - the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be “expensive” to search for partitions for <em>all</em> features);</li><li><code>min_samples_leaf</code> – the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few members.</li></ul><p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" target="_blank" rel="noopener"><code>DecisionTreeRegressor</code></a> </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sklearn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">clf_tree = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=<span class="number">3</span>, </span><br><span class="line">                                  random_state=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training the tree</span></span><br><span class="line">clf_tree.fit(train_data, train_labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">tree_pred = tree.predict(X_holdout)</span><br><span class="line">accuracy_score(y_holdout, tree_pred) <span class="comment"># 0.94</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, cross_val_score</span><br><span class="line"></span><br><span class="line">tree_params = &#123;<span class="string">'max_depth'</span>: range(<span class="number">1</span>,<span class="number">11</span>),</span><br><span class="line">               <span class="string">'max_features'</span>: range(<span class="number">4</span>,<span class="number">19</span>)&#125;</span><br><span class="line"></span><br><span class="line">tree_grid = GridSearchCV(tree, tree_params,</span><br><span class="line">                         cv=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">tree_grid.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">accuracy_score(y_holdout, tree_grid.predict(X_holdout)) <span class="comment">#0.946</span></span><br></pre></td></tr></table></figure><h3 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h3><p>经过训练，我们可以使用 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz" target="_blank" rel="noopener"><code>export_graphviz</code></a> 导出器以 <a href="http://www.graphviz.org/" target="_blank" rel="noopener">Graphviz</a> 格式导出决策树. 如果你是用 <a href="http://conda.io/" target="_blank" rel="noopener">conda</a> 来管理包，那么安装 graphviz 二进制文件和 python 包可以用以下指令安装</p><blockquote><p>conda install python-graphviz</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pydotplus <span class="comment">#pip install pydotplus</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tree_graph_to_png</span><span class="params">(tree, feature_names, png_file_to_save)</span>:</span></span><br><span class="line">    tree_str = export_graphviz(tree, feature_names=feature_names, </span><br><span class="line">                                     filled=<span class="keyword">True</span>, out_file=<span class="keyword">None</span>)</span><br><span class="line">    graph = pydotplus.graph_from_dot_data(tree_str)  </span><br><span class="line">    graph.write_png(png_file_to_save)</span><br><span class="line">    </span><br><span class="line">  tree_graph_to_png(tree=clf_tree, feature_names=[<span class="string">'x1'</span>, <span class="string">'x2'</span>], </span><br><span class="line">                  png_file_to_save=<span class="string">'../../img/topic3_tree1.png'</span>)</span><br></pre></td></tr></table></figure><p><img src="/2018/11/23/机器在学习-决策树-Dicision-Tree/tree1.png" alt="tree1"></p><h3 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h3><blockquote><ul><li>对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。</li><li>考虑事先进行降维( <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/decomposition.html#pca" target="_blank" rel="noopener">PCA</a> , <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/decomposition.html#ica" target="_blank" rel="noopener">ICA</a> ，使您的树更好地找到具有分辨性的特征。</li><li>通过 <code>export</code> 功能可以可视化您的决策树。使用 <code>max_depth=3</code> 作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。</li><li>请记住，填充树的样本数量会增加树的每个附加级别。使用 <code>max_depth</code> 来控制输的大小防止过拟合。</li><li>通过使用 <code>min_samples_split</code> 和 <code>min_samples_leaf</code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。所以尝试 <code>min_samples_leaf=5</code> 作为初始值。如果样本的变化量很大，可以使用浮点数作为这两个参数中的百分比。两者之间的主要区别在于 <code>min_samples_leaf</code> 保证叶结点中最少的采样数，而 <code>min_samples_split</code> 可以创建任意小的叶子，尽管在文献中 <code>min_samples_split</code> 更常见。</li><li>在训练之前平衡您的数据集，以防止决策树偏向于主导类.可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code>sample_weight</code>) 的和归一化为相同的值。还要注意的是，基于权重的预修剪标准 (<code>min_weight_fraction_leaf</code>) 对于显性类别的偏倚偏小，而不是不了解样本权重的标准，如 <code>min_samples_leaf</code> 。</li></ul></blockquote><ul><li>如果样本被加权，则使用基于权重的预修剪标准 <code>min_weight_fraction_leaf</code> 来优化树结构将更容易，这确保叶节点包含样本权重的总和的至少一部分。</li><li>所有的决策树内部使用 <code>np.float32</code> 数组 ，如果训练数据不是这种格式，将会复制数据集。</li><li>如果输入的矩阵X为稀疏矩阵，建议您在调用fit之前将矩阵X转换为稀疏的<code>csc_matrix</code> ,在调用predict之前将 <code>csr_matrix</code> 稀疏。当特征在大多数样本中具有零值时，与密集矩阵相比，稀疏矩阵输入的训练时间可以快几个数量级。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>《机器学习》——周志华</p></li><li><p><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</p></li></ol><p>3.<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree" target="_blank" rel="noopener">Sklearn 决策树</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>读书笔记：Machine Learning Yearning</title>
    <link href="http://blog.a-stack.com/2018/09/29/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%9AMachine-Learning-Yearning/"/>
    <id>http://blog.a-stack.com/2018/09/29/读书笔记：Machine-Learning-Yearning/</id>
    <published>2018-09-29T02:00:21.000Z</published>
    <updated>2018-09-29T09:41:43.630Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/09/29/读书笔记：Machine-Learning-Yearning/MachineLearningYearning.jpg" alt="Machine Learning Yearning"></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>《Machine Learning Yearning》是Andrew Ng写的一本在深度学习领域偏重工程实践的小册子，终于在九月底把坑填完了，有需要的可以在其<a href="http://www.mlyearning.org/" target="_blank" rel="noopener">官网</a>索取,或者直接通过该地址下载<a href="https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/514ed488-bcae-4924-8fd8-dcabeec2e440/Ng_MLY13.pdf" target="_blank" rel="noopener">目前的版本</a>。从吴恩达开始撰写前面几章时我便订阅了相关邮件，持续跟进文章内容，这本小册子可以配合deeplearning.ai的深度学习课程一起学习，效果更佳。本文简单的记录一下阅读过程中一些的新的收货。</p><h2 id="端到端学习的选择"><a href="#端到端学习的选择" class="headerlink" title="端到端学习的选择"></a>端到端学习的选择</h2><p>深度学习与传统机器学习算法的一个主要差异，在于它本质上是一个端到端的学习过程，不再需要或很少需要手工特征的介入。但端到端学习真的好么？适用于所有的业务场景么？</p><p>深度学习的端到端学习策略有效是建立在足够训练数据的基础上的，当数据量有限的情况下，手工特征的过程有助于加速网络收敛和限制网络的发散。而当存在足够的训练数据</p><h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p>由于模型的复杂性，导致在深度神经网络中，出现误差或错误可能来自于多个方面，甚至是多个方面的共同作用结果，所以制定合理的错误分析策略对于寻找问题根源和优化模型效果至关重要。</p><ul><li>错误分析的过程，本质上还是一个控制变量法的过程，保证其它相关变量的最优来聚焦所需要观测的变量信息；</li><li>错误分析针对那些人类自身可以做的很好的任务，比如图片分类、物体识别等，会有很好的效果，而对人类不擅长的，很难进一步的进行直接的错误分析；</li><li>当你把一个pipeline的各部分已经优化到极致的情况，如果整体模型效果依然不好时，你需要考虑是不是模型设计存在本质缺陷，如没有包含所有有效的相关因素，没有形成合理的推断流程；</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>​</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/2018/09/29/读书笔记：Machine-Learning-Yearning/MachineLearningYearning.jpg&quot; alt=&quot;Machine Learning Yearning&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;概述&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="读书笔记" scheme="http://blog.a-stack.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="工程实践" scheme="http://blog.a-stack.com/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"/>
    
      <category term="教学" scheme="http://blog.a-stack.com/tags/%E6%95%99%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>HMM与Veterbi算法</title>
    <link href="http://blog.a-stack.com/2018/08/28/HMM%E4%B8%8EVeterbi%E7%AE%97%E6%B3%95/"/>
    <id>http://blog.a-stack.com/2018/08/28/HMM与Veterbi算法/</id>
    <published>2018-08-28T08:53:06.000Z</published>
    <updated>2018-08-28T09:26:36.540Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/13.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="从概率图模型谈起"><a href="#从概率图模型谈起" class="headerlink" title="从概率图模型谈起"></a>从概率图模型谈起</h2><p>概率模型(probabilistic model)提供了一种描述框架，将学习任务归结于计算变量的概率分布。在概 率模型中，利用已知变量推测未知变量的分布称为”推断” (inference)，其 核心是如何基于可观测变量推测出未知变量的条件分布具体来说，假定所关心的变量集合为Y,可观测变量集合为 O，其他变量的集合为 R，”生成式” (generative)模型考虑联合分布 P(Y，R，O) ， “判别式” (discriminative)模 型考虑条件分布 P(Y，R I O)。给定一组观测变量值，推断就是要由 P(Y ,R,O) 或 P(Y, R |O) 得到条件概率分布 P(Y I O)。</p><p>概率图是一类利用图来表达变量相关关系的概率模型。若变量之间存在显示的因果关系，则常用贝叶斯网络（有向图模型）；若变量之间存在相关性，但难以获得显示的因果关系，则常使用马尔可夫网络（无向图模型）。</p><h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><p>隐马尔科夫模型（Hidden Markov Model, HMM）是结构最简单的动态贝叶斯网络，主要用于时序数据建模，应用于语音识别、自然语言处理等领域。</p><h3 id="马尔可夫假设"><a href="#马尔可夫假设" class="headerlink" title="马尔可夫假设"></a>马尔可夫假设</h3><ol><li>系统下一时刻的状态仅有当前状态决定，不依赖于以往的任何状态，即<script type="math/tex; mode=display">P(x_1,y_1,...,x_n,y_n)=P(y_1)P(x_1|y_1)</script></li></ol><h3 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h3><ol><li>概率计算问题： 给定模型，如何有效计算产生关测序列的概率$P(x|\lambda)$，可以利用求解结果求未来的观测数据值；</li><li>预测/解码问题： 给定模型和观测序列$x$，如何找到于此观测序列最匹配的状态序列$y$, $P(y|0)$；</li><li>学习问题： 给定观测序列$x$，如何调整模型参数，使得该序列出现的概率$P(x|\lambda)$ 最大，即如何训练模型使其能更好的描述观测数据；</li></ol><h2 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h2><p><img src="/2018/08/28/HMM与Veterbi算法/Viterbi_animated_demo.gif" alt="Viterbi_animated_demo"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>《统计学习方法》—李航</li><li>《机器学习》—周志华</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/13.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>读书笔记:数学之美</title>
    <link href="http://blog.a-stack.com/2018/08/26/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    <id>http://blog.a-stack.com/2018/08/26/读书笔记-数学之美/</id>
    <published>2018-08-26T11:25:58.000Z</published>
    <updated>2018-08-27T13:50:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/08/26/读书笔记-数学之美/数学之美.jpg" alt="数学之美"></p><ul><li><p>作者： 吴军</p></li><li><p>2014年11月第二版</p></li><li><p>北京：人民邮电出版社</p><hr><a id="more"></a></li></ul><p>时隔多年，在人工智能概念进入鼎盛时期的今天，花一个周末，细细品读了吴军先生的《数学之美》，又有了不少新的感悟，新的收货，稍作整理，便于复查。</p><p>此次重读恰逢归纳自然语言处理相关技术和脉络，而吴军本身就是这个学科的专家，所以其著作中对相关内容的描述条例清晰，结构完备，值得思考。</p><h2 id="经典语句摘录"><a href="#经典语句摘录" class="headerlink" title="经典语句摘录"></a>经典语句摘录</h2><blockquote><p><strong>WWW发明人蒂姆.伯纳斯。李：</strong></p><p>简单性和模块化是软件工程的基石；分布式和容错性是互联网的生命。</p></blockquote><h2 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h2><h3 id="从历史看：语言、数字、文字都是信息的载体，或是对信息的编码"><a href="#从历史看：语言、数字、文字都是信息的载体，或是对信息的编码" class="headerlink" title="从历史看：语言、数字、文字都是信息的载体，或是对信息的编码"></a>从历史看：语言、数字、文字都是信息的载体，或是对信息的编码</h3><p>从人来进化角度来看，先有的语言（声音元素最先），再有的数字，然后是文字。目的都在于方便人与人之间通信（传递信息）。</p><p>早期人来了解和需要传递的信息十分有限，因此不需要高级的语言或者数字、文字来进行表达。但随着人类的进化和文明的发展，需要表达的信息越来越多，不再是几种不同的声音就能够完全覆盖，语言就此产生。知识也通过口述的方式，逐代传递。</p><p>我们祖先迅速学习新鲜事物，语言越来越丰富，内容越来越抽象，语言描述的共同要素，比如某些物体、数量、行为动作便被抽象出来形成了<strong>词汇</strong>。当语言和词汇多到一定程度，人类靠大脑已经记不住所有词汇，于是高效记录信息的需求就产生了，这便是文字的起源。</p><blockquote><p>由此可见，产品果真是由需求决定的！</p></blockquote><p>最初文字的词汇依靠模拟/模仿物体或行为形成的象形文字来展现，但随着文明进步，信息量逐步增加，象形文字数目不再随着文明发展而增加了，因为没人能够记住这么多的文字。于是，概念的第一次概括和归类开始了。比如“日”除了表示太阳，还可以表示一天。</p><blockquote><p>在那个时间，人类就已经领会了聚类算法的精髓 …只不过由于“算力不足”，这个聚类的过程一般持续几千年才能收敛到合适的位置。</p></blockquote><p>聚类虽然减少了表达信息的文字数量，却带来了一个严重的问题——歧义性。多义字在一段文字中的表达需要依靠上下文推理来完成去除歧义。</p><blockquote><p>虽然经过训练的人类可以很好的依靠理解消除歧义，但苦了计算机模型，需要打破状态无记忆的机理，沿着上下文的思路去解决问题。可见这是语言产生伊始就埋的一个坑。</p></blockquote><h4 id="不同文明在信息的维度上是等价的"><a href="#不同文明在信息的维度上是等价的" class="headerlink" title="不同文明在信息的维度上是等价的"></a>不同文明在信息的维度上是等价的</h4><p>虽然受地域和文明发展进度差异影响，不同文明发明了不同的语言和文字，但本质上都是一种对信息的编码方式，所以可以通过一定的手段进行翻译的。</p><blockquote><p>文字是信息的载体，而非信息本身。</p></blockquote><h4 id="罗塞塔"><a href="#罗塞塔" class="headerlink" title="罗塞塔"></a>罗塞塔</h4><p>古埃及罗塞塔的石碑记录了最早的象形文字多语平行语料库，上面有三种语言：埃及象形文字、埃及拼音文字和古希腊文。因此Google的机器翻译项目也被命名为罗塞塔。</p><p><img src="/2018/08/26/读书笔记-数学之美/罗塞塔石碑.jpg" alt="罗塞塔石碑"></p><blockquote><p><strong>百度百科解读：</strong></p><p>罗塞塔<a href="https://baike.baidu.com/item/%E7%9F%B3%E7%A2%91/36268" target="_blank" rel="noopener">石碑</a>（Rosetta Stone，也译作罗塞达碑），高1.14米，宽0.73米，制作于公元前196年，刻有<a href="https://baike.baidu.com/item/%E5%8F%A4%E5%9F%83%E5%8F%8A/226771" target="_blank" rel="noopener">古埃及</a>国王<a href="https://baike.baidu.com/item/%E6%89%98%E5%8B%92%E5%AF%86%E4%BA%94%E4%B8%96/3481978" target="_blank" rel="noopener">托勒密五世</a>登基的诏书。石碑上用<a href="https://baike.baidu.com/item/%E5%B8%8C%E8%85%8A%E6%96%87%E5%AD%97/12722009" target="_blank" rel="noopener">希腊文字</a>、<a href="https://baike.baidu.com/item/%E5%8F%A4%E5%9F%83%E5%8F%8A%E6%96%87%E5%AD%97/2494489" target="_blank" rel="noopener">古埃及文字</a>和当时的通俗体文字刻了同样的内容，这使得近代的<a href="https://baike.baidu.com/item/%E8%80%83%E5%8F%A4%E5%AD%A6%E5%AE%B6/1217571" target="_blank" rel="noopener">考古学家</a>得以有机会对照各语言<a href="https://baike.baidu.com/item/%E7%89%88%E6%9C%AC/505574" target="_blank" rel="noopener">版本</a>的内容后，解读出已经失传千余年的埃及象形文之意义与结构，而成为今日研究古埃及历史的重要里程碑。 </p></blockquote><h4 id="数字产生及编码"><a href="#数字产生及编码" class="headerlink" title="数字产生及编码"></a>数字产生及编码</h4><p>数字产生与文字类似，当人类发展到一定程度，需要计数系统来进行计算的时候便产生了信息的载体数字。当然，早期数字没有书写的形式，而是掰指头，这就是今天我们使用十进制的原因。</p><blockquote><p>玛雅文明使用的是二十进制（手脚都算上…）</p></blockquote><p>罗马人用字符I代表1，V代表5，X代表10，L代表50，C代表100，D代表500，M代表1000，解码规则是加减法（中国解码规则是乘法），小数字出现在大数字左边为减，右边为加。比如IIXX（10+10-1-1=18），如果要用罗马数字表示100万 …</p><p>阿拉伯数字最初由古印度人发明，由阿拉伯人传入欧洲，被中间商冠名了。</p><h4 id="从象形文字到楔形文字"><a href="#从象形文字到楔形文字" class="headerlink" title="从象形文字到楔形文字"></a>从象形文字到楔形文字</h4><p>楔形文字又称拼音文字，是一个很大的进化，人类在描述物体的方式上，从外表化到了抽象概念的层级，是一种高效的信息编码方式。比如，常用字短，生僻字长就是符合最短编码原理的一种实现。</p><h4 id="语言与语法"><a href="#语言与语法" class="headerlink" title="语言与语法"></a>语言与语法</h4><p>词可以被认为是有限而且封闭的集合，而语言则是无限开放的集合；从数学上讲，前者可以由完备的编码规则，后者则不具备。</p><p>任何一种语言都是一种信息的编码方式；</p><p>语法可以认为是解码的规则，但很多时候文章的撰写不完全符合语法，比如鲁迅先生的文字。</p><p>语言vs语法： 语言简直从真实的语句文本（称为语料）出发，而后者坚持从规则出发；经过三四十年的争论，最后实践证明，自然语言的成就最终宣布了前者的胜利。</p><h3 id="自然语言发展的两个阶段：从规则到统计"><a href="#自然语言发展的两个阶段：从规则到统计" class="headerlink" title="自然语言发展的两个阶段：从规则到统计"></a>自然语言发展的两个阶段：从规则到统计</h3><p><img src="/2018/08/26/读书笔记-数学之美/信息传输.jpeg" alt="信息传输"></p><p><img src="/2018/08/26/读书笔记-数学之美/4组织语言.png" alt="4组织语言"></p><blockquote><p>我们要把一个意思表达出来，通过某种语言的一句话表述出来就是用这种语言的编码方式对头脑中的信息做了一次编码，编码结果就是一串文字；而如果对方懂得这么语言，他就可以利用这么语言的语法解码处说话人想表达的意思。</p></blockquote><h4 id="NLP的历史"><a href="#NLP的历史" class="headerlink" title="NLP的历史"></a>NLP的历史</h4><ul><li><p>1950年，图灵提出的图灵测试可以作为NLP的开始；</p></li><li><p>20世纪50年代到70年代，计算机处理NLP问题的认识都局限在人类学习语言的方式上，也就是用电脑模拟人脑，这20多年的成果近乎为零，其中由大量的语言学家参与其中；</p><ul><li>让计算机理解自然语言，必须让计算机拥有类似人类的智能；</li><li>更多的采用仿生学的方法解决问题；</li><li>为什么会有这种错误的认识？ 因为人类就是这么做的，这就是直觉的作用；</li><li>这样的方法被称为“鸟飞派”（怀特兄弟发明飞机靠的是空气动力学而不是仿生学）；</li><li>手段是：句法分析和语义分析<ul><li>分析句法结构，建立文法规则</li><li>20世纪80年代以前，NLP的文法规则都是由人工总结完成的</li><li>有两大困难：<ul><li>为了覆盖哪怕20%的真实语句，文法规则需要至少几万条；语言学家来不及写；更可怕的是，写到后面出现了各种矛盾，为了解决矛盾需要为规则建立限定规则；如果要覆盖50%的语句，文法规则会多到每增加一个语句，需要添加一条规则；（正如英语语法学的再好，不练习，不看文献，照样看不懂英文电影）</li><li>计算机解析规则困难重重，计算机解码方式是上下文无关的，而NLP都是复杂的上下文有关的文法；前者算法复杂度为语句长度的二次方，后者是语句长度的六次方；</li></ul></li></ul></li></ul><p><img src="/2018/08/26/读书笔记-数学之美/句法分析树.png" alt="句法分析树"></p></li><li><p>20世纪70年代，开始使用统计方法和数学模型解决NLP问题</p><ul><li>语义方面的多义性也遇到了困难；</li><li>贾里尼克和他领导的IBM华生实验室（T.J. Watson）采用基于统计的方法解决语音识别取得了突破；感染了许多研究人员转投统计领域；比如李开复和洪小文；</li><li>但基于规则和基于统计的争执还是持续了15年左右，直到20世纪90年代才分出胜负<ul><li>基于统计方法的核心模型是通信系统+隐马尔科夫模型，最早成功的领域是语音识别、词性分析（一维符号序列），机器翻译和句法分析输入是一维，输出是多维或乱序信息，简单应用不管用；同时数据量不足，发展不是很快；</li><li>传统方法认为基于统计的方法只能处理浅层的自然语言问题，无法进入深层次研究；</li><li>随着计算力的提高和数据量的增加，一切变的不一样了，2005年随着Google机器翻译战胜SysTran,最后一个领域也被统计方法攻占；</li><li>另一个重要原因，用基于统计的方法替代传统方法，需要等原有的一批语言学家退休；</li></ul></li></ul></li></ul><blockquote><p>类比到我们学习英语的过程,当时教授课程过程还是以英语语法、词性和构词法为主，主谓宾、定状补；而我基本上都这些语法缺乏概念，更多的是靠语感在做题目，这是建立在读了大量英语文献基础上，也是一种基于统计的预测算法。</p></blockquote><h4 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h4><p>统计语言模型的目的在于为计算机建立一种具备上下文相关的数学模型。</p><p>自然语言处理过程可以简单的认为是判断一句话是不是自然语句，以前是通过语法规则来判断的，统计语言模型是通过概率来衡量的。</p><h4 id="马尔可夫"><a href="#马尔可夫" class="headerlink" title="马尔可夫"></a>马尔可夫</h4><p>马尔可夫假设认为一个词出现的概率只与它前面一个词有关，形成二元模型；如果跟前面两个词有关就是三元模型(二阶马尔可夫模型)。</p><p>这样一个句子出现的概率可以表示为：</p><script type="math/tex; mode=display">P(S) = P(w_1)*P(w_2|w_1)*P(w_3|w_2)*...*P(w_n|w_{n-1})</script><blockquote><p>关于N元模型：</p><p>N越大效果越好，但需要的计算量越大，N元模型的空间复杂度几乎是N的指数函数$O(|V|^N)$,其中$|V|$是语言词典的词汇量，一般几万到几十万个。</p><p>当N从3到4时，其实模型效果的提升已经不是十分明显了。</p><p>Google的罗塞塔翻译使用了四元模型，存储与至少500台Google服务器中，当然2017年也被神经翻译机替代了。</p></blockquote><p>马尔可夫的局限性： 由于文本的上下文信息可能跳跃性存在，所以当N不是十分大的情况下，无法覆盖所有的语言现象。需要具备一定记忆性的单元来完成长期记忆的存储。</p><blockquote><p>关于语料库的覆盖问题：</p><p>汉语词汇量大概20万，训练一个三元模型产生$200000^3=8X10^{15}$个参数；</p><p>加深互联网中抛去垃圾数据，有100亿个有意义的中文网页，每个网页平均1000个词，即使爬取所有的词只有$10^{13}$个语料词汇。大部分条件概率为零。</p><ul><li>卡茨退避法，拿出一部分概率给语料库为出现的词，同时调低低频词的概率；</li><li>线性插值法</li></ul></blockquote><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>词是表达语义的最小单位；对于拼音语言来说，由于词之间有明确的分界符，统计和使用语言模型非常直接。</p><ul><li>最简单的分词方法是查词典，遇到复合词进行最长词匹配；<ul><li>对于二义性的分割没法解决；</li><li>“发展中国家”==发展 中国 家</li><li>“上海大学城” == 上海大学 城 书店</li><li>“北京大学生” == 北京大学 生</li></ul></li><li>统计方法：保证分完词之后这个句子出现的概率最大<ul><li>求解过程可以建模成动态规划问题</li></ul></li><li>但是，并不是所有的语句都有分词结果的，比如：<ul><li>此地安能居住，其人好不悲伤</li><li>下雨天留客天天留我不留</li></ul></li><li>中文分词技术后来还用于了英文手写字的分词；</li><li>分词问题已经属于解决了的问题，目前没有太多值得深入研究的技术领域了，除了偶尔增加一些新词</li></ul><h3 id="隐马尔科夫模型（HMM）"><a href="#隐马尔科夫模型（HMM）" class="headerlink" title="隐马尔科夫模型（HMM）"></a>隐马尔科夫模型（HMM）</h3><p>隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是<a href="http://www.cnblogs.com/pinard/p/6509630.html" target="_blank" rel="noopener">RNN</a>，<a href="http://www.cnblogs.com/pinard/p/6519110.html" target="_blank" rel="noopener">LSTM</a>等神经网络序列模型的火热，HMM的地位有所下降。</p><p>使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。</p><p>马尔可夫的求解过程就是从观测序列推断出隐藏序列的过程。</p><h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><h3 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h3><h2 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h2><h2 id="搜索引擎"><a href="#搜索引擎" class="headerlink" title="搜索引擎"></a>搜索引擎</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.cnblogs.com/pinard/p/6945257.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6945257.html</a></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2018/08/26/读书笔记-数学之美/数学之美.jpg&quot; alt=&quot;数学之美&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;作者： 吴军&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2014年11月第二版&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;北京：人民邮电出版社&lt;/p&gt;
&lt;hr&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://blog.a-stack.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="书籍" scheme="http://blog.a-stack.com/tags/%E4%B9%A6%E7%B1%8D/"/>
    
  </entry>
  
  <entry>
    <title>FastText-A better choice for word embedding?</title>
    <link href="http://blog.a-stack.com/2018/08/21/FastText-A-better-choice-for-word-embedding/"/>
    <id>http://blog.a-stack.com/2018/08/21/FastText-A-better-choice-for-word-embedding/</id>
    <published>2018-08-21T07:45:46.000Z</published>
    <updated>2018-08-28T14:08:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/10.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="FastText-vs-Word2vec"><a href="#FastText-vs-Word2vec" class="headerlink" title="FastText vs Word2vec"></a>FastText vs Word2vec</h2><p>fastText的架构和word2vec中的CBOW的架构类似，因为它们的作者都是Facebook的科学家Tomas Mikolov，而且确实fastText也算是words2vec所衍生出来的。 </p><p>According to a detailed comparison of Word2Vec and FastText in <a href="https://render.githubusercontent.com/view/Word2Vec_FastText_Comparison.ipynb" target="_blank" rel="noopener">this notebook</a>, fastText does significantly better on syntactic tasks as compared to the original Word2Vec, especially when the size of the training corpus is small. Word2Vec slightly outperforms FastText on semantic tasks though. The differences grow smaller as the size of training corpus increases. Training time for fastText is significantly higher than the Gensim version of Word2Vec (<code>15min 42s</code> vs <code>6min 42s</code> on text8, 17 mil tokens, 5 epochs, and a vector size of 100). </p><p><img src="/2018/08/21/FastText-A-better-choice-for-word-embedding/fasttextVSword2vec.png" alt="fasttextVSword2vec"></p><blockquote><p><strong>重要：</strong></p><p>fastText can be used to obtain vectors for <strong>out-of-vocabulary (OOV)</strong> words, by summing up vectors for its component char-ngrams, provided at least one of the char-ngrams was present in the training data. </p></blockquote><p>FastText is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is <code>app</code>, <code>ppl</code>, and <code>ple</code>(ignoring the starting and ending of boundaries of words). The word embedding vector for apple will be the sum of all these n-grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words. </p><h2 id="Training-Models"><a href="#Training-Models" class="headerlink" title="Training Models"></a>Training Models</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">importimport  gensimgensim</span><br><span class="line"> importimport  osos</span><br><span class="line"> fromfrom  gensim.models.word2vecgensim.  <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="keyword">from</span> gensim.models.fasttext <span class="keyword">import</span> FastText <span class="keyword">as</span> FT_gensim</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set file names for train and test data</span></span><br><span class="line">data_dir = <span class="string">'&#123;&#125;'</span>.format(os.sep).join([gensim.__path__[<span class="number">0</span>], <span class="string">'test'</span>, <span class="string">'test_data'</span>]) + os.sep</span><br><span class="line">lee_train_file = data_dir + <span class="string">'lee_background.cor'</span></span><br><span class="line">lee_data = LineSentence(lee_train_file)</span><br><span class="line"></span><br><span class="line">model_gensim = FT_gensim(size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build the vocabulary</span></span><br><span class="line">model_gensim.build_vocab(lee_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train the model</span></span><br><span class="line">model_gensim.train(lee_data, total_examples=model_gensim.corpus_count, epochs=model_gensim.iter)</span><br><span class="line"></span><br><span class="line">print(model_gensim)</span><br><span class="line">----</span><br><span class="line">FastText(vocab=<span class="number">1763</span>, size=<span class="number">100</span>, alpha=<span class="number">0.025</span>)</span><br></pre></td></tr></table></figure><p>训练中使用的相关参数：</p><ul><li>model: Training architecture. Allowed values: <code>cbow</code>, <code>skipgram</code> (Default <code>cbow</code>) </li><li>size: Size of embeddings to be learnt (Default 100) </li><li>alpha: Initial learning rate (Default 0.025) </li><li>window: Context window size (Default 5) </li><li>min_count: Ignore words with number of occurrences below this (Default 5) </li><li>loss: Training objective. Allowed values: <code>ns</code>, <code>hs</code>, <code>softmax</code> (Default <code>ns</code>) </li><li>sample: Threshold for downsampling higher-frequency words (Default 0.001) </li><li>negative: Number of negative words to sample, for <code>ns</code> (Default 5) </li><li>iter: Number of epochs (Default 5) </li><li>sorted_vocab: Sort vocab by descending frequency (Default 1) </li><li>threads: Number of threads to use (Default 12) </li><li>min_n: min length of char ngrams (Default 3) </li><li>max_n: max length of char ngrams (Default 6) </li><li>bucket: number of buckets used for hashing ngrams (Default 2000000) </li></ul><h3 id="模型的存储与加载"><a href="#模型的存储与加载" class="headerlink" title="模型的存储与加载"></a>模型的存储与加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># saving a model trained via Gensim's fastText implementation</span></span><br><span class="line">model_gensim.save(<span class="string">'saved_model_gensim'</span>)</span><br><span class="line">loaded_model = FT_gensim.load(<span class="string">'saved_model_gensim'</span>)</span><br><span class="line">print(loaded_model)</span><br></pre></td></tr></table></figure><h2 id="Using-FastText"><a href="#Using-FastText" class="headerlink" title="Using FastText"></a>Using FastText</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> FastText</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = FastText.load_fasttext_format(<span class="string">"./data/fasttext/cc.zh.300.bin"</span>)</span><br><span class="line">len(model.wv.vocab)</span><br><span class="line">w1 = <span class="string">"人工智能"</span></span><br><span class="line">w1 <span class="keyword">in</span> model.wv.vocab</span><br><span class="line">model.wv.most_similar(w1)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb" target="_blank" rel="noopener">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb</a></li><li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb" target="_blank" rel="noopener">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb</a></li><li><a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank" rel="noopener">models.fasttext – FastText model</a></li><li><a href="https://towardsdatascience.com/fasttext-batteries-included-fa23f46d52e4" target="_blank" rel="noopener">https://towardsdatascience.com/fasttext-batteries-included-fa23f46d52e4</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/10.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://blog.a-stack.com/categories/NLP/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="词嵌入" scheme="http://blog.a-stack.com/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/"/>
    
      <category term="FastText" scheme="http://blog.a-stack.com/tags/FastText/"/>
    
  </entry>
  
  <entry>
    <title>弄懂word2vec之原理解析</title>
    <link href="http://blog.a-stack.com/2018/08/21/%E5%BC%84%E6%87%82word2vec%E4%B9%8B%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    <id>http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/</id>
    <published>2018-08-21T05:36:53.000Z</published>
    <updated>2018-08-24T15:18:05.202Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/07.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="关键点提要"><a href="#关键点提要" class="headerlink" title="关键点提要"></a>关键点提要</h2><ul><li>word2vec不是深度学习模型（只有输入层、输出层）</li><li>效果并非最好，但易用性最强（其实FastText开源之后易用性直逼Word2vec）</li><li>受训练语料库影响大，尤其中文语料库本身缺乏</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Word2vec核心算法包括CBOW和skip-gram，再加上两种目标优化方式层次Softmax和负采样的组合，可以形成总共四种实现方式。需要特别理解的一点，其实词向量是模型的中间产物（词向量矩阵在模型中为隐藏层的权重值）而不是优化目标产物。模型的训练数据为（sourceword，targetword）的词语对。本文简单整理近段时间通过翻看代码、原理解析总结的一些对word2vec的认知，希望能够加深对整合词嵌入模型的理解。</p><p><img src="/2018/08/21/弄懂word2vec之原理解析/word2vec_architectures.png" alt="word2vec_architectures"></p><p>其中</p><ul><li>CBOW的实现原理为由中心词来预测前后的相关词；</li><li>Skip-Gram正好相反，为由中心词的前后相关词来倒推中心词为某词的可能性；</li><li>无论哪种方法，都不需要事先对数据进行标记，换句话说这可以看作一种无监督的学习方法。</li></ul><blockquote><p>有相关研究也对两种方法的实现效果进行了比较，CBOW在小数据集上表现要优于Skip-Gram，在大数据集合上两者相当。</p></blockquote><p><img src="/2018/08/21/弄懂word2vec之原理解析/模型原理示意.png" alt="模型原理示意"></p><h2 id="训练数据的定义"><a href="#训练数据的定义" class="headerlink" title="训练数据的定义"></a>训练数据的定义</h2><blockquote><p>详见代码</p></blockquote><p>在生成训练数据之前先要明确窗口的概念，窗口的概念与n-gram中强调的前后词之前的语义关联有关，用于描述某个词语与其后多个词语有直接或间接的关系。在word2vec中使用的是中心词前后各n个词组成的窗口大小。原则上，窗口设置越大预测效果越好，但计算量等比例增大，一般选择5-8作为恰当的窗口值。</p><p><strong>Embedding Lookup的解释</strong>：词向量存储为一个以词典词个数为行，词嵌入维度为列的权重矩阵，而输入单词可以被编码为数字值，利用数值在词向量矩阵中获得词的向量表达的过程成为Embedding Lookup.</p><p><img src="/2018/08/21/弄懂word2vec之原理解析/tokenize_lookup.png" alt="tokenize_lookup"></p><p>Words that show up often such as “the”, “of”, and “for” don’t provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we’ll discard it with probability given by </p><script type="math/tex; mode=display"> P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}</script><p>where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.</p><h3 id="训练数据的切分"><a href="#训练数据的切分" class="headerlink" title="训练数据的切分"></a>训练数据的切分</h3><p><img src="/2018/08/21/弄懂word2vec之原理解析/training_data.png" alt="training_data"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">corpus = <span class="string">''' I read magazine weekly . </span></span><br><span class="line"><span class="string">I read newspaper daily . I like to read books daily .'''</span></span><br><span class="line"></span><br><span class="line">corpus_tokens = []</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus.split(<span class="string">'.'</span>):</span><br><span class="line">    corpus_tokens.append(text.lower().split())</span><br><span class="line"></span><br><span class="line"><span class="comment">########## Create vocabulary </span></span><br><span class="line">vocab = &#123;&#125;</span><br><span class="line">counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> sentence_tokens <span class="keyword">in</span> corpus_tokens:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence_tokens:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            vocab[word] = counter</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########## Create training pairs .</span></span><br><span class="line">window = <span class="number">5</span></span><br><span class="line">target_words = []</span><br><span class="line">target_words_index = []</span><br><span class="line">c = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> corpus_tokens:</span><br><span class="line">    <span class="keyword">for</span> index,center_word <span class="keyword">in</span> enumerate(i):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            right = i[index+<span class="number">1</span>: index+<span class="number">1</span>+window]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            right == []</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            left = i[index<span class="number">-1</span>: (index<span class="number">-1</span>)+window]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            left == []</span><br><span class="line">        total_words = list(set(right+left))</span><br><span class="line">        <span class="keyword">if</span> total_words:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> total_words:</span><br><span class="line">                <span class="keyword">if</span> center_word <span class="keyword">in</span> vocab <span class="keyword">and</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">                    target_words.append((center_word, word))</span><br><span class="line">                    target_words_index.append((vocab[center_word], vocab[word]))</span><br><span class="line">    c += <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="网络的构建"><a href="#网络的构建" class="headerlink" title="网络的构建"></a>网络的构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">embedding_dimension = <span class="number">100</span></span><br><span class="line">n_classes           = len(vocab) </span><br><span class="line">X = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>, (len(vocab), embedding_dimension)) <span class="comment"># input features for X </span></span><br><span class="line">W = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>, (len(vocab), embedding_dimension))   </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> each_pair <span class="keyword">in</span> target_words_index:</span><br><span class="line">    </span><br><span class="line">    inp_pair_index = each_pair[<span class="number">0</span>]</span><br><span class="line">    op_pair_index  = each_pair[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    x_input = random_embeddings[inp_pair_index] <span class="comment"># 1 x embedding_dimension </span></span><br><span class="line">    </span><br><span class="line">    probs = softmax(np.dot(W, x_input))        <span class="comment"># 1 x n_classes   </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">####### loss(probs, op_pair_index)</span></span><br><span class="line">    <span class="comment">####### backpropagate , update W</span></span><br></pre></td></tr></table></figure><blockquote><p>虽然示意代码中使用了softmax，但从计算量上来看这并非一种好的方法； Word2vec中使用了两种替代方案，可以大大节约计算成本：分别是层次softmax和副采样算法； </p></blockquote><h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><p><img src="/2018/08/21/弄懂word2vec之原理解析/skip_gram_net_arch.png" alt="skip_gram_net_arch"></p><h2 id="负采样损失函数定义"><a href="#负采样损失函数定义" class="headerlink" title="负采样损失函数定义"></a>负采样损失函数定义</h2><p><img src="/2018/08/21/弄懂word2vec之原理解析/sampled-softmax.png" alt="sampled-softmax"></p><p>其中前一部分为真实结果的损失惩罚，后面的K部分为噪声词组的K次采样。</p><p>TensorFlow实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Number of negative labels to sample</span></span><br><span class="line">n_sampled = <span class="number">100</span></span><br><span class="line"><span class="keyword">with</span> train_graph.as_default():</span><br><span class="line">    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) <span class="comment"># create softmax weight matrix here</span></span><br><span class="line">    softmax_b = tf.Variable(tf.zeros(n_vocab), name=<span class="string">"softmax_bias"</span>) <span class="comment"># create softmax biases here</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate the loss using negative sampling</span></span><br><span class="line">    loss = tf.nn.sampled_softmax_loss(</span><br><span class="line">        weights=softmax_w,</span><br><span class="line">        biases=softmax_b,</span><br><span class="line">        labels=labels,</span><br><span class="line">        inputs=embed,</span><br><span class="line">        num_sampled=n_sampled,</span><br><span class="line">        num_classes=n_vocab)</span><br><span class="line">    </span><br><span class="line">    cost = tf.reduce_mean(loss)</span><br><span class="line">    optimizer = tf.train.AdamOptimizer().minimize(cost)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/29364112" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29364112</a></li><li><a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/representation/word2vec</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/07.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://blog.a-stack.com/categories/NLP/"/>
    
    
      <category term="词嵌入" scheme="http://blog.a-stack.com/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/"/>
    
      <category term="word2vec" scheme="http://blog.a-stack.com/tags/word2vec/"/>
    
      <category term="原理解析" scheme="http://blog.a-stack.com/tags/%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>弄懂word2vec之实践</title>
    <link href="http://blog.a-stack.com/2018/08/21/%E5%BC%84%E6%87%82word2vec%E4%B9%8B%E5%AE%9E%E8%B7%B5/"/>
    <id>http://blog.a-stack.com/2018/08/21/弄懂word2vec之实践/</id>
    <published>2018-08-21T05:34:57.000Z</published>
    <updated>2018-08-24T06:26:01.190Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/08.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="训练词向量模型"><a href="#训练词向量模型" class="headerlink" title="训练词向量模型"></a>训练词向量模型</h2><h3 id="工具准备"><a href="#工具准备" class="headerlink" title="工具准备"></a>工具准备</h3><ul><li><p>opencc 1.04</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、下载包地址（我下载的是opencc-1.0.4.tar.gz）：</span></span><br><span class="line"><span class="comment"># https://bintray.com/package/files/byvoid/opencc/OpenCC</span></span><br><span class="line"><span class="comment"># https://bintray.com/byvoid/opencc/download_file?file_path=opencc-1.0.4.tar.gz</span></span><br><span class="line"><span class="comment">#2、进入tar.gz目录，命令行解压：</span></span><br><span class="line">tar -xzvf opencc-1.0.4.tar.gz</span><br><span class="line"><span class="comment"># 3、编译（需要工具cmake、gcc（4.6）gcc -v查看gcc版本、doxygen）</span></span><br><span class="line"><span class="built_in">cd</span> opencc-1.0.4/</span><br><span class="line">make</span><br><span class="line"><span class="comment"># 4.安装</span></span><br><span class="line">sudo make install</span><br><span class="line"><span class="comment"># 5.使用</span></span><br><span class="line">opencc -i &lt;input-file&gt; -o output-file&gt; -c &lt;config-file&gt; </span><br><span class="line"><span class="comment"># 例如：opencc -i wiki.zh.txt -o wiki.zh.simp.txt -c t2s.json</span></span><br></pre></td></tr></table></figure></li><li><p>gensim</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gensim</span><br></pre></td></tr></table></figure></li><li><p>Wikipedia Extractor</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install unzip python python-dev python-pip</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/attardi/wikiextractor.git wikiextractor</span><br><span class="line">$ <span class="built_in">cd</span> wikiextractor</span><br><span class="line">$ sudo python setup.py install</span><br></pre></td></tr></table></figure></li><li><p>jieba</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li></ul><h3 id="下载语料库并准备预料资源"><a href="#下载语料库并准备预料资源" class="headerlink" title="下载语料库并准备预料资源"></a>下载语料库并准备预料资源</h3><ol><li>标准的word2vec中文语料库目前影响范围最广的为维基百科语料库，下载地址：</li></ol><ul><li><a href="https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2" target="_blank" rel="noopener">https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2</a> （1.3G）</li></ul><ol><li><p>抽取语料正文</p><p>可以使用工具<code>Wikipedia Extractor</code>进行抽取：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./WikiExtractor.py -b 1024M -o extracted zhwiki-latest-pages-articles.xml.bz2</span><br></pre></td></tr></table></figure><blockquote><p>参数-b 1024M表示以1024M为单位切分文件，默认是1M。</p></blockquote><p>（推荐，处理完成可以跳过第4步）也可以使用网上大牛写的处理脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> gensim.corpora.wikicorpus <span class="keyword">import</span> extract_pages,filter_wiki</span><br><span class="line"><span class="keyword">import</span> bz2file</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> opencc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line">wiki = extract_pages(bz2file.open(<span class="string">'zhwiki-latest-pages-articles.xml.bz2'</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wiki_replace</span><span class="params">(d)</span>:</span></span><br><span class="line">    s = d[<span class="number">1</span>]</span><br><span class="line">    s = re.sub(<span class="string">':*&#123;\|[\s\S]*?\|&#125;'</span>, <span class="string">''</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'&lt;gallery&gt;[\s\S]*?&lt;/gallery&gt;'</span>, <span class="string">''</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'(.)&#123;&#123;([^&#123;&#125;\n]*?\|[^&#123;&#125;\n]*?)&#125;&#125;'</span>, <span class="string">'\\1[[\\2]]'</span>, s)</span><br><span class="line">    s = filter_wiki(s)</span><br><span class="line">    s = re.sub(<span class="string">'\* *\n|\'&#123;2,&#125;'</span>, <span class="string">''</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'\n+'</span>, <span class="string">'\n'</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'\n[:;]|\n +'</span>, <span class="string">'\n'</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'\n=='</span>, <span class="string">'\n\n=='</span>, s)</span><br><span class="line">    s = <span class="string">u'【'</span> + d[<span class="number">0</span>] + <span class="string">u'】\n'</span> + s</span><br><span class="line">    <span class="keyword">return</span> opencc.convert(s).strip()</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">f = codecs.open(<span class="string">'wiki.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">w = tqdm(wiki, desc=<span class="string">u'已获取0篇文章'</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> re.findall(<span class="string">'^[a-zA-Z]+:'</span>, d[<span class="number">0</span>]) <span class="keyword">and</span> d[<span class="number">0</span>] <span class="keyword">and</span> <span class="keyword">not</span> re.findall(<span class="string">u'^#'</span>, d[<span class="number">1</span>]):</span><br><span class="line">        s = wiki_replace(d)</span><br><span class="line">        f.write(s+<span class="string">'\n\n\n'</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            w.set_description(<span class="string">u'已获取%s篇文章'</span>%i)</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure></li><li><p>繁体转简体</p><p>维基百科的中文数据是繁简混杂的，里面包含大陆简体、台湾繁体、港澳繁体等多种不同的数据。有时候在一篇文章的不同段落间也会使用不同的繁简字。</p><p>为了处理方便起见，我们直接使用了开源项目<code>opencc</code>。参照安装说明的方法，安装完成之后，使用下面的命令进行繁简转换，整个过程也很快：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install opencc</span><br><span class="line">$ opencc -i wiki_00 -o zh_wiki_00 -c zht2zhs.ini</span><br></pre></td></tr></table></figure><p>命令中的<code>wiki_00</code>这个文件是此前使用<code>Wikipedia Extractor</code>得到的。到了这里，我们已经完成了大部分繁简转换工作。</p></li><li><p>预处理</p><p>由于Wikipedia Extractor抽取正文时，会将有特殊标记的外文直接剔除。我们最后再将「」『』这些符号替换成引号，顺便删除空括号，就大功告成了！代码如下： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfun</span><span class="params">(input_file)</span>:</span></span><br><span class="line">    p1 = re.compile(<span class="string">ur'-\&#123;.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\&#125;-'</span>)</span><br><span class="line">    p2 = re.compile(<span class="string">ur'[（\(][，；。？！\s]*[）\)]'</span>)</span><br><span class="line">    p3 = re.compile(<span class="string">ur'[「『]'</span>)</span><br><span class="line">    p4 = re.compile(<span class="string">ur'[」』]'</span>)</span><br><span class="line">    outfile = codecs.open(<span class="string">'std_'</span> + input_file, <span class="string">'w'</span>, <span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">with</span> codecs.open(input_file, <span class="string">'r'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> myfile:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> myfile:</span><br><span class="line">            line = p1.sub(<span class="string">ur'\2'</span>, line)</span><br><span class="line">            line = p2.sub(<span class="string">ur''</span>, line)</span><br><span class="line">            line = p3.sub(<span class="string">ur'“'</span>, line)</span><br><span class="line">            line = p4.sub(<span class="string">ur'”'</span>, line)</span><br><span class="line">   <span class="comment"># line = re.sub('[a-zA-Z0-9]','',line)</span></span><br><span class="line">            outfile.write(line)</span><br><span class="line">    outfile.close()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Usage: python script.py inputfile"</span></span><br><span class="line">        sys.exit()</span><br><span class="line">    reload(sys)</span><br><span class="line">    sys.setdefaultencoding(<span class="string">'utf-8'</span>)</span><br><span class="line">    input_file = sys.argv[<span class="number">1</span>]</span><br><span class="line">    myfun(input_file)</span><br></pre></td></tr></table></figure></li></ol><p>5.中文分词</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python -m jieba -d <span class="string">" "</span> ./std_zh_wiki_00 &gt; ./cut_std_zh_wiki_00</span><br></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>模型训练使用工具<code>gensim</code>，可以通过<code>pip install gensim</code>直接安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import word2vec</span><br><span class="line">import logging</span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line">sentences = word2vec.LineSentence(u<span class="string">'./cut_std_zh_wiki_00'</span>)</span><br><span class="line">model = word2vec.Word2Vec(sentences,size=200,window=5,min_count=5,workers=4)</span><br><span class="line">outp1 = <span class="string">'wiki.zh.text.model'</span></span><br><span class="line">outp2 = <span class="string">'wiki.zh.text.vector'</span></span><br><span class="line">model.save(outp1)</span><br><span class="line">model.wv.save_word2vec_format(outp2, binary=False)</span><br></pre></td></tr></table></figure><h3 id="训练效果评价"><a href="#训练效果评价" class="headerlink" title="训练效果评价"></a>训练效果评价</h3><h4 id="词聚类"><a href="#词聚类" class="headerlink" title="词聚类"></a>词聚类</h4><p>可以采用 kmeans 聚类，看聚类簇的分布</p><h4 id="词cos-相关性"><a href="#词cos-相关性" class="headerlink" title="词cos 相关性"></a>词cos 相关性</h4><p>查找cos相近的词</p><h4 id="Analogy对比"><a href="#Analogy对比" class="headerlink" title="Analogy对比"></a>Analogy对比</h4><p>a:b 与 c:d的cos距离 (man-king woman-queen )</p><h4 id="使用tnse，pca等降维可视化展示"><a href="#使用tnse，pca等降维可视化展示" class="headerlink" title="使用tnse，pca等降维可视化展示"></a>使用tnse，pca等降维可视化展示</h4><p>词的分布，推荐用google的<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="noopener">tensorboard</a>，可以多视角查看，如果不想搭建服务，直接<a href="https://link.zhihu.com/?target=http%3A//projector.tensorflow.org/" target="_blank" rel="noopener">访问这里</a>。另外可以用python的matplotlib。</p><p><img src="/2018/08/21/弄懂word2vec之实践/v2-7a3b04197eab7a2ccc40bfab3f81b7f6_hd.jpg" alt="img"></p><p><img src="/2018/08/21/弄懂word2vec之实践/v2-e8f100b3b0b5be4c343317e2db2bb706_hd.jpg" alt="img"></p><h4 id="Categorization-分类-看词在每个分类中的概率"><a href="#Categorization-分类-看词在每个分类中的概率" class="headerlink" title="Categorization 分类 看词在每个分类中的概率"></a>Categorization 分类 看词在每个分类中的概率</h4><div class="table-container"><table><thead><tr><th>词</th><th>动物</th><th>食物</th><th>汽车</th><th>电子</th></tr></thead><tbody><tr><td>橘子</td><td>0.11</td><td>0.68</td><td>0.12</td><td>0.11</td></tr><tr><td>鸟</td><td>0.66</td><td>0.11</td><td>0.13</td><td>0.11</td></tr><tr><td>雅阁</td><td>0.14</td><td>0.23</td><td>0.67</td><td>0.11</td></tr><tr><td>苹果</td><td>0.11</td><td>0.65</td><td>0.11</td><td>0.65</td></tr></tbody></table></div><blockquote><p>前三条来自<a href="https://link.zhihu.com/?target=https%3A//code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">官网的评测</a>方法</p><p>网上也有相关的word embedding 的评估方法，可以<a href="https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/D15-1036" target="_blank" rel="noopener">参考这里</a></p></blockquote><h2 id="模型使用"><a href="#模型使用" class="headerlink" title="模型使用"></a>模型使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="comment"># 模型加载</span></span><br><span class="line">model = gensim.models.word2vec.Word2Vec.load(<span class="string">'/data2/word2vec/new.model'</span>)</span><br></pre></td></tr></table></figure><ol><li>比较词关联</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.wv.most_similar(positive=[<span class="string">'woman'</span>, <span class="string">'king'</span>], negative=[<span class="string">'man'</span>], topn=<span class="number">1</span>)</span><br><span class="line">[(<span class="string">'queen'</span>, <span class="number">0.50882536</span>)]</span><br><span class="line">model.doesnt_match(<span class="string">"breakfast cereal dinner lunch"</span>;.split())</span><br><span class="line"><span class="string">'cereal'</span></span><br><span class="line">model.similarity(<span class="string">'woman'</span>, <span class="string">'man'</span>)</span><br><span class="line"><span class="number">0.73723527</span></span><br></pre></td></tr></table></figure><ol><li>输出词向量</li></ol><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">model</span>[<span class="string">'中国'</span>]</span><br></pre></td></tr></table></figure><h3 id="利用word2vec实现的关键词提取"><a href="#利用word2vec实现的关键词提取" class="headerlink" title="利用word2vec实现的关键词提取"></a>利用word2vec实现的关键词提取</h3><blockquote><p>参考自<a href="https://kexue.fm/archives/4316" target="_blank" rel="noopener">【不可思议的Word2Vec】 3.提取关键词</a> </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line">model = gensim.models.word2vec.Word2Vec.load(<span class="string">'word2vec_wx'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_proba</span><span class="params">(oword, iword)</span>:</span></span><br><span class="line">    iword_vec = model[iword]</span><br><span class="line">    oword = model.wv.vocab[oword]</span><br><span class="line">    oword_l = model.syn1[oword.point].T</span><br><span class="line">    dot = np.dot(iword_vec, oword_l)</span><br><span class="line">    lprob = -sum(np.logaddexp(<span class="number">0</span>, -dot) + oword.code*dot) </span><br><span class="line">    <span class="keyword">return</span> lprob</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keywords</span><span class="params">(s)</span>:</span></span><br><span class="line">    s = [w <span class="keyword">for</span> w <span class="keyword">in</span> s <span class="keyword">if</span> w <span class="keyword">in</span> model]</span><br><span class="line">    ws = &#123;w:sum([predict_proba(u, w) <span class="keyword">for</span> u <span class="keyword">in</span> s]) <span class="keyword">for</span> w <span class="keyword">in</span> s&#125;</span><br><span class="line">    <span class="keyword">return</span> Counter(ws).most_common()</span><br><span class="line">  </span><br><span class="line"> s = <span class="string">u'2018年7月17日，国务院食品安全办在全国食品安全宣传周主会场公布了7起食品保健食品欺诈和虚假宣传整治案件：一、广东广州钟某、林某等制售非法添加药品的食品案2017年10月，广东省广州市公安机关联合食品药品监管部门破获钟某、林某等制售非法添加药品的食品案，涉案货值逾亿元。查获非法添加西药成分的咖啡1万多盒、半成品及原料粉末3吨。林某等在咖啡、蜂蜜、减肥茶等食品中非法添加他达拉非、西布曲明等西药成分，并假冒其他品牌食品销售。钟某、林某等5名犯罪嫌疑人已被依法批捕。二、河北石家庄秦某等制售非法添加药品的食品案2018年1月，河北省石家庄市公安机关、食品药品监管部门联手侦破秦某等制售非法添加药品的食品案，涉案货值3500余万元，捣毁生产、销售、储存窝点7处，打掉加工生产线12条，食品成品6500余盒。秦某等非法添加苯乙双胍等西药成分生产“胰宝唐安” “唐康”等宣称降糖功能的食品，通过电话推销、聘用假冒专家等进行销售。秦某等犯罪嫌疑人已被依法批捕。'</span></span><br><span class="line">pd.Series(keywords(jieba.cut(s)))</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line"><span class="number">0</span>        (制售, <span class="number">-1987.67916328</span>)</span><br><span class="line"><span class="number">1</span>        (药品, <span class="number">-2023.50590123</span>)</span><br><span class="line"><span class="number">2</span>        (食品, <span class="number">-2037.03856341</span>)</span><br><span class="line"><span class="number">3</span>        (查获, <span class="number">-2042.32583652</span>)</span><br><span class="line"><span class="number">4</span>        (窝点, <span class="number">-2056.79129232</span>)</span><br><span class="line"><span class="number">5</span>      (保健食品, <span class="number">-2064.19993636</span>)</span><br><span class="line"><span class="number">6</span>        (西药, <span class="number">-2073.77825261</span>)</span><br><span class="line"><span class="number">7</span>      (监管部门, <span class="number">-2082.29241248</span>)</span><br><span class="line"><span class="number">8</span>        (非法, <span class="number">-2089.17887737</span>)</span><br><span class="line"><span class="number">9</span>        (销售, <span class="number">-2099.67683021</span>)</span><br><span class="line"><span class="number">10</span>     (食品安全, <span class="number">-2104.52601517</span>)</span><br><span class="line"><span class="number">11</span>       (涉案, <span class="number">-2106.51930849</span>)</span><br><span class="line"><span class="number">12</span>        (加工, <span class="number">-2110.4709589</span>)</span><br><span class="line"><span class="number">13</span>       (破获, <span class="number">-2111.47298087</span>)</span><br><span class="line"><span class="number">14</span>        (案, <span class="number">-2111.82806077</span>)</span><br><span class="line"><span class="number">15</span>       (欺诈, <span class="number">-2112.95137531</span>)</span><br><span class="line"><span class="number">16</span>       (生产, <span class="number">-2115.57760446</span>)</span><br><span class="line"><span class="number">17</span>       (原料, <span class="number">-2124.36521695</span>)</span><br><span class="line"><span class="number">18</span>       (案件, <span class="number">-2125.21520575</span>)</span><br><span class="line"><span class="number">19</span>        (等, <span class="number">-2126.20670381</span>)</span><br><span class="line"><span class="number">20</span>       (依法, <span class="number">-2126.65278663</span>)</span><br><span class="line"><span class="number">21</span>     (公安机关, <span class="number">-2130.61494485</span>)</span><br><span class="line"><span class="number">22</span>       (咖啡, <span class="number">-2140.98083555</span>)</span><br><span class="line"><span class="number">23</span>      (半成品, <span class="number">-2143.70307556</span>)</span><br><span class="line"><span class="number">24</span>        (虚假, <span class="number">-2144.3973437</span>)</span><br><span class="line"><span class="number">25</span>        (侦破, <span class="number">-2144.4260255</span>)</span><br><span class="line"><span class="number">26</span>       (假冒, <span class="number">-2145.73825612</span>)</span><br><span class="line"><span class="number">27</span>       (曲明, <span class="number">-2147.48180124</span>)</span><br><span class="line"><span class="number">28</span>     (食品药品, <span class="number">-2151.19137764</span>)</span><br><span class="line"><span class="number">29</span>       (专家, <span class="number">-2151.42459357</span>)</span><br><span class="line">                ...          </span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure><h3 id="基于词向量的聚类分析"><a href="#基于词向量的聚类分析" class="headerlink" title="基于词向量的聚类分析"></a>基于词向量的聚类分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line">sentences = [[<span class="string">'this'</span>, <span class="string">'is'</span>, <span class="string">'the'</span>, <span class="string">'good'</span>, <span class="string">'machine'</span>, <span class="string">'learning'</span>, <span class="string">'book'</span>],</span><br><span class="line">            [<span class="string">'this'</span>, <span class="string">'is'</span>,  <span class="string">'another'</span>, <span class="string">'book'</span>],</span><br><span class="line">            [<span class="string">'one'</span>, <span class="string">'more'</span>, <span class="string">'book'</span>],</span><br><span class="line">            [<span class="string">'this'</span>, <span class="string">'is'</span>, <span class="string">'the'</span>, <span class="string">'new'</span>, <span class="string">'post'</span>],</span><br><span class="line">                        [<span class="string">'this'</span>, <span class="string">'is'</span>, <span class="string">'about'</span>, <span class="string">'machine'</span>, <span class="string">'learning'</span>, <span class="string">'post'</span>],  </span><br><span class="line">            [<span class="string">'and'</span>, <span class="string">'this'</span>, <span class="string">'is'</span>, <span class="string">'the'</span>, <span class="string">'last'</span>, <span class="string">'post'</span>]]</span><br><span class="line">model = Word2Vec(sentences, min_count=<span class="number">1</span>)</span><br><span class="line">X = model[model.wv.vocab]</span><br></pre></td></tr></table></figure><ol><li><p>使用nltk.cluster.KMeansClusterer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from nltk.cluster import KMeansClusterer</span><br><span class="line">import nltk</span><br><span class="line">NUM_CLUSTERS=3</span><br><span class="line">kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)</span><br><span class="line">assigned_clusters = kclusterer.cluster(X, assign_clusters=True)</span><br><span class="line"><span class="built_in">print</span> (assigned_clusters)</span><br><span class="line"></span><br><span class="line">[1, 0, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 1, 0]</span><br><span class="line">words = list(model.wv.vocab)</span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(words):  </span><br><span class="line">    <span class="built_in">print</span> (word + <span class="string">":"</span> + str(assigned_clusters[i]))</span><br><span class="line">post:1</span><br><span class="line">one:0</span><br><span class="line">the:2</span><br><span class="line">machine:2</span><br><span class="line">another:0</span><br><span class="line">this:2</span><br><span class="line">last:2</span><br><span class="line">is:2</span><br><span class="line">book:2</span><br><span class="line">learning:2</span><br><span class="line">new:1</span><br><span class="line">good:0</span><br><span class="line">more:2</span><br><span class="line">and:1</span><br><span class="line">about:0</span><br></pre></td></tr></table></figure></li><li><p>使用sklearn.Keams</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> cluster</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line"> </span><br><span class="line">labels = kmeans.labels_</span><br><span class="line">centroids = kmeans.cluster_centers_</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Cluster id labels for inputted data"</span>)</span><br><span class="line"><span class="keyword">print</span> (labels)</span><br><span class="line"><span class="comment">#print ("Centroids data")</span></span><br><span class="line"><span class="comment">#print (centroids)</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):"</span>)</span><br><span class="line"><span class="keyword">print</span> (kmeans.score(X))</span><br><span class="line"> </span><br><span class="line">silhouette_score = metrics.silhouette_score(X, labels, metric=<span class="string">'euclidean'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Silhouette_score: "</span>)</span><br><span class="line"><span class="keyword">print</span> (silhouette_score)</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">Cluster id labels <span class="keyword">for</span> inputted data</span><br><span class="line">[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span>]</span><br><span class="line">Score (Opposite of the value of X on the K-means objective which <span class="keyword">is</span> Sum of distances of samples to their closest cluster center):</span><br><span class="line"><span class="number">-0.00961963500595</span></span><br><span class="line">Silhouette_score: </span><br><span class="line"><span class="number">0.0289495</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Word2vec的可视化"><a href="#Word2vec的可视化" class="headerlink" title="Word2vec的可视化"></a>Word2vec的可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.tensorboard.plugins <span class="keyword">import</span> projector</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(model, output_path)</span>:</span></span><br><span class="line">    meta_file = <span class="string">"w2x_metadata.tsv"</span></span><br><span class="line">    placeholder = np.zeros((len(model.wv.index2word), <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(output_path,meta_file), <span class="string">'wb'</span>) <span class="keyword">as</span> file_metadata:</span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(model.wv.index2word):</span><br><span class="line">            placeholder[i] = model[word]</span><br><span class="line">            <span class="comment"># temporary solution for https://github.com/tensorflow/tensorflow/issues/9094</span></span><br><span class="line">            <span class="keyword">if</span> word == <span class="string">''</span>:</span><br><span class="line">                print(<span class="string">"Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard"</span>)</span><br><span class="line">                file_metadata.write(<span class="string">"&#123;0&#125;"</span>.format(<span class="string">'&lt;Empty Line&gt;'</span>).encode(<span class="string">'utf-8'</span>) + <span class="string">b'\n'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                file_metadata.write(<span class="string">"&#123;0&#125;"</span>.format(word).encode(<span class="string">'utf-8'</span>) + <span class="string">b'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define the model without training</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">    embedding = tf.Variable(placeholder, trainable = <span class="keyword">False</span>, name = <span class="string">'w2x_metadata'</span>)</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    writer = tf.summary.FileWriter(output_path, sess.graph)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># adding into projector</span></span><br><span class="line">    config = projector.ProjectorConfig()</span><br><span class="line">    embed = config.embeddings.add()</span><br><span class="line">    embed.tensor_name = <span class="string">'w2x_metadata'</span></span><br><span class="line">    embed.metadata_path = meta_file</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Specify the width and height of a single thumbnail.</span></span><br><span class="line">    projector.visualize_embeddings(writer, config)</span><br><span class="line">    saver.save(sess, os.path.join(output_path,<span class="string">'w2x_metadata.ckpt'</span>))</span><br><span class="line">    print(<span class="string">'Run `tensorboard --logdir=&#123;0&#125;` to run visualize result on tensorboard'</span>.format(output_path))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    model = Word2Vec.load(<span class="string">"./data/model/word2vec_wx"</span>)</span><br><span class="line">    visualize(model,<span class="string">"./visual/"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Run `tensorboard --logdir=./visual/` to run visualize result on tensorboard</span><br></pre></td></tr></table></figure><h2 id="Word2vec的应用场景"><a href="#Word2vec的应用场景" class="headerlink" title="Word2vec的应用场景"></a>Word2vec的应用场景</h2><blockquote><p>参考自<a href="http://www.zhuanzhi.ai/document/b614a5ceb61c95574515415a53cbf2e3" target="_blank" rel="noopener">word2vec在工业界的应用场景</a></p></blockquote><h3 id="在社交网络中的推荐"><a href="#在社交网络中的推荐" class="headerlink" title="在社交网络中的推荐"></a>在社交网络中的推荐</h3><p>在个性化推荐的场景，给当前用户推荐他可能关注的『大V』。对一个新用户，此题基本无解，如果在已知用户关注了几个『大V』之后，相当于知道了当前用户的一些关注偏好，根据此偏好给他推荐和他关注过大V相似的大V，就是一个很不错的推荐策略。所以，如果可以求出来任何两个V用户的相似度，上面问题就可以基本得到解决。</p><p>我们知道word2vec中两个词的相似度可以直接通过余弦来衡量，接下来就是如何将每个V用户变为一个词向量的问题了。巧妙的地方就是如何定义doc和word，针对上面问题，可以将doc和word定义为：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">word</span> -&gt;</span>   每一个大V就是一个词</span><br><span class="line"><span class="function"><span class="title">doc</span>  -&gt;</span>   根据每一个用户关注大V的顺序，生成一篇文章</span><br></pre></td></tr></table></figure><p>由于用户量很大（大约4亿），可以将关注word个数少的doc删掉，因为本身大V的种类是十万级别（如果我没记错的话）， 选择可以覆盖绝大多数大V的文章数量就足够了。</p><h3 id="计算商品的相似度"><a href="#计算商品的相似度" class="headerlink" title="计算商品的相似度"></a>计算商品的相似度</h3><p>在商品推荐的场景中，竞品推荐和搭配推荐的时候都有可能需要计算任何两个商品的相似度，根据浏览/收藏/下单/App下载等行为，可以将商品看做词，将每一个用户的一类行为序看做一个文档，通过word2vec将其训练为一个向量。</p><p>同样的，在计算广告中，根据用户的点击广告的点击序列，将每一个广告变为一个向量。变为向量后，用此向量可以生成特征融入到rank模型中。</p><h3 id="作为另一个模型的输入"><a href="#作为另一个模型的输入" class="headerlink" title="作为另一个模型的输入"></a>作为另一个模型的输入</h3><p>在nlp的任务中，可以通过将词聚类后，生成一维新的特征来使用。在CRF实体识别的任务中，聚类结果类似词性，可以作为特征来使用。</p><p>在依存句法分析的任务中，哈工大ltp的nndepparser则是将词向量直接作为输入。</p><p>具体论文『A Fast and Accurate Dependency Parser using Neural Networks』</p><h3 id="向量快速检索"><a href="#向量快速检索" class="headerlink" title="向量快速检索"></a>向量快速检索</h3><p>当我们将一个文档变成一个向量之后，如何根据余弦/欧氏距离快速得到其最相似的top k个文章，是工程实现上不得不考虑的问题。例如线上可以允许的时间是5ms以内，如果文章数量往往上万或者更多，O(n)的方式计算明显不可接受了。</p><p>如果文章更新的速度很慢，可以通过离线的方式一天或者几天计算一次，导入redis（或者别的）提供线上快速查询。 但是如果文章实时新增，并且大量流量来自新文章，这个问题就要好好考虑一下。</p><p>一般可以通过PQ, kd-tree、simhash、聚类等方式解决，选择不同的方式和具体的推荐场景、数据分布有关。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/29364112" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29364112</a></li><li><a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">https://code.google.com/archive/p/word2vec/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/08.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://blog.a-stack.com/categories/NLP/"/>
    
    
      <category term="实践" scheme="http://blog.a-stack.com/tags/%E5%AE%9E%E8%B7%B5/"/>
    
      <category term="word2vec" scheme="http://blog.a-stack.com/tags/word2vec/"/>
    
      <category term="词向量" scheme="http://blog.a-stack.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
</feed>
