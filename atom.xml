<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ebby&#39;s Notes</title>
  
  <subtitle>=Blog for AI Learning=</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.a-stack.com/"/>
  <updated>2018-12-26T13:16:58.578Z</updated>
  <id>http://blog.a-stack.com/</id>
  
  <author>
    <name>Ebby DD</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习资源调度平台OpenPAI部署</title>
    <link href="http://blog.a-stack.com/2018/12/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0OpenPAI%E9%83%A8%E7%BD%B2/"/>
    <id>http://blog.a-stack.com/2018/12/26/深度学习资源调度平台OpenPAI部署/</id>
    <published>2018-12-26T10:42:53.000Z</published>
    <updated>2018-12-26T13:16:58.578Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/12/26/深度学习资源调度平台OpenPAI部署/sysarch.png" alt="OpenPAI"></p><a id="more"></a><blockquote><p>date: 2018-12-26</p><p>Version: 0.8.3</p><p>操作系统：Ubuntu 16.04</p><p>GitHub: <a href="https://github.com/Microsoft/pai" target="_blank" rel="noopener">https://github.com/Microsoft/pai</a></p><p>All-in-One部署</p></blockquote><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="1-环境要求"><a href="#1-环境要求" class="headerlink" title="1. 环境要求"></a>1. 环境要求</h3><ul><li>一组集群主机，PC或服务器均可；</li><li>所有节点全新安装<strong>Ubuntu Server 16.04</strong>，不需安装GPU驱动及CUDA；</li><li>各节点具有统一的登录账户及密码，此账户不需要root账户角色，但<strong>必须具有sudo权限</strong>；</li><li>各节点具有静态IP地址；</li><li>各节点均可访问互联网；</li><li>各节点间可互访；</li><li>各节点时间一致，即<strong>ntp</strong>功能可用；</li></ul><h3 id="2-依赖包安装"><a href="#2-依赖包安装" class="headerlink" title="2. 依赖包安装"></a>2. 依赖包安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0.1</span></span><br><span class="line">gaoc@openpaiTest03:~$ sudo apt-get -y update</span><br><span class="line"><span class="comment"># 0.2</span></span><br><span class="line">gaoc@openpaiTest03:~$ sudo apt-get -y install \</span><br><span class="line">      nano \</span><br><span class="line">      vim \</span><br><span class="line">      joe \</span><br><span class="line">      wget \</span><br><span class="line">      curl \</span><br><span class="line">      jq \</span><br><span class="line">      gawk \</span><br><span class="line">      psmisc \</span><br><span class="line">      python \</span><br><span class="line">      python-yaml \</span><br><span class="line">      python-jinja2 \</span><br><span class="line">      python-paramiko \</span><br><span class="line">      python-urllib3 \</span><br><span class="line">      python-tz \</span><br><span class="line">      python-nose \</span><br><span class="line">      python-prettytable \</span><br><span class="line">      python-netifaces \</span><br><span class="line">      python-dev \</span><br><span class="line">      python-pip \</span><br><span class="line">      python-mysqldb \</span><br><span class="line">      openjdk-8-jre \</span><br><span class="line">      openjdk-8-jdk \</span><br><span class="line">      openssh-server \</span><br><span class="line">      openssh-client \</span><br><span class="line">      git \</span><br><span class="line">      bash-completion \</span><br><span class="line">      inotify-tools \</span><br><span class="line">      rsync \</span><br><span class="line">      realpath \</span><br><span class="line">      net-tools</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 0.3</span></span><br><span class="line">gaoc@openpaiTest03:~$ pip install python-etcd docker kubernetes GitPython</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.4 </span></span><br><span class="line">gaoc@openpaiTest03:~$ git <span class="built_in">clone</span> https://github.com/Microsoft/pai.git</span><br></pre></td></tr></table></figure><blockquote><ul><li><p>0.3 pip包安装失败报错如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;   ...</span><br><span class="line">&gt;     File <span class="string">"/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py"</span>, line 118, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">&gt;       SSL_ST_INIT = _lib.SSL_ST_INIT</span><br><span class="line">&gt;   AttributeError: <span class="string">'module'</span> object has no attribute <span class="string">'SSL_ST_INIT'</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><p>  修复如下：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;   <span class="comment">#1.1</span></span><br><span class="line">&gt;   sudo python -m easy_install --upgrade pyOpenSSL</span><br><span class="line">&gt;   <span class="comment">#1.2 更新pip版本</span></span><br><span class="line">&gt;   gaoc@openpaiTest03:~$ pip install --upgrade pip</span><br><span class="line">&gt;   <span class="comment">#1.3 修复pip新版本bug </span></span><br><span class="line">&gt;   sudo vim /usr/bin/pip</span><br><span class="line">&gt;   </span><br><span class="line">&gt;   from pip import __main__</span><br><span class="line">&gt;   <span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">&gt;       sys.exit(__main__._main())</span><br><span class="line">&gt;   <span class="comment">#1.4 重新安装包</span></span><br><span class="line">&gt;   gaoc@openpaiTest03:~$ pip install python-etcd docker kubernetes GitPython</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="配置文件准备"><a href="#配置文件准备" class="headerlink" title="配置文件准备"></a>配置文件准备</h2><ol><li>使用快速部署文件<code>quick-start.yaml</code></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gaoc@openpaiTest03:~$ <span class="built_in">cd</span> pai/deployment/quick-start/</span><br><span class="line">gaoc@openpaiTest03:~/pai/deployment/quick-start$ mv quick-start-example.yaml quick-start.yaml</span><br></pre></td></tr></table></figure><ol><li><p>修改<code>quick-start.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.</span></span><br><span class="line">machines:</span><br><span class="line">  - &lt;ip-of-master&gt;</span><br><span class="line">  - &lt;ip-of-worker1&gt;</span><br><span class="line">  - &lt;ip-of-worder2&gt;</span><br><span class="line"><span class="comment">#--------------------</span></span><br><span class="line"><span class="comment">#修改为本机内网IP地址，因为All-in-One,所以配置一个master IP即可：</span></span><br><span class="line">machines:</span><br><span class="line">  - 10.0.7.4</span><br><span class="line"><span class="comment">#2.</span></span><br><span class="line">ssh-username: &lt;username&gt;</span><br><span class="line">ssh-password: &lt;password&gt;</span><br><span class="line"><span class="comment">#--------------------</span></span><br><span class="line"><span class="comment">#修改为openPAI用户的用户名/密码</span></span><br></pre></td></tr></table></figure></li><li><p>生成配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /pai</span><br><span class="line"></span><br><span class="line"><span class="comment"># cmd should be executed under pai directory</span></span><br><span class="line"></span><br><span class="line">python paictl.py config generate -i ~/pai/deployment/quick-start/quick-start.yaml -o ~/pai-config -f</span><br></pre></td></tr></table></figure></li><li><p>修改生成的配置文件</p><blockquote><p>目录 <code>~/pai-config</code></p></blockquote><p><code>cluster-configuration.yaml  k8s-role-definition.yaml  kubernetes-configuration.yaml  services-configuration.yaml</code></p></li></ol><ul><li><p><code>`cluster-configuration.yaml</code></p><p>修改<code>machine-sku</code>,如果有需要进一步修改<code>machine-list</code></p><p>因此部署电脑没有GPU，将GPU的<code>count</code>调整为0，同时按照服务器内存和cpu虚拟核心数目调整参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">machine-sku:</span><br><span class="line">  GENERIC:</span><br><span class="line">    mem: 56</span><br><span class="line">    gpu:</span><br><span class="line">      <span class="built_in">type</span>: generic</span><br><span class="line">      count: 0</span><br><span class="line">    cpu:</span><br><span class="line">      vcore: 16</span><br><span class="line">    os: ubuntu16.04</span><br><span class="line"></span><br><span class="line">machine-list:</span><br><span class="line">  - hostname: openpaiTest03</span><br><span class="line">    hostip: 10.0.7.4</span><br><span class="line">    machine-type: GENERIC</span><br><span class="line">    k8s-role: master</span><br><span class="line">    etcdid: etcdid1</span><br><span class="line">    zkid: <span class="string">"1"</span></span><br><span class="line">    dashboard: <span class="string">"true"</span></span><br><span class="line">    pai-master: <span class="string">"true"</span></span><br><span class="line">    pai-worker: <span class="string">"true"</span></span><br><span class="line">    docker-data: /var/lib/docker</span><br></pre></td></tr></table></figure></li><li><p><code>kubernetes-configuration.yaml</code></p><p>修改docker registry为国内源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-registry: docker.io/openpai</span><br></pre></td></tr></table></figure></li><li><p><code>services-configuration.yaml</code></p><p>修改<code>docker-registry</code>的<code>tag</code>值为openPAI版本号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tag: 0.8.3</span><br></pre></td></tr></table></figure></li></ul><p>更多配置修改详见： <a href="https://github.com/Microsoft/pai/blob/master/docs/pai-management/doc/how-to-generate-cluster-config.md" target="_blank" rel="noopener">https://github.com/Microsoft/pai/blob/master/docs/pai-management/doc/how-to-generate-cluster-config.md</a></p><h2 id="部署Kubernetes-集群"><a href="#部署Kubernetes-集群" class="headerlink" title="部署Kubernetes 集群"></a>部署Kubernetes 集群</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/pai</span><br><span class="line"></span><br><span class="line">sudo python paictl.py cluster k8s-bootup -p ~/pai-config</span><br></pre></td></tr></table></figure><p>集群部署完成可通过如下地址访问集群管理系统：</p><p><code>http://&lt;master&gt;:9090</code></p><p>部署过程中遇到问题重新部署前先执行如下命令删掉<code>kubelet</code>的容器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo docker ps</span><br><span class="line">sudo docker stop kubelet</span><br><span class="line">sudo docker rm kubelet</span><br></pre></td></tr></table></figure><h2 id="更新集群配置"><a href="#更新集群配置" class="headerlink" title="更新集群配置"></a>更新集群配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/pai</span><br><span class="line">python paictl.py config push -p ~/pai-config</span><br></pre></td></tr></table></figure><p>执行完成之后会提示输入一个cluster id：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Please input the cluster-id <span class="keyword">for</span> your cluster: 123456</span><br></pre></td></tr></table></figure><h2 id="启动OpenPAI服务"><a href="#启动OpenPAI服务" class="headerlink" title="启动OpenPAI服务"></a>启动OpenPAI服务</h2><blockquote><p>此步骤将依次安装如下服务：</p><p><code>[&#39;watchdog&#39;, &#39;node-exporter&#39;, &#39;yarn-frameworklauncher&#39;, &#39;hadoop-name-node&#39;, &#39;cleaner&#39;, &#39;rest-server&#39;, &#39;grafana&#39;, &#39;hadoop-resource-manager&#39;, &#39;hadoop-batch-job&#39;, &#39;drivers&#39;, &#39;cluster-configuration&#39;, &#39;alert-manager&#39;, &#39;pylon&#39;, &#39;hadoop-jobhistory&#39;, &#39;hadoop-node-manager&#39;, &#39;end-to-end-test&#39;, &#39;job-exporter&#39;, &#39;webportal&#39;, &#39;zookeeper&#39;, &#39;prometheus&#39;, &#39;hadoop-data-node&#39;]</code></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/pai</span><br><span class="line">python paictl.py service start</span><br></pre></td></tr></table></figure><p>此时可以通过kebernetes的管理界面查看服务所在容器的部署情况：<code>http://127.0.0.1:9090/#!/job?namespace=default</code></p><p><img src="/2018/12/26/深度学习资源调度平台OpenPAI部署/k8s-management.png" alt="k8s-management"></p><p>在此步骤中遇到了<code>watchdog</code>无法部署成功的问题，指定的配置文件版本无法找到对应0.8.3的watchdog：</p><p><img src="/2018/12/26/深度学习资源调度平台OpenPAI部署/watchdog-error.png" alt="watchdog-error"></p><ul><li><p>解决办法</p><ul><li><p>通过如下命令删除已经部署的watchdog容器</p></li><li><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python paictl.py service delete -n watchdog</span><br></pre></td></tr></table></figure><p>更新<code>pai-config</code>中的配置文件<code>services-configuration.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">namespace: xudifsd</span><br><span class="line">tag: latest</span><br></pre></td></tr></table></figure></li><li><p>重新执行配置更新命令</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/pai</span><br><span class="line">python paictl.py<span class="built_in"> config </span>push -p ~/pai-config</span><br></pre></td></tr></table></figure></li><li><p>重启启动服务部署命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/pai</span><br><span class="line">python paictl.py service start</span><br></pre></td></tr></table></figure></li></ul></li></ul><blockquote><p><code>end-to-end-test</code>服务可以通过如下命令删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;python paictl.py service delete -n end-to-end-test</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>最后，可以通过如下链接的方法验证部署成功与否：</p><p><a href="https://github.com/Microsoft/pai/blob/master/docs/pai-management/doc/validate-deployment.md" target="_blank" rel="noopener">https://github.com/Microsoft/pai/blob/master/docs/pai-management/doc/validate-deployment.md</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2018/12/26/深度学习资源调度平台OpenPAI部署/sysarch.png&quot; alt=&quot;OpenPAI&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手实践营" scheme="http://blog.a-stack.com/categories/%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E8%90%A5/"/>
    
    
      <category term="部署" scheme="http://blog.a-stack.com/tags/%E9%83%A8%E7%BD%B2/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="资源调度" scheme="http://blog.a-stack.com/tags/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/"/>
    
      <category term="开源平台" scheme="http://blog.a-stack.com/tags/%E5%BC%80%E6%BA%90%E5%B9%B3%E5%8F%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习——EDA</title>
    <link href="http://blog.a-stack.com/2018/12/11/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94EDA/"/>
    <id>http://blog.a-stack.com/2018/12/11/机器在学习——EDA/</id>
    <published>2018-12-11T06:46:47.000Z</published>
    <updated>2018-12-12T12:38:55.539Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/12/11/机器在学习——EDA/pairplot.png" alt="EDA"></p><a id="more"></a><hr><h1 id="Topic-1：-EDA-with-pandas"><a href="#Topic-1：-EDA-with-pandas" class="headerlink" title="Topic 1： EDA with pandas"></a>Topic 1： EDA with pandas</h1><ol><li><p>调整column类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'Churn'</span>] = df[<span class="string">'Churn'</span>].astype(<span class="string">'int64'</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>df.describe()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.describe(include=[<span class="string">'object'</span>, <span class="string">'bool'</span>])</span><br></pre></td></tr></table></figure></li><li><p><code>df.value_counts(normalize=True)</code></p><blockquote><p>对于离散量数据（<code>object</code>或<code>bool</code>）统计不同类型的数目</p></blockquote><p>另外可以通过如下命令来统计一个离散属性的所有类别数目：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flights_df[<span class="string">'UniqueCarrier'</span>].nunique()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flights_df.groupby(<span class="string">'UniqueCarrier'</span>).size().plot(kind=<span class="string">'bar'</span>);</span><br><span class="line"><span class="comment">#等价于</span></span><br><span class="line">flights_df[<span class="string">'UniqueCarrier'</span>].value_counts().plot(kind=<span class="string">'bar'</span>)</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>排序方法：<code>sort_values(by=&#39;...&#39;,ascending=False)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.sort_values(by=<span class="string">'Total day charge'</span>, ascending=<span class="keyword">False</span>).head()</span><br><span class="line"></span><br><span class="line">df.sort_values(by=[<span class="string">'Churn'</span>, <span class="string">'Total day charge'</span>],</span><br><span class="line">        ascending=[<span class="keyword">True</span>, <span class="keyword">False</span>])</span><br></pre></td></tr></table></figure></li><li><p>Boolean indexing：<code>df[P(df[&#39;Name&#39;])]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[(df[<span class="string">'Churn'</span>] == <span class="number">0</span>) &amp; (df[<span class="string">'International plan'</span>] == <span class="string">'No'</span>)][<span class="string">'Total intl minutes'</span>].max()</span><br></pre></td></tr></table></figure></li><li><p><code>.loc</code>与<code>.iloc</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.loc[<span class="number">0</span>:<span class="number">5</span>, <span class="string">'State'</span>:<span class="string">'Area code'</span>]</span><br><span class="line">df.iloc[<span class="number">0</span>:<span class="number">5</span>, <span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">df[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure></li><li><p><code>apply</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将apply应用到每一行，注意添加 axis=1</span></span><br><span class="line">df[<span class="string">'State'</span>].apply(<span class="keyword">lambda</span> state: state[<span class="number">0</span>] == <span class="string">'W'</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>map</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'No'</span> : <span class="keyword">False</span>, <span class="string">'Yes'</span> : <span class="keyword">True</span>&#125;</span><br><span class="line">df[<span class="string">'International plan'</span>] = df[<span class="string">'International plan'</span>].map(d)</span><br></pre></td></tr></table></figure></li><li><p><code>Grouping</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.groupby(by=grouping_columns)[columns_to_show].function()</span><br><span class="line"></span><br><span class="line">columns_to_show = [<span class="string">'Total day minutes'</span>, <span class="string">'Total eve minutes'</span>, </span><br><span class="line">                   <span class="string">'Total night minutes'</span>]</span><br><span class="line"></span><br><span class="line">df.groupby([<span class="string">'Churn'</span>])[columns_to_show].agg([np.mean, np.std, np.min, </span><br><span class="line">                                            np.max])</span><br></pre></td></tr></table></figure></li><li><p>Summary Tables</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.crosstab(df[<span class="string">'Churn'</span>], df[<span class="string">'International plan'</span>])</span><br></pre></td></tr></table></figure></li></ol><h1 id="Topic-2：-EDA-with-plot"><a href="#Topic-2：-EDA-with-plot" class="headerlink" title="Topic 2： EDA with plot"></a>Topic 2： EDA with plot</h1><blockquote><p>收集 ML toolbox： <code>pandas</code>,<code>matplotlib</code>,<code>seaborn</code></p></blockquote><h2 id="1-看数据分布"><a href="#1-看数据分布" class="headerlink" title="1. 看数据分布"></a>1. 看数据分布</h2><h3 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h3><blockquote><p>使用<code>pandas</code> DataFrame的<code>hist()</code>方法直接绘制：将数据划分至等长区间(bins)。</p><p>用于统计数据分布，在不同区间的计数值；</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[features].hist(figsize=(<span class="number">10</span>, <span class="number">4</span>));</span><br></pre></td></tr></table></figure><h3 id="Box图"><a href="#Box图" class="headerlink" title="Box图"></a>Box图</h3><p>箱形图（Box-plot）又称为盒须图、盒式图或箱线图，是一种用作显示一组数据分散情况资料的统计图。它能显示出一组数据的<strong>最大值</strong>、<strong>最小值</strong>、<strong>中位数</strong>及<strong>上下四分位数</strong>。因形状如箱子而得名。在各种领域也经常被使用，常见于品质管理。</p><p>接下来我们介绍Seaborn中的箱型图的具体实现方法，这是boxplot的API：</p><blockquote><p>seaborn.boxplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, dodge=True, fliersize=5, linewidth=None, whis=1.5, notch=False, ax=None, **kwargs)</p></blockquote><h3 id="Kernel-Density-Plots"><a href="#Kernel-Density-Plots" class="headerlink" title="Kernel Density Plots"></a>Kernel Density Plots</h3><blockquote><p>看作为直方图的平滑版本</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[features].plot(kind=<span class="string">'density'</span>, subplots=<span class="keyword">True</span>, layout=(<span class="number">1</span>, <span class="number">2</span>), </span><br><span class="line">                  sharex=<span class="keyword">False</span>, figsize=(<span class="number">10</span>, <span class="number">4</span>));</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/kernel-density-plots.png" alt="kernel-density-plots"></p><h3 id="Seaborn的displot方法"><a href="#Seaborn的displot方法" class="headerlink" title="Seaborn的displot方法"></a>Seaborn的<code>displot</code>方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(df[<span class="string">'Total intl calls'</span>]);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/sns.distplot.png" alt="sns.distplot"></p><h3 id="pandas的describe-方法"><a href="#pandas的describe-方法" class="headerlink" title="pandas的describe()方法"></a><code>pandas</code>的<code>describe()</code>方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[features].describe()</span><br></pre></td></tr></table></figure><blockquote><p>也可以通过seaborn的box方法</p></blockquote><h2 id="2-离散变量"><a href="#2-离散变量" class="headerlink" title="2. 离散变量"></a>2. 离散变量</h2><h3 id="pandas的value-counts"><a href="#pandas的value-counts" class="headerlink" title="pandas的value_counts"></a><code>pandas</code>的<code>value_counts</code></h3><blockquote><p> <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html" target="_blank" rel="noopener"><code>value_counts()</code></a> </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'feature_name'</span>].value_counts(normalize=Fase)</span><br></pre></td></tr></table></figure><h3 id="seaborn的countplot"><a href="#seaborn的countplot" class="headerlink" title="seaborn的countplot"></a><code>seaborn</code>的<code>countplot</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, axes = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">sns.countplot(x=<span class="string">'Churn'</span>, data=df, ax=axes[<span class="number">0</span>]);</span><br><span class="line">sns.countplot(x=<span class="string">'Customer service calls'</span>, data=df, ax=axes[<span class="number">1</span>]);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/barplot.png" alt="barplot"></p><h3 id="lmplot"><a href="#lmplot" class="headerlink" title="lmplot"></a><code>lmplot</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.lmplot(<span class="string">'Total day minutes'</span>, <span class="string">'Total night minutes'</span>, data=df, hue=<span class="string">'Churn'</span>, fit_reg=<span class="keyword">False</span>);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/lmplot.png" alt="lmplot"></p><h2 id="3-关联分析"><a href="#3-关联分析" class="headerlink" title="3. 关联分析"></a>3. 关联分析</h2><h3 id="相关性分析"><a href="#相关性分析" class="headerlink" title="相关性分析"></a>相关性分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Drop non-numerical variables</span></span><br><span class="line">numerical = list(set(df.columns) - </span><br><span class="line">                 set([<span class="string">'State'</span>, <span class="string">'International plan'</span>, <span class="string">'Voice mail plan'</span>, </span><br><span class="line">                      <span class="string">'Area code'</span>, <span class="string">'Churn'</span>, <span class="string">'Customer service calls'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate and plot</span></span><br><span class="line">corr_matrix = df[numerical].corr()</span><br><span class="line">sns.heatmap(corr_matrix);</span><br></pre></td></tr></table></figure><h3 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h3><blockquote><p><a href="https://seaborn.pydata.org/generated/seaborn.jointplot.html" target="_blank" rel="noopener"><code>jointplot()</code></a> </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.jointplot(x=<span class="string">'Total day minutes'</span>, y=<span class="string">'Total night minutes'</span>, </span><br><span class="line">              data=df, kind=<span class="string">'scatter'</span>);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/scatter.png" alt="scatter"></p><h4 id="散点图的相关性矩阵：-pairplot"><a href="#散点图的相关性矩阵：-pairplot" class="headerlink" title="散点图的相关性矩阵： pairplot"></a>散点图的相关性矩阵： <code>pairplot</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `pairplot()` may become very slow with the SVG format</span></span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">sns.pairplot(df[numerical]);</span><br></pre></td></tr></table></figure><p><img src="/2018/12/11/机器在学习——EDA/pairplot.png" alt="pairplot"></p><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><h2 id="技巧总结"><a href="#技巧总结" class="headerlink" title="技巧总结"></a>技巧总结</h2><ol><li><p><code>seaborn</code>的<code>hue</code>属性是一个可视化很直接，比较有用的配置属性；</p><p>比如通过下图来看某个特征对于结果的不同分布：</p><p><img src="/2018/12/11/机器在学习——EDA/count-plot-hue.png" alt="count-plot-hue"></p></li><li><p>在量化离散变量之前，将数值变量和类别变量分别进行分析</p></li><li><p>环境准备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Matplotlib forms basis for visualization in Python</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># We will use the Seaborn library</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Graphics in SVG format are more sharp and legible</span></span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Increase the default plot size and set the color scheme</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = <span class="number">8</span>, <span class="number">5</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'viridis'</span></span><br></pre></td></tr></table></figure></li><li><p>熟练使用特征选择命令，限定画图范围</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top_features = df[<span class="string">'features'</span>].value_counts().sort_values(ascending=<span class="keyword">False</span>).head(<span class="number">5</span>).index.values</span><br></pre></td></tr></table></figure></li><li><p>善用<code>pandas.crosstab</code>进行分析</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2018/12/11/机器在学习——EDA/pairplot.png&quot; alt=&quot;EDA&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-时序数据分析</title>
    <link href="http://blog.a-stack.com/2018/12/04/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0-%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://blog.a-stack.com/2018/12/04/机器在学习-时序数据分析/</id>
    <published>2018-12-04T08:50:11.000Z</published>
    <updated>2018-12-11T04:00:45.956Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="Test Picture"></p><a id="more"></a><h2 id="算法理论"><a href="#算法理论" class="headerlink" title="算法理论"></a>算法理论</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><blockquote><p><strong>Time series</strong> is a series of data points indexed (or listed or graphed) in time order. </p></blockquote><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination" target="_blank" rel="noopener">R squared</a>: coefficient of determination (in econometrics, this can be interpreted as the percentage of variance explained by the model), $(-\infty, 1]$</li></ul><script type="math/tex; mode=display">R^2 = 1 - \frac{SS_{res}}{SS_{tot}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.r2_score</span><br></pre></td></tr></table></figure><hr><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error" target="_blank" rel="noopener">Mean Absolute Error</a>: this is an interpretable metric because it has the same unit of measurment as the initial series, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MAE = \frac{\sum\limits_{i=1}^{n} |y_i - \hat{y}_i|}{n}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_absolute_error</span><br></pre></td></tr></table></figure><hr><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error" target="_blank" rel="noopener">Median Absolute Error</a>: again, an interpretable metric that is particularly interesting because it is robust to outliers, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MedAE = median(|y_1 - \hat{y}_1|, ... , |y_n - \hat{y}_n|)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.median_absolute_error</span><br></pre></td></tr></table></figure><hr><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error" target="_blank" rel="noopener">Mean Squared Error</a>: the most commonly used metric that gives a higher penalty to large errors and vice versa, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MSE = \frac{1}{n}\sum\limits_{i=1}^{n} (y_i - \hat{y}_i)^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error</span><br></pre></td></tr></table></figure><hr><ul><li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error" target="_blank" rel="noopener">Mean Squared Logarithmic Error</a>: practically, this is the same as MSE, but we take the logarithm of the series. As a result, we give more weight to small mistakes as well. This is usually used when the data has exponential trends, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MSLE = \frac{1}{n}\sum\limits_{i=1}^{n} (log(1+y_i) - log(1+\hat{y}_i))^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_log_error</span><br></pre></td></tr></table></figure><hr><ul><li>Mean Absolute Percentage Error: this is the same as MAE but is computed as a percentage, which is very convenient when you want to explain the quality of the model to management, $[0, +\infty)$</li></ul><script type="math/tex; mode=display">MAPE = \frac{100}{n}\sum\limits_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{y_i}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_absolute_percentage_error</span><span class="params">(y_true, y_pred)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> np.mean(np.abs((y_true - y_pred) / y_true)) * <span class="number">100</span></span><br></pre></td></tr></table></figure><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="使用相关模型库"><a href="#使用相关模型库" class="headerlink" title="使用相关模型库"></a>使用相关模型库</h3><ol><li><p><a href="http://statsmodels.sourceforge.net/stable/" target="_blank" rel="noopener">statsmodels</a> library </p><blockquote><p>R的统计学习库，被移植到了python中，包含对于时序数据分析的各种函数</p></blockquote></li><li></li></ol><h3 id="滑动平均"><a href="#滑动平均" class="headerlink" title="滑动平均"></a>滑动平均</h3><p><a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html" target="_blank" rel="noopener"><code>DataFrame.rolling(window).mean()</code></a> </p><h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><h3 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h3><p>随机森林支持树的并行构建和预测结果的并行计算，这可以通过 <code>n_jobs</code> 参数实现。 如果设置 <code>n_jobs = k</code> ，则计算被划分为 <code>k</code> 个作业，并运行在机器的 <code>k</code> 个核上。 如果设置 <code>n_jobs = -1</code> ，则使用机器的所有核。 注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 <code>k</code> 个作业不会快 <code>k</code> 倍）。 当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>《机器学习》——周志华</p></li><li><p><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</p></li></ol><p>3.<a href="http://sklearn.apachecn.org/cn/0.19.0/" target="_blank" rel="noopener">Sklearn文档</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="时序数据" scheme="http://blog.a-stack.com/tags/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Time Series" scheme="http://blog.a-stack.com/tags/Time-Series/"/>
    
      <category term="SARIMA" scheme="http://blog.a-stack.com/tags/SARIMA/"/>
    
      <category term="Prophet" scheme="http://blog.a-stack.com/tags/Prophet/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-线性回归与分类</title>
    <link href="http://blog.a-stack.com/2018/11/29/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://blog.a-stack.com/2018/11/29/机器在学习-随机森林/</id>
    <published>2018-11-29T08:50:11.000Z</published>
    <updated>2018-12-11T04:04:47.431Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="Test Picture"></p><h2 id="算法理论"><a href="#算法理论" class="headerlink" title="算法理论"></a>算法理论</h2><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p><strong>集成方法</strong> 的目标是把多个使用给定学习算法构建的基估计器的预测结果结合起来，从而获得比单个估计器更好的泛化能力/鲁棒性。</p><p>集成方法通常分为两种:</p><ul><li><p><strong>平均方法</strong>，该方法的原理是构建多个独立的估计器，然后取它们的预测结果的平均。一般来说组合之后的估计器是会比单个估计器要好的，因为它的方差减小了。</p><p><strong>示例:</strong> <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#bagging" target="_blank" rel="noopener">Bagging 方法</a> , <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#forest" target="_blank" rel="noopener">随机森林</a> , …</p></li><li><p>相比之下，在 <strong>boosting 方法</strong> 中，基估计器是依次构建的，并且每一个基估计器都尝试去减少组合估计器的偏差。这种方法主要目的是为了结合多个弱模型，使集成的模型更加强大。</p><p><strong>示例:</strong> <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#adaboost" target="_blank" rel="noopener">AdaBoost</a> , <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#gradient-boosting" target="_blank" rel="noopener">梯度提升树</a> , …</p><a id="more"></a></li></ul><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>在随机森林中（参见 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" target="_blank" rel="noopener"><code>RandomForestClassifier</code></a> 和 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" target="_blank" rel="noopener"><code>RandomForestRegressor</code></a> 类）， 集成模型中的每棵树构建时的样本都是由训练集经过有放回抽样得来的（例如，自助采样法-bootstrap sample）。 另外，在构建树的过程中进行结点分割时，选择的分割点不再是所有特征中最佳分割点，而是特征的一个随机子集中的最佳分割点。 由于这种随机性，森林的偏差通常会有略微的增大（相对于单个非随机树的偏差），但是由于取了平均，其方差也会减小，通常能够补偿偏差的增加，从而产生一个总体上更好的模型。 </p><script type="math/tex; mode=display">\large p_{+} = \frac{OR_{+}}{1 + OR_{+}} = \frac{\exp^{\textbf{w}^\text{T}\textbf{x}}}{1 + \exp^{\textbf{w}^\text{T}\textbf{x}}} = \frac{1}{1 + \exp^{-\textbf{w}^\text{T}\textbf{x}}} = \sigma(\textbf{w}^\text{T}\textbf{x})</script><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>要调整的参数主要是 <code>n_estimators</code> 和 <code>max_features</code>。 前者（n_estimators）是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好。 后者（max_features）是分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多。 根据经验，回归问题中使用 <code>max_features = n_features</code>， 分类问题使用 <code>max_features = sqrt（n_features）</code> （其中 <code>n_features</code> 是特征的个数）是比较好的默认值。 <code>max_depth = None</code> 和 <code>min_samples_split = 2</code> 结合通常会有不错的效果（即生成完全的树）。 请记住，这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由交叉验证获得。 另外，请注意，在随机森林中，默认使用自助采样法（<code>bootstrap = True</code>）， 然而 extra-trees 的默认策略是使用整个数据集（<code>bootstrap = False</code>）。 当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外的样本来估算的，设置 <code>oob_score = True</code> 即可实现。</p><blockquote><p><strong>提示:</strong></p><p>默认参数下模型复杂度是：<code>O(M*N*log(N))</code> ， 其中 <code>M</code> 是树的数目， <code>N</code> 是样本数。 可以通过设置以下参数来降低模型复杂度： <code>min_samples_split</code> , <code>min_samples_leaf</code> , <code>max_leaf_nodes`` 和 ``max_depth</code> 。</p></blockquote><h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><h3 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h3><p>随机森林支持树的并行构建和预测结果的并行计算，这可以通过 <code>n_jobs</code> 参数实现。 如果设置 <code>n_jobs = k</code> ，则计算被划分为 <code>k</code> 个作业，并运行在机器的 <code>k</code> 个核上。 如果设置 <code>n_jobs = -1</code> ，则使用机器的所有核。 注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 <code>k</code> 个作业不会快 <code>k</code> 倍）。 当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>《机器学习》——周志华</p></li><li><p><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</p></li></ol><p>3.<a href="http://sklearn.apachecn.org/cn/0.19.0/" target="_blank" rel="noopener">Sklearn文档</a></p><p>- </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;算法理论&quot;&gt;&lt;a href=&quot;#算法理论&quot; class=&quot;headerlink&quot; title=&quot;算法理论&quot;&gt;&lt;/a&gt;算法理论&lt;/h2&gt;&lt;h3 id=&quot;集成学习&quot;&gt;&lt;a href=&quot;#集成学习&quot; class=&quot;headerlink&quot; title=&quot;集成学习&quot;&gt;&lt;/a&gt;集成学习&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;集成方法&lt;/strong&gt; 的目标是把多个使用给定学习算法构建的基估计器的预测结果结合起来，从而获得比单个估计器更好的泛化能力/鲁棒性。&lt;/p&gt;
&lt;p&gt;集成方法通常分为两种:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;平均方法&lt;/strong&gt;，该方法的原理是构建多个独立的估计器，然后取它们的预测结果的平均。一般来说组合之后的估计器是会比单个估计器要好的，因为它的方差减小了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例:&lt;/strong&gt; &lt;a href=&quot;http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#bagging&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bagging 方法&lt;/a&gt; , &lt;a href=&quot;http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#forest&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;随机森林&lt;/a&gt; , …&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;相比之下，在 &lt;strong&gt;boosting 方法&lt;/strong&gt; 中，基估计器是依次构建的，并且每一个基估计器都尝试去减少组合估计器的偏差。这种方法主要目的是为了结合多个弱模型，使集成的模型更加强大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例:&lt;/strong&gt; &lt;a href=&quot;http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#adaboost&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AdaBoost&lt;/a&gt; , &lt;a href=&quot;http://sklearn.apachecn.org/cn/0.19.0/modules/ensemble.html#gradient-boosting&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;梯度提升树&lt;/a&gt; , …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-KNN</title>
    <link href="http://blog.a-stack.com/2018/11/23/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0%E2%80%94-KNN/"/>
    <id>http://blog.a-stack.com/2018/11/23/机器在学习—-KNN/</id>
    <published>2018-11-23T08:50:37.000Z</published>
    <updated>2018-12-11T10:30:24.216Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="KNN"></p><a id="more"></a><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/classes.html#module-sklearn.neighbors" target="_blank" rel="noopener"><code>sklearn.neighbors</code></a> 提供了 neighbors-based (基于邻居的) 无监督学习以及监督学习方法的功能。 无监督的最近邻是许多其它学习方法的基础，尤其是 manifold learning (流行学习) 和 spectral clustering (谱聚类)。 neighbors-based (基于邻居的) 监督学习分为两种： <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html#classification" target="_blank" rel="noopener">classification</a> （分类）针对的是具有离散标签的数据，<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html#regression" target="_blank" rel="noopener">regression</a> （回归）针对的是具有连续标签的数据。 </p><ol><li>Calculate the distance to each of the samples in the training set.</li><li>Select 𝑘k samples from the training set with the minimal distance to them.</li><li>The class of the test sample will be the most frequent class among those 𝑘k nearest neighbors.</li></ol><blockquote><p>在Kaggle比赛中，KNN经常作为前期特征生成的算法；</p><p>算法处理有两个关键：</p><ul><li>K的选取；</li><li>距离的评价准则（Hamming，Euclidean，cosine, Minkowski distance）</li><li>所有属性必须进行归一化操作，保证距离计算的一致性</li></ul></blockquote><h3 id="K-D-树¶"><a href="#K-D-树¶" class="headerlink" title="K-D 树¶"></a>K-D 树<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html#k-d" target="_blank" rel="noopener">¶</a></h3><p>为了解决效率低下的暴力计算方法，已经发明了大量的基于树的数据结构。总的来说， 这些结构试图通过有效地编码样本的 aggregate distance (聚合距离) 信息来减少所需的距离计算量。 基本思想是，若A点距离B点非常远，B距离C点非常近， 可知 A点与 C点很遥远，<em>不需要明确计算它们的距离</em>。 通过这样的方式，近邻搜索的计算成本可以降低为 $O[DNlog(N)]$或更低。 这是对于暴力搜索在大样本数 N中表现的显著改善。</p><p>利用这种聚合信息的早期方法是 <em>KD tree</em> 数据结构（<em> K-dimensional tree</em> 的简写）, 它将二维 <em>Quad-trees</em> 和三维 <em>Oct-trees</em>推广到任意数量的维度. KD 树是一个二叉树结构，它沿着数据轴递归地划分参数空间，将其划分为嵌入数据点的嵌套的各向异性区域。 KD 树的构造非常快：因为只需沿数据轴执行分区, 无需计算 D-dimensional 距离。 一旦构建完成, 查询点的最近邻距离计算复杂度仅为 $O[log(N)]$。 虽然 KD 树的方法对于低维度 （D&lt;20）近邻搜索非常快, 当D增长到很大时, 效率变低: 这就是所谓的 “维度灾难” 的一种体现。 在 scikit-learn 中, KD 树近邻搜索可以使用关键字 <code>algorithm = &#39;kd_tree&#39;</code> 来指定, 并且使用类 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" target="_blank" rel="noopener"><code>KDTree</code></a> 来计算。</p><h2 id="工程实现"><a href="#工程实现" class="headerlink" title="工程实现"></a>工程实现</h2><p>The main parameters of the class <code>sklearn.neighbors.KNeighborsClassifier</code> are:</p><ul><li>weights: <code>uniform</code> (all weights are equal), <code>distance</code> (the weight is inversely proportional to the distance from the test sample), or any other user-defined function;</li><li>algorithm (optional): <code>brute</code>, <code>ball_tree</code>, <code>KD_tree</code>, or <code>auto</code>. In the first case, the nearest neighbors for each test case are computed by a grid search over the training set. In the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. If you set this parameter to <code>auto</code>, the right way to find the neighbors will be automatically chosen based on the training set.</li><li>leaf_size (optional): threshold for switching to grid search if the algorithm for finding neighbors is BallTree or KDTree;</li><li>metric: <code>minkowski</code>, <code>manhattan</code>, <code>euclidean</code>, <code>chebyshev</code>, or other.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">knn_pipe = Pipeline([(<span class="string">'scaler'</span>, StandardScaler()), (<span class="string">'knn'</span>, KNeighborsClassifier(n_jobs=<span class="number">-1</span>))])</span><br><span class="line"></span><br><span class="line">knn_params = &#123;<span class="string">'knn__n_neighbors'</span>: range(<span class="number">1</span>, <span class="number">10</span>)&#125;</span><br><span class="line"></span><br><span class="line">knn_grid = GridSearchCV(knn_pipe, knn_params,</span><br><span class="line">                        cv=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">knn_grid.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">knn_grid.best_params_, knn_grid.best_score_</span><br><span class="line"></span><br><span class="line">accuracy_score(y_holdout, knn_grid.predict(X_holdout))</span><br></pre></td></tr></table></figure><h2 id="KNN的优缺点"><a href="#KNN的优缺点" class="headerlink" title="KNN的优缺点"></a>KNN的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>实现简单，理论完备；</li><li>一般作为机器学习使用的第一个测试算法；</li><li>可解释性强；</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>处理大数据训练样本效率低；</li><li>难以处理特征数目多的数据集；</li><li>对距离测量算法依赖高；</li><li>K的选择也需要反复尝试；</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</li></ol><p>2.<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html" target="_blank" rel="noopener">Sklearn 最近邻</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;KNN&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-线性回归与分类</title>
    <link href="http://blog.a-stack.com/2018/11/23/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://blog.a-stack.com/2018/11/23/机器在学习-线性回归/</id>
    <published>2018-11-23T08:50:11.000Z</published>
    <updated>2018-11-26T10:08:04.170Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="Test Picture"></p><h2 id="算法理论"><a href="#算法理论" class="headerlink" title="算法理论"></a>算法理论</h2><script type="math/tex; mode=display">y = W^TX + b</script><p>更一般地，</p><script type="math/tex; mode=display">\large \textbf y = \textbf X \textbf w + \epsilon</script><p>其中 $\epsilon $ 为噪声，均值为0，方差服从正态分布。</p><ul><li>expectation of random errors is zero:  $\forall i: \mathbb{E}\left[\epsilon_i\right] = 0 $;</li><li>the random error has the same finite variance, this property is called <a href="https://en.wikipedia.org/wiki/Homoscedasticity" target="_blank" rel="noopener">homoscedasticity</a>:  $\forall i: \text{Var}\left(\epsilon_i\right) = \sigma^2 &lt; \infty $;</li><li>random errors are uncorrelated:  $\forall i \neq j: \text{Cov}\left(\epsilon_i, \epsilon_j\right) = 0 $.</li></ul><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><blockquote><p>最小化均方差误差值</p></blockquote><script type="math/tex; mode=display">\Large \begin{array}{rcl}\mathcal{L}\left(\textbf X, \textbf{y}, \textbf{w} \right) &=& \frac{1}{2n} \sum_{i=1}^n \left(y_i - \textbf{w}^\text{T} \textbf{x}_i\right)^2 \\&=& \frac{1}{2n} \left\| \textbf{y} - \textbf X \textbf{w} \right\|_2^2 \\&=& \frac{1}{2n} \left(\textbf{y} - \textbf X \textbf{w}\right)^\text{T} \left(\textbf{y} - \textbf X \textbf{w}\right)\end{array}</script><p>求解</p><script type="math/tex; mode=display">\Large \begin{array}{rcl} \frac{\partial \mathcal{L}}{\partial \textbf{w}} = 0 &\Leftrightarrow& \frac{1}{2n} \left(-2 \textbf{X}^{\text{T}} \textbf{y} + 2\textbf{X}^{\text{T}} \textbf{X} \textbf{w}\right) = 0 \\&\Leftrightarrow& -\textbf{X}^{\text{T}} \textbf{y} + \textbf{X}^{\text{T}} \textbf{X} \textbf{w} = 0 \\&\Leftrightarrow& \textbf{X}^{\text{T}} \textbf{X} \textbf{w} = \textbf{X}^{\text{T}} \textbf{y} \\&\Leftrightarrow& \textbf{w} = \left(\textbf{X}^{\text{T}} \textbf{X}\right)^{-1} \textbf{X}^{\text{T}} \textbf{y}\end{array}</script><blockquote><p>然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵$X$的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性（multicollinearity）的情况可能真的会出现。 </p></blockquote><h4 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h4><blockquote><p>在损失函数中增加对于权重的l2正则化；l1正则化叫做Lasso回归；</p></blockquote><h4 id="极大似然"><a href="#极大似然" class="headerlink" title="极大似然"></a>极大似然</h4><blockquote><p>最大化似然估计值</p></blockquote><script type="math/tex; mode=display">\Large \begin{array}{rcl} y_i &=& \sum_{j=1}^m w_j X_{ij} + \epsilon_i \\&\sim& \sum_{j=1}^m w_j X_{ij} + \mathcal{N}\left(0, \sigma^2\right) \\p\left(y_i \mid \textbf X; \textbf{w}\right) &=& \mathcal{N}\left(\sum_{j=1}^m w_j X_{ij}, \sigma^2\right)\end{array}</script><h3 id="偏差-方差理论"><a href="#偏差-方差理论" class="headerlink" title="偏差-方差理论"></a>偏差-方差理论</h3><h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><ul><li>true value of the target variable is the sum of a deterministic function $f\left(\textbf{x}\right)$ and random error $\epsilon$: $y = f\left(\textbf{x}\right) + \epsilon$;</li><li>error is normally distributed with zero mean and some variance: $\epsilon \sim \mathcal{N}\left(0, \sigma^2\right)$;</li><li>true value of the target variable is also normally distributed: $y \sim \mathcal{N}\left(f\left(\textbf{x}\right), \sigma^2\right)$;</li><li>we try to approximate a deterministic but unknown function $f\left(\textbf{x}\right)$ using a linear function of the covariates $\widehat{f}\left(\textbf{x}\right)$, which, in turn, is a point estimate of the function $f$ in function space (specifically, the family of linear functions that we have limited our space to), i.e. a random variable that has mean and variance.</li></ul><h4 id="误差公式推导"><a href="#误差公式推导" class="headerlink" title="误差公式推导"></a>误差公式推导</h4><script type="math/tex; mode=display">\Large \begin{array}{rcl} \text{Err}\left(\textbf{x}\right) &=& \mathbb{E}\left[\left(y - \widehat{f}\left(\textbf{x}\right)\right)^2\right] \\&=& \mathbb{E}\left[y^2\right] + \mathbb{E}\left[\left(\widehat{f}\left(\textbf{x}\right)\right)^2\right] - 2\mathbb{E}\left[y\widehat{f}\left(\textbf{x}\right)\right] \\&=& \mathbb{E}\left[y^2\right] + \mathbb{E}\left[\widehat{f}^2\right] - 2\mathbb{E}\left[y\widehat{f}\right] \\\end{array}</script><p>由$\text{Var}\left(z\right) = \mathbb{E}\left[z^2\right] - \mathbb{E}\left[z\right]^2$得，</p><script type="math/tex; mode=display">\Large \begin{array}{rcl} \mathbb{E}\left[y^2\right] &=& \text{Var}\left(y\right) + \mathbb{E}\left[y\right]^2 = \sigma^2 + f^2\\\mathbb{E}\left[\widehat{f}^2\right] &=& \text{Var}\left(\widehat{f}\right) + \mathbb{E}\left[\widehat{f}\right]^2 \\\end{array}</script><p>其中，</p><script type="math/tex; mode=display">\Large \begin{array}{rcl} \text{Var}\left(y\right) &=& \mathbb{E}\left[\left(y - \mathbb{E}\left[y\right]\right)^2\right] \\&=& \mathbb{E}\left[\left(y - f\right)^2\right] \\&=& \mathbb{E}\left[\left(f + \epsilon - f\right)^2\right] \\&=& \mathbb{E}\left[\epsilon^2\right] = \sigma^2\end{array}</script><script type="math/tex; mode=display">\Large \mathbb{E}[y] = \mathbb{E}[f + \epsilon] = \mathbb{E}[f] + \mathbb{E}[\epsilon] = f</script><script type="math/tex; mode=display">\Large \begin{array}{rcl} \mathbb{E}\left[y\widehat{f}\right] &=& \mathbb{E}\left[\left(f + \epsilon\right)\widehat{f}\right] \\&=& \mathbb{E}\left[f\widehat{f}\right] + \mathbb{E}\left[\epsilon\widehat{f}\right] \\&=& f\mathbb{E}\left[\widehat{f}\right] + \mathbb{E}\left[\epsilon\right] \mathbb{E}\left[\widehat{f}\right]  = f\mathbb{E}\left[\widehat{f}\right]\end{array}</script><p>最终，</p><script type="math/tex; mode=display">\Large \begin{array}{rcl} \text{Err}\left(\textbf{x}\right) &=& \mathbb{E}\left[\left(y - \widehat{f}\left(\textbf{x}\right)\right)^2\right] \\&=& \sigma^2 + f^2 + \text{Var}\left(\widehat{f}\right) + \mathbb{E}\left[\widehat{f}\right]^2 - 2f\mathbb{E}\left[\widehat{f}\right] \\&=& \left(f - \mathbb{E}\left[\widehat{f}\right]\right)^2 + \text{Var}\left(\widehat{f}\right) + \sigma^2 \\&=& \text{Bias}\left(\widehat{f}\right)^2 + \text{Var}\left(\widehat{f}\right) + \sigma^2\end{array}</script><h3 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h3><p>$OR(X)=\frac{P(X)}{1-P(X)}$</p><script type="math/tex; mode=display">\large p_{+} = \frac{OR_{+}}{1 + OR_{+}} = \frac{\exp^{\textbf{w}^\text{T}\textbf{x}}}{1 + \exp^{\textbf{w}^\text{T}\textbf{x}}} = \frac{1}{1 + \exp^{-\textbf{w}^\text{T}\textbf{x}}} = \sigma(\textbf{w}^\text{T}\textbf{x})</script><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="超参优化：正则化参数C"><a href="#超参优化：正则化参数C" class="headerlink" title="超参优化：正则化参数C"></a>超参优化：正则化参数C</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression, LogisticRegressionCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">skf = StratifiedKFold(n_splits=<span class="number">5</span>, shuffle=<span class="keyword">True</span>, random_state=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line">c_values = np.logspace(<span class="number">-2</span>, <span class="number">3</span>, <span class="number">500</span>)</span><br><span class="line"></span><br><span class="line">logit_searcher = LogisticRegressionCV(Cs=c_values, cv=skf, verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">logit_searcher.fit(X_poly, y)</span><br></pre></td></tr></table></figure><h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><h3 id="1-BoW构造稀疏矩阵"><a href="#1-BoW构造稀疏矩阵" class="headerlink" title="1. BoW构造稀疏矩阵"></a>1. BoW构造稀疏矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">cv = CountVectorizer()</span><br><span class="line">cv.fit(text_train)</span><br><span class="line">X_train = cv.transform(text_train)</span><br><span class="line">X_test = cv.transform(text_test)</span><br></pre></td></tr></table></figure><h3 id="2-构造多项式特征"><a href="#2-构造多项式特征" class="headerlink" title="2. 构造多项式特征"></a>2. 构造多项式特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">7</span>)</span><br><span class="line">X_poly = poly.fit_transform(X)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>《机器学习》——周志华</p></li><li><p><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</p></li></ol><p>3.<a href="http://sklearn.apachecn.org/cn/0.19.0/" target="_blank" rel="noopener">Sklearn文档</a></p><ul><li>Course materials as a <a href="https://www.kaggle.com/kashnitsky/mlcourse" target="_blank" rel="noopener">Kaggle Dataset</a></li><li>A nice and concise overview of linear models is given in the book <a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener">“Deep Learning”</a> (I. Goodfellow, Y. Bengio, and A. Courville).</li><li>Linear models are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).</li><li>If you prefer a thorough overview of linear model from a statistician’s viewpoint, then look at “The elements of statistical learning” (T. Hastie, R. Tibshirani, and J. Friedman).</li><li>The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.</li><li><a href="http://scikit-learn.org/stable/documentation.html" target="_blank" rel="noopener">Scikit-learn</a> library. These guys work hard on writing really clear documentation.</li><li>Scipy 2017 <a href="https://github.com/amueller/scipy-2017-sklearn" target="_blank" rel="noopener">scikit-learn tutorial</a> by Alex Gramfort and Andreas Mueller.</li><li>One more <a href="https://github.com/diefimov/MTH594_MachineLearning" target="_blank" rel="noopener">ML course</a> with very good materials.</li><li><a href="https://github.com/rushter/MLAlgorithms" target="_blank" rel="noopener">Implementations</a> of many ML algorithms. Search for linear regression and logistic regression.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;算法理论&quot;&gt;&lt;a href=&quot;#算法理论&quot; class=&quot;headerlink&quot; title=&quot;算法理论&quot;&gt;&lt;/a&gt;算法理论&lt;/h2&gt;&lt;sc
      
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>机器在学习-决策树(Dicision Tree)</title>
    <link href="http://blog.a-stack.com/2018/11/23/%E6%9C%BA%E5%99%A8%E5%9C%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-Dicision-Tree/"/>
    <id>http://blog.a-stack.com/2018/11/23/机器在学习-决策树-Dicision-Tree/</id>
    <published>2018-11-23T08:50:11.000Z</published>
    <updated>2018-11-25T07:22:23.785Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/15.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="决策树的算法"><a href="#决策树的算法" class="headerlink" title="决策树的算法"></a>决策树的算法</h2><blockquote><p>ID3  C4.5  CART</p><p>ID3（Iterative Dichotomiser 3）由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛华能力。</p><p>C4.5 是 ID3 的后继者，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。</p><p>C5.0 是 Quinlan 根据专有许可证发布的最新版本。它使用更少的内存，并建立比 C4.5 更小的规则集，同时更准确。</p><p>CART（Classification and Regression Trees （分类和回归树））与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。</p><p>scikit-learn 使用 CART 算法的优化版本。</p></blockquote><p><strong>Decision Trees (DTs)</strong> 是一种用来 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree-classification" target="_blank" rel="noopener">classification</a> 和 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree-regression" target="_blank" rel="noopener">regression</a> 的无参监督学习方法。其目的是创建一种模型从数据特征中学习简单的决策规则来预测一个目标变量的值。 </p><p><img src="/2018/11/23/机器在学习-决策树-Dicision-Tree/credit_scoring_toy_tree_english.png" alt="credit_scoring_toy_tree_english"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(L)</span>:</span></span><br><span class="line">    create node t</span><br><span class="line">    <span class="keyword">if</span> the stopping criterion <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">        assign a predictive model to t</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Find the best binary split L = L_left + L_right</span><br><span class="line">        t.left = build(L_left)</span><br><span class="line">        t.right = build(L_right)</span><br><span class="line">    <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure><h3 id="决策树的递归返回"><a href="#决策树的递归返回" class="headerlink" title="决策树的递归返回"></a>决策树的递归返回</h3><ul><li>(1) 当前结点包含的样本全属于同一类别，无需划分; </li><li>(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分; </li><li>(3) 当前结点包含的样本集合为空，不能划分.</li></ul><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><ul><li><p>信息熵（香农）： $E(D) = -\sum_k p_klog_2p_k$</p></li><li><p>增益： $IG(D,a) = E(D) - \sum_v \frac{|D^v|}{|D|}E(D^v)$</p><p><img src="/2018/11/23/机器在学习-决策树-Dicision-Tree/topic3_credit_scoring_entropy.png" alt="scoring_entropy"></p><blockquote><ol><li>信息增益可以衡量使用某个属性划分的好坏；</li><li>信息增益准则对可能取值数目较多的属性有偏好，为减少这种偏好带来的不利影响，C4.5决策树算法使用增益率来选择最优划分属性。</li></ol></blockquote></li><li><p>增益率： $G_r(D,a)=\frac{IG(D,a)}{IV(a)}$</p><script type="math/tex; mode=display">IV(a) =\sum_v \frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}</script></li></ul><h3 id="Gini"><a href="#Gini" class="headerlink" title="Gini"></a>Gini</h3><script type="math/tex; mode=display">Gini(D)=1-\sum_k p^2_k</script><blockquote><p>CART决策树使用的算法,Gini数值约小，数据集D的纯度越高。</p></blockquote><p><img src="/2018/11/23/机器在学习-决策树-Dicision-Tree/Criteria of quality as a function of binary classification.png" alt="Criteria of quality as a function of binary classification"></p><h3 id="剪枝（pruning）"><a href="#剪枝（pruning）" class="headerlink" title="剪枝（pruning）"></a>剪枝（pruning）</h3><ul><li><strong>预剪枝：</strong> （自顶向下） 预剪枝是指在决策树生成过程中，对每个结点在划 分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;</li><li><strong>后剪枝：</strong>（自底向上）后剪枝则是先从训练集生成一棵完整的决策树， 然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.</li></ul><blockquote><p>预剪枝减少了计算量，减少了过拟合，但存在欠拟合的风险；</p><p>后剪枝是自底向上的对树中所有非叶节点进行逐一考察，训练开销比预剪枝要大很多。</p></blockquote><h3 id="一些处理技巧"><a href="#一些处理技巧" class="headerlink" title="一些处理技巧"></a>一些处理技巧</h3><h4 id="处理连续值"><a href="#处理连续值" class="headerlink" title="处理连续值"></a>处理连续值</h4><ul><li>连续属性离散化技术（二分法）；</li><li>对训练样本值排序，查找最大信息增益的分割点（对结果影响大的点），划分为多个连续区间；</li><li>如果训练样本某个属性不同样本值特别多，可以选择top-N增益最大的点作为划分分类的点；</li></ul><h4 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h4><ul><li>根据缺失值赋权；</li></ul><h4 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h4><ul><li>组合多个变量，可以形成非与坐标轴平行的分类边界；</li></ul><h4 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h4><script type="math/tex; mode=display">\Large D = \frac{1}{\ell} \sum\limits_{i =1}^{\ell} (y_i - \frac{1}{\ell} \sum\limits_{j=1}^{\ell} y_j)^2</script><p>where $\ell$ is the number of samples in a leaf, $y_i$ is the value of the target variable.</p><h2 id="决策树算法的优缺点"><a href="#决策树算法的优缺点" class="headerlink" title="决策树算法的优缺点"></a>决策树算法的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>可解释性强，决策树的计算原理和人做决策的过程十分类似，是解释性最强的机器学习模型；</li><li>可视化方便，计算过程可以在图中直观的反映出来；</li><li>训练和预测都十分迅速，计算代价小；</li><li>训练需要的数据少。其他机器学习模型通常需要数据规范化，比如构建虚拟变量和移除缺失值,不过请注意，这种模型不支持缺失值。</li><li>超参十分少，常用的只有层数、叶子节点中元素个数、最大特征数等少数几个超参；</li><li>对于连续变量、离散变量、分类问题和回归问题都可以处理。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>对噪声数据敏感，训练数据发生轻微变动，可能导致整个模型参数发生很大变化；</li><li>分割界面只能平行或垂直坐标轴方向，处理逻辑太过简单；</li><li>需要特别注意过拟合的发生；决策树模型容易产生一个过于复杂的模型,这样的模型对数据的泛化性能会很差。这就是所谓的过拟合.一些策略像剪枝、设置叶节点所需的最小样本数或设置数的最大深度是避免出现 该问题最为有效地方法。</li><li>稳定性差；因为数据中的微小变化可能会导致完全不同的树生成。这个问题可以通过决策树的集成来得到缓解 </li><li>最优决策树的选择是一个NP-完备问题。需要使用一些启发式算法寻找最优的信息增益，但未必能找到最优值；这个问题可以通过集成学习来训练多棵决策树来缓解,这多棵决策树一般通过对特征和样本有放回的随机采样来生成。 </li><li>缺失数据的处理比较麻烦；</li><li>数据集没有覆盖的区域，没有区分能力</li><li>有些概念很难被决策树学习到,因为决策树很难清楚的表述这些概念。例如XOR，奇偶或者复用器的问题。</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><blockquote><p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" target="_blank" rel="noopener"><code>sklearn.tree.DecisionTreeClassifier</code></a></p><ul><li><code>max_depth</code> – the maximum depth of the tree;</li><li><code>max_features</code> - the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be “expensive” to search for partitions for <em>all</em> features);</li><li><code>min_samples_leaf</code> – the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few members.</li></ul><p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" target="_blank" rel="noopener"><code>DecisionTreeRegressor</code></a> </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sklearn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">clf_tree = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=<span class="number">3</span>, </span><br><span class="line">                                  random_state=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training the tree</span></span><br><span class="line">clf_tree.fit(train_data, train_labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">tree_pred = tree.predict(X_holdout)</span><br><span class="line">accuracy_score(y_holdout, tree_pred) <span class="comment"># 0.94</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, cross_val_score</span><br><span class="line"></span><br><span class="line">tree_params = &#123;<span class="string">'max_depth'</span>: range(<span class="number">1</span>,<span class="number">11</span>),</span><br><span class="line">               <span class="string">'max_features'</span>: range(<span class="number">4</span>,<span class="number">19</span>)&#125;</span><br><span class="line"></span><br><span class="line">tree_grid = GridSearchCV(tree, tree_params,</span><br><span class="line">                         cv=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">tree_grid.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">accuracy_score(y_holdout, tree_grid.predict(X_holdout)) <span class="comment">#0.946</span></span><br></pre></td></tr></table></figure><h3 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h3><p>经过训练，我们可以使用 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz" target="_blank" rel="noopener"><code>export_graphviz</code></a> 导出器以 <a href="http://www.graphviz.org/" target="_blank" rel="noopener">Graphviz</a> 格式导出决策树. 如果你是用 <a href="http://conda.io/" target="_blank" rel="noopener">conda</a> 来管理包，那么安装 graphviz 二进制文件和 python 包可以用以下指令安装</p><blockquote><p>conda install python-graphviz</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pydotplus <span class="comment">#pip install pydotplus</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tree_graph_to_png</span><span class="params">(tree, feature_names, png_file_to_save)</span>:</span></span><br><span class="line">    tree_str = export_graphviz(tree, feature_names=feature_names, </span><br><span class="line">                                     filled=<span class="keyword">True</span>, out_file=<span class="keyword">None</span>)</span><br><span class="line">    graph = pydotplus.graph_from_dot_data(tree_str)  </span><br><span class="line">    graph.write_png(png_file_to_save)</span><br><span class="line">    </span><br><span class="line">  tree_graph_to_png(tree=clf_tree, feature_names=[<span class="string">'x1'</span>, <span class="string">'x2'</span>], </span><br><span class="line">                  png_file_to_save=<span class="string">'../../img/topic3_tree1.png'</span>)</span><br></pre></td></tr></table></figure><p><img src="/2018/11/23/机器在学习-决策树-Dicision-Tree/tree1.png" alt="tree1"></p><h3 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h3><blockquote><ul><li>对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。</li><li>考虑事先进行降维( <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/decomposition.html#pca" target="_blank" rel="noopener">PCA</a> , <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/decomposition.html#ica" target="_blank" rel="noopener">ICA</a> ，使您的树更好地找到具有分辨性的特征。</li><li>通过 <code>export</code> 功能可以可视化您的决策树。使用 <code>max_depth=3</code> 作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。</li><li>请记住，填充树的样本数量会增加树的每个附加级别。使用 <code>max_depth</code> 来控制输的大小防止过拟合。</li><li>通过使用 <code>min_samples_split</code> 和 <code>min_samples_leaf</code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。所以尝试 <code>min_samples_leaf=5</code> 作为初始值。如果样本的变化量很大，可以使用浮点数作为这两个参数中的百分比。两者之间的主要区别在于 <code>min_samples_leaf</code> 保证叶结点中最少的采样数，而 <code>min_samples_split</code> 可以创建任意小的叶子，尽管在文献中 <code>min_samples_split</code> 更常见。</li><li>在训练之前平衡您的数据集，以防止决策树偏向于主导类.可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code>sample_weight</code>) 的和归一化为相同的值。还要注意的是，基于权重的预修剪标准 (<code>min_weight_fraction_leaf</code>) 对于显性类别的偏倚偏小，而不是不了解样本权重的标准，如 <code>min_samples_leaf</code> 。</li></ul></blockquote><ul><li>如果样本被加权，则使用基于权重的预修剪标准 <code>min_weight_fraction_leaf</code> 来优化树结构将更容易，这确保叶节点包含样本权重的总和的至少一部分。</li><li>所有的决策树内部使用 <code>np.float32</code> 数组 ，如果训练数据不是这种格式，将会复制数据集。</li><li>如果输入的矩阵X为稀疏矩阵，建议您在调用fit之前将矩阵X转换为稀疏的<code>csc_matrix</code> ,在调用predict之前将 <code>csr_matrix</code> 稀疏。当特征在大多数样本中具有零值时，与密集矩阵相比，稀疏矩阵输入的训练时间可以快几个数量级。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>《机器学习》——周志华</p></li><li><p><a href="https://mlcourse.ai/" target="_blank" rel="noopener">mlcourse.ai</a> – Open Machine Learning Course</p></li></ol><p>3.<a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html#tree" target="_blank" rel="noopener">Sklearn 决策树</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/15.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>读书笔记：Machine Learning Yearning</title>
    <link href="http://blog.a-stack.com/2018/09/29/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%9AMachine-Learning-Yearning/"/>
    <id>http://blog.a-stack.com/2018/09/29/读书笔记：Machine-Learning-Yearning/</id>
    <published>2018-09-29T02:00:21.000Z</published>
    <updated>2018-09-29T09:41:43.630Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/09/29/读书笔记：Machine-Learning-Yearning/MachineLearningYearning.jpg" alt="Machine Learning Yearning"></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>《Machine Learning Yearning》是Andrew Ng写的一本在深度学习领域偏重工程实践的小册子，终于在九月底把坑填完了，有需要的可以在其<a href="http://www.mlyearning.org/" target="_blank" rel="noopener">官网</a>索取,或者直接通过该地址下载<a href="https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/514ed488-bcae-4924-8fd8-dcabeec2e440/Ng_MLY13.pdf" target="_blank" rel="noopener">目前的版本</a>。从吴恩达开始撰写前面几章时我便订阅了相关邮件，持续跟进文章内容，这本小册子可以配合deeplearning.ai的深度学习课程一起学习，效果更佳。本文简单的记录一下阅读过程中一些的新的收货。</p><h2 id="端到端学习的选择"><a href="#端到端学习的选择" class="headerlink" title="端到端学习的选择"></a>端到端学习的选择</h2><p>深度学习与传统机器学习算法的一个主要差异，在于它本质上是一个端到端的学习过程，不再需要或很少需要手工特征的介入。但端到端学习真的好么？适用于所有的业务场景么？</p><p>深度学习的端到端学习策略有效是建立在足够训练数据的基础上的，当数据量有限的情况下，手工特征的过程有助于加速网络收敛和限制网络的发散。而当存在足够的训练数据</p><h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p>由于模型的复杂性，导致在深度神经网络中，出现误差或错误可能来自于多个方面，甚至是多个方面的共同作用结果，所以制定合理的错误分析策略对于寻找问题根源和优化模型效果至关重要。</p><ul><li>错误分析的过程，本质上还是一个控制变量法的过程，保证其它相关变量的最优来聚焦所需要观测的变量信息；</li><li>错误分析针对那些人类自身可以做的很好的任务，比如图片分类、物体识别等，会有很好的效果，而对人类不擅长的，很难进一步的进行直接的错误分析；</li><li>当你把一个pipeline的各部分已经优化到极致的情况，如果整体模型效果依然不好时，你需要考虑是不是模型设计存在本质缺陷，如没有包含所有有效的相关因素，没有形成合理的推断流程；</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>​</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/2018/09/29/读书笔记：Machine-Learning-Yearning/MachineLearningYearning.jpg&quot; alt=&quot;Machine Learning Yearning&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;概述&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="读书笔记" scheme="http://blog.a-stack.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="工程实践" scheme="http://blog.a-stack.com/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"/>
    
      <category term="教学" scheme="http://blog.a-stack.com/tags/%E6%95%99%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>HMM与Veterbi算法</title>
    <link href="http://blog.a-stack.com/2018/08/28/HMM%E4%B8%8EVeterbi%E7%AE%97%E6%B3%95/"/>
    <id>http://blog.a-stack.com/2018/08/28/HMM与Veterbi算法/</id>
    <published>2018-08-28T08:53:06.000Z</published>
    <updated>2018-08-28T09:26:36.540Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/13.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="从概率图模型谈起"><a href="#从概率图模型谈起" class="headerlink" title="从概率图模型谈起"></a>从概率图模型谈起</h2><p>概率模型(probabilistic model)提供了一种描述框架，将学习任务归结于计算变量的概率分布。在概 率模型中，利用已知变量推测未知变量的分布称为”推断” (inference)，其 核心是如何基于可观测变量推测出未知变量的条件分布具体来说，假定所关心的变量集合为Y,可观测变量集合为 O，其他变量的集合为 R，”生成式” (generative)模型考虑联合分布 P(Y，R，O) ， “判别式” (discriminative)模 型考虑条件分布 P(Y，R I O)。给定一组观测变量值，推断就是要由 P(Y ,R,O) 或 P(Y, R |O) 得到条件概率分布 P(Y I O)。</p><p>概率图是一类利用图来表达变量相关关系的概率模型。若变量之间存在显示的因果关系，则常用贝叶斯网络（有向图模型）；若变量之间存在相关性，但难以获得显示的因果关系，则常使用马尔可夫网络（无向图模型）。</p><h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><p>隐马尔科夫模型（Hidden Markov Model, HMM）是结构最简单的动态贝叶斯网络，主要用于时序数据建模，应用于语音识别、自然语言处理等领域。</p><h3 id="马尔可夫假设"><a href="#马尔可夫假设" class="headerlink" title="马尔可夫假设"></a>马尔可夫假设</h3><ol><li>系统下一时刻的状态仅有当前状态决定，不依赖于以往的任何状态，即<script type="math/tex; mode=display">P(x_1,y_1,...,x_n,y_n)=P(y_1)P(x_1|y_1)</script></li></ol><h3 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h3><ol><li>概率计算问题： 给定模型，如何有效计算产生关测序列的概率$P(x|\lambda)$，可以利用求解结果求未来的观测数据值；</li><li>预测/解码问题： 给定模型和观测序列$x$，如何找到于此观测序列最匹配的状态序列$y$, $P(y|0)$；</li><li>学习问题： 给定观测序列$x$，如何调整模型参数，使得该序列出现的概率$P(x|\lambda)$ 最大，即如何训练模型使其能更好的描述观测数据；</li></ol><h2 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h2><p><img src="/2018/08/28/HMM与Veterbi算法/Viterbi_animated_demo.gif" alt="Viterbi_animated_demo"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>《统计学习方法》—李航</li><li>《机器学习》—周志华</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/13.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://blog.a-stack.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>读书笔记:数学之美</title>
    <link href="http://blog.a-stack.com/2018/08/26/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    <id>http://blog.a-stack.com/2018/08/26/读书笔记-数学之美/</id>
    <published>2018-08-26T11:25:58.000Z</published>
    <updated>2018-08-27T13:50:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2018/08/26/读书笔记-数学之美/数学之美.jpg" alt="数学之美"></p><ul><li><p>作者： 吴军</p></li><li><p>2014年11月第二版</p></li><li><p>北京：人民邮电出版社</p><hr><a id="more"></a></li></ul><p>时隔多年，在人工智能概念进入鼎盛时期的今天，花一个周末，细细品读了吴军先生的《数学之美》，又有了不少新的感悟，新的收货，稍作整理，便于复查。</p><p>此次重读恰逢归纳自然语言处理相关技术和脉络，而吴军本身就是这个学科的专家，所以其著作中对相关内容的描述条例清晰，结构完备，值得思考。</p><h2 id="经典语句摘录"><a href="#经典语句摘录" class="headerlink" title="经典语句摘录"></a>经典语句摘录</h2><blockquote><p><strong>WWW发明人蒂姆.伯纳斯。李：</strong></p><p>简单性和模块化是软件工程的基石；分布式和容错性是互联网的生命。</p></blockquote><h2 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h2><h3 id="从历史看：语言、数字、文字都是信息的载体，或是对信息的编码"><a href="#从历史看：语言、数字、文字都是信息的载体，或是对信息的编码" class="headerlink" title="从历史看：语言、数字、文字都是信息的载体，或是对信息的编码"></a>从历史看：语言、数字、文字都是信息的载体，或是对信息的编码</h3><p>从人来进化角度来看，先有的语言（声音元素最先），再有的数字，然后是文字。目的都在于方便人与人之间通信（传递信息）。</p><p>早期人来了解和需要传递的信息十分有限，因此不需要高级的语言或者数字、文字来进行表达。但随着人类的进化和文明的发展，需要表达的信息越来越多，不再是几种不同的声音就能够完全覆盖，语言就此产生。知识也通过口述的方式，逐代传递。</p><p>我们祖先迅速学习新鲜事物，语言越来越丰富，内容越来越抽象，语言描述的共同要素，比如某些物体、数量、行为动作便被抽象出来形成了<strong>词汇</strong>。当语言和词汇多到一定程度，人类靠大脑已经记不住所有词汇，于是高效记录信息的需求就产生了，这便是文字的起源。</p><blockquote><p>由此可见，产品果真是由需求决定的！</p></blockquote><p>最初文字的词汇依靠模拟/模仿物体或行为形成的象形文字来展现，但随着文明进步，信息量逐步增加，象形文字数目不再随着文明发展而增加了，因为没人能够记住这么多的文字。于是，概念的第一次概括和归类开始了。比如“日”除了表示太阳，还可以表示一天。</p><blockquote><p>在那个时间，人类就已经领会了聚类算法的精髓 …只不过由于“算力不足”，这个聚类的过程一般持续几千年才能收敛到合适的位置。</p></blockquote><p>聚类虽然减少了表达信息的文字数量，却带来了一个严重的问题——歧义性。多义字在一段文字中的表达需要依靠上下文推理来完成去除歧义。</p><blockquote><p>虽然经过训练的人类可以很好的依靠理解消除歧义，但苦了计算机模型，需要打破状态无记忆的机理，沿着上下文的思路去解决问题。可见这是语言产生伊始就埋的一个坑。</p></blockquote><h4 id="不同文明在信息的维度上是等价的"><a href="#不同文明在信息的维度上是等价的" class="headerlink" title="不同文明在信息的维度上是等价的"></a>不同文明在信息的维度上是等价的</h4><p>虽然受地域和文明发展进度差异影响，不同文明发明了不同的语言和文字，但本质上都是一种对信息的编码方式，所以可以通过一定的手段进行翻译的。</p><blockquote><p>文字是信息的载体，而非信息本身。</p></blockquote><h4 id="罗塞塔"><a href="#罗塞塔" class="headerlink" title="罗塞塔"></a>罗塞塔</h4><p>古埃及罗塞塔的石碑记录了最早的象形文字多语平行语料库，上面有三种语言：埃及象形文字、埃及拼音文字和古希腊文。因此Google的机器翻译项目也被命名为罗塞塔。</p><p><img src="/2018/08/26/读书笔记-数学之美/罗塞塔石碑.jpg" alt="罗塞塔石碑"></p><blockquote><p><strong>百度百科解读：</strong></p><p>罗塞塔<a href="https://baike.baidu.com/item/%E7%9F%B3%E7%A2%91/36268" target="_blank" rel="noopener">石碑</a>（Rosetta Stone，也译作罗塞达碑），高1.14米，宽0.73米，制作于公元前196年，刻有<a href="https://baike.baidu.com/item/%E5%8F%A4%E5%9F%83%E5%8F%8A/226771" target="_blank" rel="noopener">古埃及</a>国王<a href="https://baike.baidu.com/item/%E6%89%98%E5%8B%92%E5%AF%86%E4%BA%94%E4%B8%96/3481978" target="_blank" rel="noopener">托勒密五世</a>登基的诏书。石碑上用<a href="https://baike.baidu.com/item/%E5%B8%8C%E8%85%8A%E6%96%87%E5%AD%97/12722009" target="_blank" rel="noopener">希腊文字</a>、<a href="https://baike.baidu.com/item/%E5%8F%A4%E5%9F%83%E5%8F%8A%E6%96%87%E5%AD%97/2494489" target="_blank" rel="noopener">古埃及文字</a>和当时的通俗体文字刻了同样的内容，这使得近代的<a href="https://baike.baidu.com/item/%E8%80%83%E5%8F%A4%E5%AD%A6%E5%AE%B6/1217571" target="_blank" rel="noopener">考古学家</a>得以有机会对照各语言<a href="https://baike.baidu.com/item/%E7%89%88%E6%9C%AC/505574" target="_blank" rel="noopener">版本</a>的内容后，解读出已经失传千余年的埃及象形文之意义与结构，而成为今日研究古埃及历史的重要里程碑。 </p></blockquote><h4 id="数字产生及编码"><a href="#数字产生及编码" class="headerlink" title="数字产生及编码"></a>数字产生及编码</h4><p>数字产生与文字类似，当人类发展到一定程度，需要计数系统来进行计算的时候便产生了信息的载体数字。当然，早期数字没有书写的形式，而是掰指头，这就是今天我们使用十进制的原因。</p><blockquote><p>玛雅文明使用的是二十进制（手脚都算上…）</p></blockquote><p>罗马人用字符I代表1，V代表5，X代表10，L代表50，C代表100，D代表500，M代表1000，解码规则是加减法（中国解码规则是乘法），小数字出现在大数字左边为减，右边为加。比如IIXX（10+10-1-1=18），如果要用罗马数字表示100万 …</p><p>阿拉伯数字最初由古印度人发明，由阿拉伯人传入欧洲，被中间商冠名了。</p><h4 id="从象形文字到楔形文字"><a href="#从象形文字到楔形文字" class="headerlink" title="从象形文字到楔形文字"></a>从象形文字到楔形文字</h4><p>楔形文字又称拼音文字，是一个很大的进化，人类在描述物体的方式上，从外表化到了抽象概念的层级，是一种高效的信息编码方式。比如，常用字短，生僻字长就是符合最短编码原理的一种实现。</p><h4 id="语言与语法"><a href="#语言与语法" class="headerlink" title="语言与语法"></a>语言与语法</h4><p>词可以被认为是有限而且封闭的集合，而语言则是无限开放的集合；从数学上讲，前者可以由完备的编码规则，后者则不具备。</p><p>任何一种语言都是一种信息的编码方式；</p><p>语法可以认为是解码的规则，但很多时候文章的撰写不完全符合语法，比如鲁迅先生的文字。</p><p>语言vs语法： 语言简直从真实的语句文本（称为语料）出发，而后者坚持从规则出发；经过三四十年的争论，最后实践证明，自然语言的成就最终宣布了前者的胜利。</p><h3 id="自然语言发展的两个阶段：从规则到统计"><a href="#自然语言发展的两个阶段：从规则到统计" class="headerlink" title="自然语言发展的两个阶段：从规则到统计"></a>自然语言发展的两个阶段：从规则到统计</h3><p><img src="/2018/08/26/读书笔记-数学之美/信息传输.jpeg" alt="信息传输"></p><p><img src="/2018/08/26/读书笔记-数学之美/4组织语言.png" alt="4组织语言"></p><blockquote><p>我们要把一个意思表达出来，通过某种语言的一句话表述出来就是用这种语言的编码方式对头脑中的信息做了一次编码，编码结果就是一串文字；而如果对方懂得这么语言，他就可以利用这么语言的语法解码处说话人想表达的意思。</p></blockquote><h4 id="NLP的历史"><a href="#NLP的历史" class="headerlink" title="NLP的历史"></a>NLP的历史</h4><ul><li><p>1950年，图灵提出的图灵测试可以作为NLP的开始；</p></li><li><p>20世纪50年代到70年代，计算机处理NLP问题的认识都局限在人类学习语言的方式上，也就是用电脑模拟人脑，这20多年的成果近乎为零，其中由大量的语言学家参与其中；</p><ul><li>让计算机理解自然语言，必须让计算机拥有类似人类的智能；</li><li>更多的采用仿生学的方法解决问题；</li><li>为什么会有这种错误的认识？ 因为人类就是这么做的，这就是直觉的作用；</li><li>这样的方法被称为“鸟飞派”（怀特兄弟发明飞机靠的是空气动力学而不是仿生学）；</li><li>手段是：句法分析和语义分析<ul><li>分析句法结构，建立文法规则</li><li>20世纪80年代以前，NLP的文法规则都是由人工总结完成的</li><li>有两大困难：<ul><li>为了覆盖哪怕20%的真实语句，文法规则需要至少几万条；语言学家来不及写；更可怕的是，写到后面出现了各种矛盾，为了解决矛盾需要为规则建立限定规则；如果要覆盖50%的语句，文法规则会多到每增加一个语句，需要添加一条规则；（正如英语语法学的再好，不练习，不看文献，照样看不懂英文电影）</li><li>计算机解析规则困难重重，计算机解码方式是上下文无关的，而NLP都是复杂的上下文有关的文法；前者算法复杂度为语句长度的二次方，后者是语句长度的六次方；</li></ul></li></ul></li></ul><p><img src="/2018/08/26/读书笔记-数学之美/句法分析树.png" alt="句法分析树"></p></li><li><p>20世纪70年代，开始使用统计方法和数学模型解决NLP问题</p><ul><li>语义方面的多义性也遇到了困难；</li><li>贾里尼克和他领导的IBM华生实验室（T.J. Watson）采用基于统计的方法解决语音识别取得了突破；感染了许多研究人员转投统计领域；比如李开复和洪小文；</li><li>但基于规则和基于统计的争执还是持续了15年左右，直到20世纪90年代才分出胜负<ul><li>基于统计方法的核心模型是通信系统+隐马尔科夫模型，最早成功的领域是语音识别、词性分析（一维符号序列），机器翻译和句法分析输入是一维，输出是多维或乱序信息，简单应用不管用；同时数据量不足，发展不是很快；</li><li>传统方法认为基于统计的方法只能处理浅层的自然语言问题，无法进入深层次研究；</li><li>随着计算力的提高和数据量的增加，一切变的不一样了，2005年随着Google机器翻译战胜SysTran,最后一个领域也被统计方法攻占；</li><li>另一个重要原因，用基于统计的方法替代传统方法，需要等原有的一批语言学家退休；</li></ul></li></ul></li></ul><blockquote><p>类比到我们学习英语的过程,当时教授课程过程还是以英语语法、词性和构词法为主，主谓宾、定状补；而我基本上都这些语法缺乏概念，更多的是靠语感在做题目，这是建立在读了大量英语文献基础上，也是一种基于统计的预测算法。</p></blockquote><h4 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h4><p>统计语言模型的目的在于为计算机建立一种具备上下文相关的数学模型。</p><p>自然语言处理过程可以简单的认为是判断一句话是不是自然语句，以前是通过语法规则来判断的，统计语言模型是通过概率来衡量的。</p><h4 id="马尔可夫"><a href="#马尔可夫" class="headerlink" title="马尔可夫"></a>马尔可夫</h4><p>马尔可夫假设认为一个词出现的概率只与它前面一个词有关，形成二元模型；如果跟前面两个词有关就是三元模型(二阶马尔可夫模型)。</p><p>这样一个句子出现的概率可以表示为：</p><script type="math/tex; mode=display">P(S) = P(w_1)*P(w_2|w_1)*P(w_3|w_2)*...*P(w_n|w_{n-1})</script><blockquote><p>关于N元模型：</p><p>N越大效果越好，但需要的计算量越大，N元模型的空间复杂度几乎是N的指数函数$O(|V|^N)$,其中$|V|$是语言词典的词汇量，一般几万到几十万个。</p><p>当N从3到4时，其实模型效果的提升已经不是十分明显了。</p><p>Google的罗塞塔翻译使用了四元模型，存储与至少500台Google服务器中，当然2017年也被神经翻译机替代了。</p></blockquote><p>马尔可夫的局限性： 由于文本的上下文信息可能跳跃性存在，所以当N不是十分大的情况下，无法覆盖所有的语言现象。需要具备一定记忆性的单元来完成长期记忆的存储。</p><blockquote><p>关于语料库的覆盖问题：</p><p>汉语词汇量大概20万，训练一个三元模型产生$200000^3=8X10^{15}$个参数；</p><p>加深互联网中抛去垃圾数据，有100亿个有意义的中文网页，每个网页平均1000个词，即使爬取所有的词只有$10^{13}$个语料词汇。大部分条件概率为零。</p><ul><li>卡茨退避法，拿出一部分概率给语料库为出现的词，同时调低低频词的概率；</li><li>线性插值法</li></ul></blockquote><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>词是表达语义的最小单位；对于拼音语言来说，由于词之间有明确的分界符，统计和使用语言模型非常直接。</p><ul><li>最简单的分词方法是查词典，遇到复合词进行最长词匹配；<ul><li>对于二义性的分割没法解决；</li><li>“发展中国家”==发展 中国 家</li><li>“上海大学城” == 上海大学 城 书店</li><li>“北京大学生” == 北京大学 生</li></ul></li><li>统计方法：保证分完词之后这个句子出现的概率最大<ul><li>求解过程可以建模成动态规划问题</li></ul></li><li>但是，并不是所有的语句都有分词结果的，比如：<ul><li>此地安能居住，其人好不悲伤</li><li>下雨天留客天天留我不留</li></ul></li><li>中文分词技术后来还用于了英文手写字的分词；</li><li>分词问题已经属于解决了的问题，目前没有太多值得深入研究的技术领域了，除了偶尔增加一些新词</li></ul><h3 id="隐马尔科夫模型（HMM）"><a href="#隐马尔科夫模型（HMM）" class="headerlink" title="隐马尔科夫模型（HMM）"></a>隐马尔科夫模型（HMM）</h3><p>隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是<a href="http://www.cnblogs.com/pinard/p/6509630.html" target="_blank" rel="noopener">RNN</a>，<a href="http://www.cnblogs.com/pinard/p/6519110.html" target="_blank" rel="noopener">LSTM</a>等神经网络序列模型的火热，HMM的地位有所下降。</p><p>使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。</p><p>马尔可夫的求解过程就是从观测序列推断出隐藏序列的过程。</p><h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><h3 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h3><h2 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h2><h2 id="搜索引擎"><a href="#搜索引擎" class="headerlink" title="搜索引擎"></a>搜索引擎</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.cnblogs.com/pinard/p/6945257.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6945257.html</a></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2018/08/26/读书笔记-数学之美/数学之美.jpg&quot; alt=&quot;数学之美&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;作者： 吴军&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2014年11月第二版&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;北京：人民邮电出版社&lt;/p&gt;
&lt;hr&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://blog.a-stack.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="书籍" scheme="http://blog.a-stack.com/tags/%E4%B9%A6%E7%B1%8D/"/>
    
  </entry>
  
  <entry>
    <title>FastText-A better choice for word embedding?</title>
    <link href="http://blog.a-stack.com/2018/08/21/FastText-A-better-choice-for-word-embedding/"/>
    <id>http://blog.a-stack.com/2018/08/21/FastText-A-better-choice-for-word-embedding/</id>
    <published>2018-08-21T07:45:46.000Z</published>
    <updated>2018-08-28T14:08:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/10.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="FastText-vs-Word2vec"><a href="#FastText-vs-Word2vec" class="headerlink" title="FastText vs Word2vec"></a>FastText vs Word2vec</h2><p>fastText的架构和word2vec中的CBOW的架构类似，因为它们的作者都是Facebook的科学家Tomas Mikolov，而且确实fastText也算是words2vec所衍生出来的。 </p><p>According to a detailed comparison of Word2Vec and FastText in <a href="https://render.githubusercontent.com/view/Word2Vec_FastText_Comparison.ipynb" target="_blank" rel="noopener">this notebook</a>, fastText does significantly better on syntactic tasks as compared to the original Word2Vec, especially when the size of the training corpus is small. Word2Vec slightly outperforms FastText on semantic tasks though. The differences grow smaller as the size of training corpus increases. Training time for fastText is significantly higher than the Gensim version of Word2Vec (<code>15min 42s</code> vs <code>6min 42s</code> on text8, 17 mil tokens, 5 epochs, and a vector size of 100). </p><p><img src="/2018/08/21/FastText-A-better-choice-for-word-embedding/fasttextVSword2vec.png" alt="fasttextVSword2vec"></p><blockquote><p><strong>重要：</strong></p><p>fastText can be used to obtain vectors for <strong>out-of-vocabulary (OOV)</strong> words, by summing up vectors for its component char-ngrams, provided at least one of the char-ngrams was present in the training data. </p></blockquote><p>FastText is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is <code>app</code>, <code>ppl</code>, and <code>ple</code>(ignoring the starting and ending of boundaries of words). The word embedding vector for apple will be the sum of all these n-grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words. </p><h2 id="Training-Models"><a href="#Training-Models" class="headerlink" title="Training Models"></a>Training Models</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">importimport  gensimgensim</span><br><span class="line"> importimport  osos</span><br><span class="line"> fromfrom  gensim.models.word2vecgensim.  <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="keyword">from</span> gensim.models.fasttext <span class="keyword">import</span> FastText <span class="keyword">as</span> FT_gensim</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set file names for train and test data</span></span><br><span class="line">data_dir = <span class="string">'&#123;&#125;'</span>.format(os.sep).join([gensim.__path__[<span class="number">0</span>], <span class="string">'test'</span>, <span class="string">'test_data'</span>]) + os.sep</span><br><span class="line">lee_train_file = data_dir + <span class="string">'lee_background.cor'</span></span><br><span class="line">lee_data = LineSentence(lee_train_file)</span><br><span class="line"></span><br><span class="line">model_gensim = FT_gensim(size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build the vocabulary</span></span><br><span class="line">model_gensim.build_vocab(lee_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train the model</span></span><br><span class="line">model_gensim.train(lee_data, total_examples=model_gensim.corpus_count, epochs=model_gensim.iter)</span><br><span class="line"></span><br><span class="line">print(model_gensim)</span><br><span class="line">----</span><br><span class="line">FastText(vocab=<span class="number">1763</span>, size=<span class="number">100</span>, alpha=<span class="number">0.025</span>)</span><br></pre></td></tr></table></figure><p>训练中使用的相关参数：</p><ul><li>model: Training architecture. Allowed values: <code>cbow</code>, <code>skipgram</code> (Default <code>cbow</code>) </li><li>size: Size of embeddings to be learnt (Default 100) </li><li>alpha: Initial learning rate (Default 0.025) </li><li>window: Context window size (Default 5) </li><li>min_count: Ignore words with number of occurrences below this (Default 5) </li><li>loss: Training objective. Allowed values: <code>ns</code>, <code>hs</code>, <code>softmax</code> (Default <code>ns</code>) </li><li>sample: Threshold for downsampling higher-frequency words (Default 0.001) </li><li>negative: Number of negative words to sample, for <code>ns</code> (Default 5) </li><li>iter: Number of epochs (Default 5) </li><li>sorted_vocab: Sort vocab by descending frequency (Default 1) </li><li>threads: Number of threads to use (Default 12) </li><li>min_n: min length of char ngrams (Default 3) </li><li>max_n: max length of char ngrams (Default 6) </li><li>bucket: number of buckets used for hashing ngrams (Default 2000000) </li></ul><h3 id="模型的存储与加载"><a href="#模型的存储与加载" class="headerlink" title="模型的存储与加载"></a>模型的存储与加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># saving a model trained via Gensim's fastText implementation</span></span><br><span class="line">model_gensim.save(<span class="string">'saved_model_gensim'</span>)</span><br><span class="line">loaded_model = FT_gensim.load(<span class="string">'saved_model_gensim'</span>)</span><br><span class="line">print(loaded_model)</span><br></pre></td></tr></table></figure><h2 id="Using-FastText"><a href="#Using-FastText" class="headerlink" title="Using FastText"></a>Using FastText</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> FastText</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = FastText.load_fasttext_format(<span class="string">"./data/fasttext/cc.zh.300.bin"</span>)</span><br><span class="line">len(model.wv.vocab)</span><br><span class="line">w1 = <span class="string">"人工智能"</span></span><br><span class="line">w1 <span class="keyword">in</span> model.wv.vocab</span><br><span class="line">model.wv.most_similar(w1)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb" target="_blank" rel="noopener">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb</a></li><li><a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb" target="_blank" rel="noopener">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb</a></li><li><a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank" rel="noopener">models.fasttext – FastText model</a></li><li><a href="https://towardsdatascience.com/fasttext-batteries-included-fa23f46d52e4" target="_blank" rel="noopener">https://towardsdatascience.com/fasttext-batteries-included-fa23f46d52e4</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/10.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://blog.a-stack.com/categories/NLP/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="词嵌入" scheme="http://blog.a-stack.com/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/"/>
    
      <category term="FastText" scheme="http://blog.a-stack.com/tags/FastText/"/>
    
  </entry>
  
  <entry>
    <title>弄懂word2vec之原理解析</title>
    <link href="http://blog.a-stack.com/2018/08/21/%E5%BC%84%E6%87%82word2vec%E4%B9%8B%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    <id>http://blog.a-stack.com/2018/08/21/弄懂word2vec之原理解析/</id>
    <published>2018-08-21T05:36:53.000Z</published>
    <updated>2018-08-24T15:18:05.202Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/07.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="关键点提要"><a href="#关键点提要" class="headerlink" title="关键点提要"></a>关键点提要</h2><ul><li>word2vec不是深度学习模型（只有输入层、输出层）</li><li>效果并非最好，但易用性最强（其实FastText开源之后易用性直逼Word2vec）</li><li>受训练语料库影响大，尤其中文语料库本身缺乏</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Word2vec核心算法包括CBOW和skip-gram，再加上两种目标优化方式层次Softmax和负采样的组合，可以形成总共四种实现方式。需要特别理解的一点，其实词向量是模型的中间产物（词向量矩阵在模型中为隐藏层的权重值）而不是优化目标产物。模型的训练数据为（sourceword，targetword）的词语对。本文简单整理近段时间通过翻看代码、原理解析总结的一些对word2vec的认知，希望能够加深对整合词嵌入模型的理解。</p><p><img src="/2018/08/21/弄懂word2vec之原理解析/word2vec_architectures.png" alt="word2vec_architectures"></p><p>其中</p><ul><li>CBOW的实现原理为由中心词来预测前后的相关词；</li><li>Skip-Gram正好相反，为由中心词的前后相关词来倒推中心词为某词的可能性；</li><li>无论哪种方法，都不需要事先对数据进行标记，换句话说这可以看作一种无监督的学习方法。</li></ul><blockquote><p>有相关研究也对两种方法的实现效果进行了比较，CBOW在小数据集上表现要优于Skip-Gram，在大数据集合上两者相当。</p></blockquote><p><img src="/2018/08/21/弄懂word2vec之原理解析/模型原理示意.png" alt="模型原理示意"></p><h2 id="训练数据的定义"><a href="#训练数据的定义" class="headerlink" title="训练数据的定义"></a>训练数据的定义</h2><blockquote><p>详见代码</p></blockquote><p>在生成训练数据之前先要明确窗口的概念，窗口的概念与n-gram中强调的前后词之前的语义关联有关，用于描述某个词语与其后多个词语有直接或间接的关系。在word2vec中使用的是中心词前后各n个词组成的窗口大小。原则上，窗口设置越大预测效果越好，但计算量等比例增大，一般选择5-8作为恰当的窗口值。</p><p><strong>Embedding Lookup的解释</strong>：词向量存储为一个以词典词个数为行，词嵌入维度为列的权重矩阵，而输入单词可以被编码为数字值，利用数值在词向量矩阵中获得词的向量表达的过程成为Embedding Lookup.</p><p><img src="/2018/08/21/弄懂word2vec之原理解析/tokenize_lookup.png" alt="tokenize_lookup"></p><p>Words that show up often such as “the”, “of”, and “for” don’t provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we’ll discard it with probability given by </p><script type="math/tex; mode=display"> P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}</script><p>where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.</p><h3 id="训练数据的切分"><a href="#训练数据的切分" class="headerlink" title="训练数据的切分"></a>训练数据的切分</h3><p><img src="/2018/08/21/弄懂word2vec之原理解析/training_data.png" alt="training_data"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">corpus = <span class="string">''' I read magazine weekly . </span></span><br><span class="line"><span class="string">I read newspaper daily . I like to read books daily .'''</span></span><br><span class="line"></span><br><span class="line">corpus_tokens = []</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus.split(<span class="string">'.'</span>):</span><br><span class="line">    corpus_tokens.append(text.lower().split())</span><br><span class="line"></span><br><span class="line"><span class="comment">########## Create vocabulary </span></span><br><span class="line">vocab = &#123;&#125;</span><br><span class="line">counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> sentence_tokens <span class="keyword">in</span> corpus_tokens:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence_tokens:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            vocab[word] = counter</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########## Create training pairs .</span></span><br><span class="line">window = <span class="number">5</span></span><br><span class="line">target_words = []</span><br><span class="line">target_words_index = []</span><br><span class="line">c = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> corpus_tokens:</span><br><span class="line">    <span class="keyword">for</span> index,center_word <span class="keyword">in</span> enumerate(i):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            right = i[index+<span class="number">1</span>: index+<span class="number">1</span>+window]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            right == []</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            left = i[index<span class="number">-1</span>: (index<span class="number">-1</span>)+window]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            left == []</span><br><span class="line">        total_words = list(set(right+left))</span><br><span class="line">        <span class="keyword">if</span> total_words:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> total_words:</span><br><span class="line">                <span class="keyword">if</span> center_word <span class="keyword">in</span> vocab <span class="keyword">and</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">                    target_words.append((center_word, word))</span><br><span class="line">                    target_words_index.append((vocab[center_word], vocab[word]))</span><br><span class="line">    c += <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="网络的构建"><a href="#网络的构建" class="headerlink" title="网络的构建"></a>网络的构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">embedding_dimension = <span class="number">100</span></span><br><span class="line">n_classes           = len(vocab) </span><br><span class="line">X = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>, (len(vocab), embedding_dimension)) <span class="comment"># input features for X </span></span><br><span class="line">W = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>, (len(vocab), embedding_dimension))   </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> each_pair <span class="keyword">in</span> target_words_index:</span><br><span class="line">    </span><br><span class="line">    inp_pair_index = each_pair[<span class="number">0</span>]</span><br><span class="line">    op_pair_index  = each_pair[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    x_input = random_embeddings[inp_pair_index] <span class="comment"># 1 x embedding_dimension </span></span><br><span class="line">    </span><br><span class="line">    probs = softmax(np.dot(W, x_input))        <span class="comment"># 1 x n_classes   </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">####### loss(probs, op_pair_index)</span></span><br><span class="line">    <span class="comment">####### backpropagate , update W</span></span><br></pre></td></tr></table></figure><blockquote><p>虽然示意代码中使用了softmax，但从计算量上来看这并非一种好的方法； Word2vec中使用了两种替代方案，可以大大节约计算成本：分别是层次softmax和副采样算法； </p></blockquote><h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><p><img src="/2018/08/21/弄懂word2vec之原理解析/skip_gram_net_arch.png" alt="skip_gram_net_arch"></p><h2 id="负采样损失函数定义"><a href="#负采样损失函数定义" class="headerlink" title="负采样损失函数定义"></a>负采样损失函数定义</h2><p><img src="/2018/08/21/弄懂word2vec之原理解析/sampled-softmax.png" alt="sampled-softmax"></p><p>其中前一部分为真实结果的损失惩罚，后面的K部分为噪声词组的K次采样。</p><p>TensorFlow实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Number of negative labels to sample</span></span><br><span class="line">n_sampled = <span class="number">100</span></span><br><span class="line"><span class="keyword">with</span> train_graph.as_default():</span><br><span class="line">    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) <span class="comment"># create softmax weight matrix here</span></span><br><span class="line">    softmax_b = tf.Variable(tf.zeros(n_vocab), name=<span class="string">"softmax_bias"</span>) <span class="comment"># create softmax biases here</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate the loss using negative sampling</span></span><br><span class="line">    loss = tf.nn.sampled_softmax_loss(</span><br><span class="line">        weights=softmax_w,</span><br><span class="line">        biases=softmax_b,</span><br><span class="line">        labels=labels,</span><br><span class="line">        inputs=embed,</span><br><span class="line">        num_sampled=n_sampled,</span><br><span class="line">        num_classes=n_vocab)</span><br><span class="line">    </span><br><span class="line">    cost = tf.reduce_mean(loss)</span><br><span class="line">    optimizer = tf.train.AdamOptimizer().minimize(cost)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/29364112" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29364112</a></li><li><a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/representation/word2vec</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/07.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://blog.a-stack.com/categories/NLP/"/>
    
    
      <category term="词嵌入" scheme="http://blog.a-stack.com/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/"/>
    
      <category term="word2vec" scheme="http://blog.a-stack.com/tags/word2vec/"/>
    
      <category term="原理解析" scheme="http://blog.a-stack.com/tags/%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>弄懂word2vec之实践</title>
    <link href="http://blog.a-stack.com/2018/08/21/%E5%BC%84%E6%87%82word2vec%E4%B9%8B%E5%AE%9E%E8%B7%B5/"/>
    <id>http://blog.a-stack.com/2018/08/21/弄懂word2vec之实践/</id>
    <published>2018-08-21T05:34:57.000Z</published>
    <updated>2018-08-24T06:26:01.190Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/08.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="训练词向量模型"><a href="#训练词向量模型" class="headerlink" title="训练词向量模型"></a>训练词向量模型</h2><h3 id="工具准备"><a href="#工具准备" class="headerlink" title="工具准备"></a>工具准备</h3><ul><li><p>opencc 1.04</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、下载包地址（我下载的是opencc-1.0.4.tar.gz）：</span></span><br><span class="line"><span class="comment"># https://bintray.com/package/files/byvoid/opencc/OpenCC</span></span><br><span class="line"><span class="comment"># https://bintray.com/byvoid/opencc/download_file?file_path=opencc-1.0.4.tar.gz</span></span><br><span class="line"><span class="comment">#2、进入tar.gz目录，命令行解压：</span></span><br><span class="line">tar -xzvf opencc-1.0.4.tar.gz</span><br><span class="line"><span class="comment"># 3、编译（需要工具cmake、gcc（4.6）gcc -v查看gcc版本、doxygen）</span></span><br><span class="line"><span class="built_in">cd</span> opencc-1.0.4/</span><br><span class="line">make</span><br><span class="line"><span class="comment"># 4.安装</span></span><br><span class="line">sudo make install</span><br><span class="line"><span class="comment"># 5.使用</span></span><br><span class="line">opencc -i &lt;input-file&gt; -o output-file&gt; -c &lt;config-file&gt; </span><br><span class="line"><span class="comment"># 例如：opencc -i wiki.zh.txt -o wiki.zh.simp.txt -c t2s.json</span></span><br></pre></td></tr></table></figure></li><li><p>gensim</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gensim</span><br></pre></td></tr></table></figure></li><li><p>Wikipedia Extractor</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install unzip python python-dev python-pip</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/attardi/wikiextractor.git wikiextractor</span><br><span class="line">$ <span class="built_in">cd</span> wikiextractor</span><br><span class="line">$ sudo python setup.py install</span><br></pre></td></tr></table></figure></li><li><p>jieba</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li></ul><h3 id="下载语料库并准备预料资源"><a href="#下载语料库并准备预料资源" class="headerlink" title="下载语料库并准备预料资源"></a>下载语料库并准备预料资源</h3><ol><li>标准的word2vec中文语料库目前影响范围最广的为维基百科语料库，下载地址：</li></ol><ul><li><a href="https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2" target="_blank" rel="noopener">https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2</a> （1.3G）</li></ul><ol><li><p>抽取语料正文</p><p>可以使用工具<code>Wikipedia Extractor</code>进行抽取：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./WikiExtractor.py -b 1024M -o extracted zhwiki-latest-pages-articles.xml.bz2</span><br></pre></td></tr></table></figure><blockquote><p>参数-b 1024M表示以1024M为单位切分文件，默认是1M。</p></blockquote><p>（推荐，处理完成可以跳过第4步）也可以使用网上大牛写的处理脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> gensim.corpora.wikicorpus <span class="keyword">import</span> extract_pages,filter_wiki</span><br><span class="line"><span class="keyword">import</span> bz2file</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> opencc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line">wiki = extract_pages(bz2file.open(<span class="string">'zhwiki-latest-pages-articles.xml.bz2'</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wiki_replace</span><span class="params">(d)</span>:</span></span><br><span class="line">    s = d[<span class="number">1</span>]</span><br><span class="line">    s = re.sub(<span class="string">':*&#123;\|[\s\S]*?\|&#125;'</span>, <span class="string">''</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'&lt;gallery&gt;[\s\S]*?&lt;/gallery&gt;'</span>, <span class="string">''</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'(.)&#123;&#123;([^&#123;&#125;\n]*?\|[^&#123;&#125;\n]*?)&#125;&#125;'</span>, <span class="string">'\\1[[\\2]]'</span>, s)</span><br><span class="line">    s = filter_wiki(s)</span><br><span class="line">    s = re.sub(<span class="string">'\* *\n|\'&#123;2,&#125;'</span>, <span class="string">''</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'\n+'</span>, <span class="string">'\n'</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'\n[:;]|\n +'</span>, <span class="string">'\n'</span>, s)</span><br><span class="line">    s = re.sub(<span class="string">'\n=='</span>, <span class="string">'\n\n=='</span>, s)</span><br><span class="line">    s = <span class="string">u'【'</span> + d[<span class="number">0</span>] + <span class="string">u'】\n'</span> + s</span><br><span class="line">    <span class="keyword">return</span> opencc.convert(s).strip()</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">f = codecs.open(<span class="string">'wiki.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">w = tqdm(wiki, desc=<span class="string">u'已获取0篇文章'</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> re.findall(<span class="string">'^[a-zA-Z]+:'</span>, d[<span class="number">0</span>]) <span class="keyword">and</span> d[<span class="number">0</span>] <span class="keyword">and</span> <span class="keyword">not</span> re.findall(<span class="string">u'^#'</span>, d[<span class="number">1</span>]):</span><br><span class="line">        s = wiki_replace(d)</span><br><span class="line">        f.write(s+<span class="string">'\n\n\n'</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            w.set_description(<span class="string">u'已获取%s篇文章'</span>%i)</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure></li><li><p>繁体转简体</p><p>维基百科的中文数据是繁简混杂的，里面包含大陆简体、台湾繁体、港澳繁体等多种不同的数据。有时候在一篇文章的不同段落间也会使用不同的繁简字。</p><p>为了处理方便起见，我们直接使用了开源项目<code>opencc</code>。参照安装说明的方法，安装完成之后，使用下面的命令进行繁简转换，整个过程也很快：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install opencc</span><br><span class="line">$ opencc -i wiki_00 -o zh_wiki_00 -c zht2zhs.ini</span><br></pre></td></tr></table></figure><p>命令中的<code>wiki_00</code>这个文件是此前使用<code>Wikipedia Extractor</code>得到的。到了这里，我们已经完成了大部分繁简转换工作。</p></li><li><p>预处理</p><p>由于Wikipedia Extractor抽取正文时，会将有特殊标记的外文直接剔除。我们最后再将「」『』这些符号替换成引号，顺便删除空括号，就大功告成了！代码如下： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfun</span><span class="params">(input_file)</span>:</span></span><br><span class="line">    p1 = re.compile(<span class="string">ur'-\&#123;.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\&#125;-'</span>)</span><br><span class="line">    p2 = re.compile(<span class="string">ur'[（\(][，；。？！\s]*[）\)]'</span>)</span><br><span class="line">    p3 = re.compile(<span class="string">ur'[「『]'</span>)</span><br><span class="line">    p4 = re.compile(<span class="string">ur'[」』]'</span>)</span><br><span class="line">    outfile = codecs.open(<span class="string">'std_'</span> + input_file, <span class="string">'w'</span>, <span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">with</span> codecs.open(input_file, <span class="string">'r'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> myfile:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> myfile:</span><br><span class="line">            line = p1.sub(<span class="string">ur'\2'</span>, line)</span><br><span class="line">            line = p2.sub(<span class="string">ur''</span>, line)</span><br><span class="line">            line = p3.sub(<span class="string">ur'“'</span>, line)</span><br><span class="line">            line = p4.sub(<span class="string">ur'”'</span>, line)</span><br><span class="line">   <span class="comment"># line = re.sub('[a-zA-Z0-9]','',line)</span></span><br><span class="line">            outfile.write(line)</span><br><span class="line">    outfile.close()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Usage: python script.py inputfile"</span></span><br><span class="line">        sys.exit()</span><br><span class="line">    reload(sys)</span><br><span class="line">    sys.setdefaultencoding(<span class="string">'utf-8'</span>)</span><br><span class="line">    input_file = sys.argv[<span class="number">1</span>]</span><br><span class="line">    myfun(input_file)</span><br></pre></td></tr></table></figure></li></ol><p>5.中文分词</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python -m jieba -d <span class="string">" "</span> ./std_zh_wiki_00 &gt; ./cut_std_zh_wiki_00</span><br></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>模型训练使用工具<code>gensim</code>，可以通过<code>pip install gensim</code>直接安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import word2vec</span><br><span class="line">import logging</span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line">sentences = word2vec.LineSentence(u<span class="string">'./cut_std_zh_wiki_00'</span>)</span><br><span class="line">model = word2vec.Word2Vec(sentences,size=200,window=5,min_count=5,workers=4)</span><br><span class="line">outp1 = <span class="string">'wiki.zh.text.model'</span></span><br><span class="line">outp2 = <span class="string">'wiki.zh.text.vector'</span></span><br><span class="line">model.save(outp1)</span><br><span class="line">model.wv.save_word2vec_format(outp2, binary=False)</span><br></pre></td></tr></table></figure><h3 id="训练效果评价"><a href="#训练效果评价" class="headerlink" title="训练效果评价"></a>训练效果评价</h3><h4 id="词聚类"><a href="#词聚类" class="headerlink" title="词聚类"></a>词聚类</h4><p>可以采用 kmeans 聚类，看聚类簇的分布</p><h4 id="词cos-相关性"><a href="#词cos-相关性" class="headerlink" title="词cos 相关性"></a>词cos 相关性</h4><p>查找cos相近的词</p><h4 id="Analogy对比"><a href="#Analogy对比" class="headerlink" title="Analogy对比"></a>Analogy对比</h4><p>a:b 与 c:d的cos距离 (man-king woman-queen )</p><h4 id="使用tnse，pca等降维可视化展示"><a href="#使用tnse，pca等降维可视化展示" class="headerlink" title="使用tnse，pca等降维可视化展示"></a>使用tnse，pca等降维可视化展示</h4><p>词的分布，推荐用google的<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="noopener">tensorboard</a>，可以多视角查看，如果不想搭建服务，直接<a href="https://link.zhihu.com/?target=http%3A//projector.tensorflow.org/" target="_blank" rel="noopener">访问这里</a>。另外可以用python的matplotlib。</p><p><img src="/2018/08/21/弄懂word2vec之实践/v2-7a3b04197eab7a2ccc40bfab3f81b7f6_hd.jpg" alt="img"></p><p><img src="/2018/08/21/弄懂word2vec之实践/v2-e8f100b3b0b5be4c343317e2db2bb706_hd.jpg" alt="img"></p><h4 id="Categorization-分类-看词在每个分类中的概率"><a href="#Categorization-分类-看词在每个分类中的概率" class="headerlink" title="Categorization 分类 看词在每个分类中的概率"></a>Categorization 分类 看词在每个分类中的概率</h4><div class="table-container"><table><thead><tr><th>词</th><th>动物</th><th>食物</th><th>汽车</th><th>电子</th></tr></thead><tbody><tr><td>橘子</td><td>0.11</td><td>0.68</td><td>0.12</td><td>0.11</td></tr><tr><td>鸟</td><td>0.66</td><td>0.11</td><td>0.13</td><td>0.11</td></tr><tr><td>雅阁</td><td>0.14</td><td>0.23</td><td>0.67</td><td>0.11</td></tr><tr><td>苹果</td><td>0.11</td><td>0.65</td><td>0.11</td><td>0.65</td></tr></tbody></table></div><blockquote><p>前三条来自<a href="https://link.zhihu.com/?target=https%3A//code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">官网的评测</a>方法</p><p>网上也有相关的word embedding 的评估方法，可以<a href="https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/D15-1036" target="_blank" rel="noopener">参考这里</a></p></blockquote><h2 id="模型使用"><a href="#模型使用" class="headerlink" title="模型使用"></a>模型使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="comment"># 模型加载</span></span><br><span class="line">model = gensim.models.word2vec.Word2Vec.load(<span class="string">'/data2/word2vec/new.model'</span>)</span><br></pre></td></tr></table></figure><ol><li>比较词关联</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.wv.most_similar(positive=[<span class="string">'woman'</span>, <span class="string">'king'</span>], negative=[<span class="string">'man'</span>], topn=<span class="number">1</span>)</span><br><span class="line">[(<span class="string">'queen'</span>, <span class="number">0.50882536</span>)]</span><br><span class="line">model.doesnt_match(<span class="string">"breakfast cereal dinner lunch"</span>;.split())</span><br><span class="line"><span class="string">'cereal'</span></span><br><span class="line">model.similarity(<span class="string">'woman'</span>, <span class="string">'man'</span>)</span><br><span class="line"><span class="number">0.73723527</span></span><br></pre></td></tr></table></figure><ol><li>输出词向量</li></ol><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">model</span>[<span class="string">'中国'</span>]</span><br></pre></td></tr></table></figure><h3 id="利用word2vec实现的关键词提取"><a href="#利用word2vec实现的关键词提取" class="headerlink" title="利用word2vec实现的关键词提取"></a>利用word2vec实现的关键词提取</h3><blockquote><p>参考自<a href="https://kexue.fm/archives/4316" target="_blank" rel="noopener">【不可思议的Word2Vec】 3.提取关键词</a> </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line">model = gensim.models.word2vec.Word2Vec.load(<span class="string">'word2vec_wx'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_proba</span><span class="params">(oword, iword)</span>:</span></span><br><span class="line">    iword_vec = model[iword]</span><br><span class="line">    oword = model.wv.vocab[oword]</span><br><span class="line">    oword_l = model.syn1[oword.point].T</span><br><span class="line">    dot = np.dot(iword_vec, oword_l)</span><br><span class="line">    lprob = -sum(np.logaddexp(<span class="number">0</span>, -dot) + oword.code*dot) </span><br><span class="line">    <span class="keyword">return</span> lprob</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keywords</span><span class="params">(s)</span>:</span></span><br><span class="line">    s = [w <span class="keyword">for</span> w <span class="keyword">in</span> s <span class="keyword">if</span> w <span class="keyword">in</span> model]</span><br><span class="line">    ws = &#123;w:sum([predict_proba(u, w) <span class="keyword">for</span> u <span class="keyword">in</span> s]) <span class="keyword">for</span> w <span class="keyword">in</span> s&#125;</span><br><span class="line">    <span class="keyword">return</span> Counter(ws).most_common()</span><br><span class="line">  </span><br><span class="line"> s = <span class="string">u'2018年7月17日，国务院食品安全办在全国食品安全宣传周主会场公布了7起食品保健食品欺诈和虚假宣传整治案件：一、广东广州钟某、林某等制售非法添加药品的食品案2017年10月，广东省广州市公安机关联合食品药品监管部门破获钟某、林某等制售非法添加药品的食品案，涉案货值逾亿元。查获非法添加西药成分的咖啡1万多盒、半成品及原料粉末3吨。林某等在咖啡、蜂蜜、减肥茶等食品中非法添加他达拉非、西布曲明等西药成分，并假冒其他品牌食品销售。钟某、林某等5名犯罪嫌疑人已被依法批捕。二、河北石家庄秦某等制售非法添加药品的食品案2018年1月，河北省石家庄市公安机关、食品药品监管部门联手侦破秦某等制售非法添加药品的食品案，涉案货值3500余万元，捣毁生产、销售、储存窝点7处，打掉加工生产线12条，食品成品6500余盒。秦某等非法添加苯乙双胍等西药成分生产“胰宝唐安” “唐康”等宣称降糖功能的食品，通过电话推销、聘用假冒专家等进行销售。秦某等犯罪嫌疑人已被依法批捕。'</span></span><br><span class="line">pd.Series(keywords(jieba.cut(s)))</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line"><span class="number">0</span>        (制售, <span class="number">-1987.67916328</span>)</span><br><span class="line"><span class="number">1</span>        (药品, <span class="number">-2023.50590123</span>)</span><br><span class="line"><span class="number">2</span>        (食品, <span class="number">-2037.03856341</span>)</span><br><span class="line"><span class="number">3</span>        (查获, <span class="number">-2042.32583652</span>)</span><br><span class="line"><span class="number">4</span>        (窝点, <span class="number">-2056.79129232</span>)</span><br><span class="line"><span class="number">5</span>      (保健食品, <span class="number">-2064.19993636</span>)</span><br><span class="line"><span class="number">6</span>        (西药, <span class="number">-2073.77825261</span>)</span><br><span class="line"><span class="number">7</span>      (监管部门, <span class="number">-2082.29241248</span>)</span><br><span class="line"><span class="number">8</span>        (非法, <span class="number">-2089.17887737</span>)</span><br><span class="line"><span class="number">9</span>        (销售, <span class="number">-2099.67683021</span>)</span><br><span class="line"><span class="number">10</span>     (食品安全, <span class="number">-2104.52601517</span>)</span><br><span class="line"><span class="number">11</span>       (涉案, <span class="number">-2106.51930849</span>)</span><br><span class="line"><span class="number">12</span>        (加工, <span class="number">-2110.4709589</span>)</span><br><span class="line"><span class="number">13</span>       (破获, <span class="number">-2111.47298087</span>)</span><br><span class="line"><span class="number">14</span>        (案, <span class="number">-2111.82806077</span>)</span><br><span class="line"><span class="number">15</span>       (欺诈, <span class="number">-2112.95137531</span>)</span><br><span class="line"><span class="number">16</span>       (生产, <span class="number">-2115.57760446</span>)</span><br><span class="line"><span class="number">17</span>       (原料, <span class="number">-2124.36521695</span>)</span><br><span class="line"><span class="number">18</span>       (案件, <span class="number">-2125.21520575</span>)</span><br><span class="line"><span class="number">19</span>        (等, <span class="number">-2126.20670381</span>)</span><br><span class="line"><span class="number">20</span>       (依法, <span class="number">-2126.65278663</span>)</span><br><span class="line"><span class="number">21</span>     (公安机关, <span class="number">-2130.61494485</span>)</span><br><span class="line"><span class="number">22</span>       (咖啡, <span class="number">-2140.98083555</span>)</span><br><span class="line"><span class="number">23</span>      (半成品, <span class="number">-2143.70307556</span>)</span><br><span class="line"><span class="number">24</span>        (虚假, <span class="number">-2144.3973437</span>)</span><br><span class="line"><span class="number">25</span>        (侦破, <span class="number">-2144.4260255</span>)</span><br><span class="line"><span class="number">26</span>       (假冒, <span class="number">-2145.73825612</span>)</span><br><span class="line"><span class="number">27</span>       (曲明, <span class="number">-2147.48180124</span>)</span><br><span class="line"><span class="number">28</span>     (食品药品, <span class="number">-2151.19137764</span>)</span><br><span class="line"><span class="number">29</span>       (专家, <span class="number">-2151.42459357</span>)</span><br><span class="line">                ...          </span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure><h3 id="基于词向量的聚类分析"><a href="#基于词向量的聚类分析" class="headerlink" title="基于词向量的聚类分析"></a>基于词向量的聚类分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line">sentences = [[<span class="string">'this'</span>, <span class="string">'is'</span>, <span class="string">'the'</span>, <span class="string">'good'</span>, <span class="string">'machine'</span>, <span class="string">'learning'</span>, <span class="string">'book'</span>],</span><br><span class="line">            [<span class="string">'this'</span>, <span class="string">'is'</span>,  <span class="string">'another'</span>, <span class="string">'book'</span>],</span><br><span class="line">            [<span class="string">'one'</span>, <span class="string">'more'</span>, <span class="string">'book'</span>],</span><br><span class="line">            [<span class="string">'this'</span>, <span class="string">'is'</span>, <span class="string">'the'</span>, <span class="string">'new'</span>, <span class="string">'post'</span>],</span><br><span class="line">                        [<span class="string">'this'</span>, <span class="string">'is'</span>, <span class="string">'about'</span>, <span class="string">'machine'</span>, <span class="string">'learning'</span>, <span class="string">'post'</span>],  </span><br><span class="line">            [<span class="string">'and'</span>, <span class="string">'this'</span>, <span class="string">'is'</span>, <span class="string">'the'</span>, <span class="string">'last'</span>, <span class="string">'post'</span>]]</span><br><span class="line">model = Word2Vec(sentences, min_count=<span class="number">1</span>)</span><br><span class="line">X = model[model.wv.vocab]</span><br></pre></td></tr></table></figure><ol><li><p>使用nltk.cluster.KMeansClusterer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from nltk.cluster import KMeansClusterer</span><br><span class="line">import nltk</span><br><span class="line">NUM_CLUSTERS=3</span><br><span class="line">kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)</span><br><span class="line">assigned_clusters = kclusterer.cluster(X, assign_clusters=True)</span><br><span class="line"><span class="built_in">print</span> (assigned_clusters)</span><br><span class="line"></span><br><span class="line">[1, 0, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 1, 0]</span><br><span class="line">words = list(model.wv.vocab)</span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(words):  </span><br><span class="line">    <span class="built_in">print</span> (word + <span class="string">":"</span> + str(assigned_clusters[i]))</span><br><span class="line">post:1</span><br><span class="line">one:0</span><br><span class="line">the:2</span><br><span class="line">machine:2</span><br><span class="line">another:0</span><br><span class="line">this:2</span><br><span class="line">last:2</span><br><span class="line">is:2</span><br><span class="line">book:2</span><br><span class="line">learning:2</span><br><span class="line">new:1</span><br><span class="line">good:0</span><br><span class="line">more:2</span><br><span class="line">and:1</span><br><span class="line">about:0</span><br></pre></td></tr></table></figure></li><li><p>使用sklearn.Keams</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> cluster</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line"> </span><br><span class="line">labels = kmeans.labels_</span><br><span class="line">centroids = kmeans.cluster_centers_</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Cluster id labels for inputted data"</span>)</span><br><span class="line"><span class="keyword">print</span> (labels)</span><br><span class="line"><span class="comment">#print ("Centroids data")</span></span><br><span class="line"><span class="comment">#print (centroids)</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):"</span>)</span><br><span class="line"><span class="keyword">print</span> (kmeans.score(X))</span><br><span class="line"> </span><br><span class="line">silhouette_score = metrics.silhouette_score(X, labels, metric=<span class="string">'euclidean'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Silhouette_score: "</span>)</span><br><span class="line"><span class="keyword">print</span> (silhouette_score)</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">Cluster id labels <span class="keyword">for</span> inputted data</span><br><span class="line">[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span>]</span><br><span class="line">Score (Opposite of the value of X on the K-means objective which <span class="keyword">is</span> Sum of distances of samples to their closest cluster center):</span><br><span class="line"><span class="number">-0.00961963500595</span></span><br><span class="line">Silhouette_score: </span><br><span class="line"><span class="number">0.0289495</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Word2vec的可视化"><a href="#Word2vec的可视化" class="headerlink" title="Word2vec的可视化"></a>Word2vec的可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.tensorboard.plugins <span class="keyword">import</span> projector</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(model, output_path)</span>:</span></span><br><span class="line">    meta_file = <span class="string">"w2x_metadata.tsv"</span></span><br><span class="line">    placeholder = np.zeros((len(model.wv.index2word), <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(output_path,meta_file), <span class="string">'wb'</span>) <span class="keyword">as</span> file_metadata:</span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(model.wv.index2word):</span><br><span class="line">            placeholder[i] = model[word]</span><br><span class="line">            <span class="comment"># temporary solution for https://github.com/tensorflow/tensorflow/issues/9094</span></span><br><span class="line">            <span class="keyword">if</span> word == <span class="string">''</span>:</span><br><span class="line">                print(<span class="string">"Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard"</span>)</span><br><span class="line">                file_metadata.write(<span class="string">"&#123;0&#125;"</span>.format(<span class="string">'&lt;Empty Line&gt;'</span>).encode(<span class="string">'utf-8'</span>) + <span class="string">b'\n'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                file_metadata.write(<span class="string">"&#123;0&#125;"</span>.format(word).encode(<span class="string">'utf-8'</span>) + <span class="string">b'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define the model without training</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">    embedding = tf.Variable(placeholder, trainable = <span class="keyword">False</span>, name = <span class="string">'w2x_metadata'</span>)</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    writer = tf.summary.FileWriter(output_path, sess.graph)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># adding into projector</span></span><br><span class="line">    config = projector.ProjectorConfig()</span><br><span class="line">    embed = config.embeddings.add()</span><br><span class="line">    embed.tensor_name = <span class="string">'w2x_metadata'</span></span><br><span class="line">    embed.metadata_path = meta_file</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Specify the width and height of a single thumbnail.</span></span><br><span class="line">    projector.visualize_embeddings(writer, config)</span><br><span class="line">    saver.save(sess, os.path.join(output_path,<span class="string">'w2x_metadata.ckpt'</span>))</span><br><span class="line">    print(<span class="string">'Run `tensorboard --logdir=&#123;0&#125;` to run visualize result on tensorboard'</span>.format(output_path))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    model = Word2Vec.load(<span class="string">"./data/model/word2vec_wx"</span>)</span><br><span class="line">    visualize(model,<span class="string">"./visual/"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Run `tensorboard --logdir=./visual/` to run visualize result on tensorboard</span><br></pre></td></tr></table></figure><h2 id="Word2vec的应用场景"><a href="#Word2vec的应用场景" class="headerlink" title="Word2vec的应用场景"></a>Word2vec的应用场景</h2><blockquote><p>参考自<a href="http://www.zhuanzhi.ai/document/b614a5ceb61c95574515415a53cbf2e3" target="_blank" rel="noopener">word2vec在工业界的应用场景</a></p></blockquote><h3 id="在社交网络中的推荐"><a href="#在社交网络中的推荐" class="headerlink" title="在社交网络中的推荐"></a>在社交网络中的推荐</h3><p>在个性化推荐的场景，给当前用户推荐他可能关注的『大V』。对一个新用户，此题基本无解，如果在已知用户关注了几个『大V』之后，相当于知道了当前用户的一些关注偏好，根据此偏好给他推荐和他关注过大V相似的大V，就是一个很不错的推荐策略。所以，如果可以求出来任何两个V用户的相似度，上面问题就可以基本得到解决。</p><p>我们知道word2vec中两个词的相似度可以直接通过余弦来衡量，接下来就是如何将每个V用户变为一个词向量的问题了。巧妙的地方就是如何定义doc和word，针对上面问题，可以将doc和word定义为：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">word</span> -&gt;</span>   每一个大V就是一个词</span><br><span class="line"><span class="function"><span class="title">doc</span>  -&gt;</span>   根据每一个用户关注大V的顺序，生成一篇文章</span><br></pre></td></tr></table></figure><p>由于用户量很大（大约4亿），可以将关注word个数少的doc删掉，因为本身大V的种类是十万级别（如果我没记错的话）， 选择可以覆盖绝大多数大V的文章数量就足够了。</p><h3 id="计算商品的相似度"><a href="#计算商品的相似度" class="headerlink" title="计算商品的相似度"></a>计算商品的相似度</h3><p>在商品推荐的场景中，竞品推荐和搭配推荐的时候都有可能需要计算任何两个商品的相似度，根据浏览/收藏/下单/App下载等行为，可以将商品看做词，将每一个用户的一类行为序看做一个文档，通过word2vec将其训练为一个向量。</p><p>同样的，在计算广告中，根据用户的点击广告的点击序列，将每一个广告变为一个向量。变为向量后，用此向量可以生成特征融入到rank模型中。</p><h3 id="作为另一个模型的输入"><a href="#作为另一个模型的输入" class="headerlink" title="作为另一个模型的输入"></a>作为另一个模型的输入</h3><p>在nlp的任务中，可以通过将词聚类后，生成一维新的特征来使用。在CRF实体识别的任务中，聚类结果类似词性，可以作为特征来使用。</p><p>在依存句法分析的任务中，哈工大ltp的nndepparser则是将词向量直接作为输入。</p><p>具体论文『A Fast and Accurate Dependency Parser using Neural Networks』</p><h3 id="向量快速检索"><a href="#向量快速检索" class="headerlink" title="向量快速检索"></a>向量快速检索</h3><p>当我们将一个文档变成一个向量之后，如何根据余弦/欧氏距离快速得到其最相似的top k个文章，是工程实现上不得不考虑的问题。例如线上可以允许的时间是5ms以内，如果文章数量往往上万或者更多，O(n)的方式计算明显不可接受了。</p><p>如果文章更新的速度很慢，可以通过离线的方式一天或者几天计算一次，导入redis（或者别的）提供线上快速查询。 但是如果文章实时新增，并且大量流量来自新文章，这个问题就要好好考虑一下。</p><p>一般可以通过PQ, kd-tree、simhash、聚类等方式解决，选择不同的方式和具体的推荐场景、数据分布有关。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/29364112" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29364112</a></li><li><a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">https://code.google.com/archive/p/word2vec/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/08.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://blog.a-stack.com/categories/NLP/"/>
    
    
      <category term="实践" scheme="http://blog.a-stack.com/tags/%E5%AE%9E%E8%B7%B5/"/>
    
      <category term="word2vec" scheme="http://blog.a-stack.com/tags/word2vec/"/>
    
      <category term="词向量" scheme="http://blog.a-stack.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>CRUSH in Ceph</title>
    <link href="http://blog.a-stack.com/2018/08/19/CRUSH-in-Ceph/"/>
    <id>http://blog.a-stack.com/2018/08/19/CRUSH-in-Ceph/</id>
    <published>2018-08-19T11:13:01.000Z</published>
    <updated>2018-08-19T15:32:33.813Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/22.jpg" alt="聊聊Ceph中的核心数据分布选择算法——CRUSH"></p><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>CRUSH算法构建了Ceph存储体系的核心，解决了计算效率和计算性能的同步优化，解放了元数据节点，让ceph的客户端侧完成数据目标位置的计算任务，保障了系统的可扩展性和鲁棒性。具备计算寻址、高并发能力、动态数据再平衡、可定制的副本策略等优秀特性。</p><p><strong>CRUSH</strong>：Controlled Replication Under Scalable Hashing ，基于可扩展哈希的受控副本分布策略。CRUSH本质上是一种基于hash的数据分布式算法，主要解决两个问题：</p><ol><li><p>当系统发生变化时，如何最小化数据的迁移量并尽快恢复平衡；</p></li><li><p>如何保证数据的多个副本合理分布，保证数据的可靠性；</p></li></ol><h2 id="CRUSH中的5种设备选择算法"><a href="#CRUSH中的5种设备选择算法" class="headerlink" title="CRUSH中的5种设备选择算法"></a>CRUSH中的5种设备选择算法</h2><div class="table-container"><table><thead><tr><th>对比项/算法</th><th>unique</th><th>list</th><th>tree</th><th>straw</th><th>staw2</th></tr></thead><tbody><tr><td>时间复杂度</td><td>O(1)</td><td>O(N)</td><td>O(log(N))</td><td>O(N)</td><td>O(N)</td></tr><tr><td>添加元素</td><td>差</td><td>最好</td><td>好</td><td>最好</td><td>最好</td></tr><tr><td>删除元素</td><td>差</td><td>差</td><td>好</td><td>最好</td><td>最好</td></tr></tbody></table></div><p>以上对比可以看出，straw虽然时间复杂度高，但应对环境变化能力强，所以作为ceph的默认配置算法。</p><p>straw算法的本质是一个抽签算法，选取抽取签长最长的作为结果，同时考虑不同设备的权重，来补偿签长权重，让容量大的设备更容易被抽中。</p><p>straw抽签算法可以保证，每个设备的签长计算只与该设备的权重有关，这样当添加或删除设备时，不会导致数据在除添加/删除之外的两个元素之间迁移。</p><p>但straw算法并非完美，在算法设计的权重排序中，由于需要和其它元素的权重进行比较，从而导致每次有元素加入当前集合或从当前集合删除时，都会引起不相关的数据迁移。为此，straw2提出了改良算法。</p><h2 id="CRUSH算法"><a href="#CRUSH算法" class="headerlink" title="CRUSH算法"></a>CRUSH算法</h2><p><strong>输入：</strong>x，cluster map，placement rule</p><p>目的： 为输入PG寻找合适的OSD set</p><script type="math/tex; mode=display">CRUSH（x,cluster map, placement rule） = (osd0,osd1,…, osdn)</script><h3 id="Cluster-Map"><a href="#Cluster-Map" class="headerlink" title="Cluster Map"></a>Cluster Map</h3><p>cluster map也成为CRUSH map是集群拓扑结构的逻辑描述形式，采用树这种数据结构实现，比如“数据中心——机架——主机——磁盘”的树状层级关系。其中叶子节点为device，中间节点统称bucket，根节点为root，每个节点通过ID和类型描述。</p><p>cluster map中包含CRUSH MAP Devices表，用于统计所有的设备资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#devices</span></span><br><span class="line">device &#123;num&#125; &#123;osd.name&#125;</span><br></pre></td></tr></table></figure><p>Bucket类型表，描述了不同的设备类型信息：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">type 0 osd</span><br><span class="line">type 1 host</span><br><span class="line">type 2 chassis</span><br><span class="line">type 3 rack</span><br><span class="line">type 4 row</span><br><span class="line">type 5 pdu</span><br><span class="line">type 6 pod</span><br><span class="line">type 7 room</span><br><span class="line">type 8 datacenter</span><br><span class="line">type 9 region</span><br><span class="line">type 10 root</span><br></pre></td></tr></table></figure><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[bucket-type] [bucket-name] &#123;</span><br><span class="line">        id [a unique negative numeric ID]</span><br><span class="line">        weight [the relative capacity/capability of the item(<span class="name">s</span>)]</span><br><span class="line">        alg [the bucket type: uniform | list | tree | straw | straw2]</span><br><span class="line">        hash [the hash type: <span class="number">0</span> by default]</span><br><span class="line">        item [item-name] weight [weight]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">host ceph-node3 &#123;</span><br><span class="line">        id <span class="number">-4</span>           # do not change unnecessarily</span><br><span class="line">        # weight <span class="number">0.053</span></span><br><span class="line">        alg straw</span><br><span class="line">        hash <span class="number">0</span>  # rjenkins1</span><br><span class="line">        item osd<span class="number">.4</span> weight <span class="number">0.018</span></span><br><span class="line">        item osd<span class="number">.5</span> weight <span class="number">0.018</span></span><br><span class="line">        item osd<span class="number">.8</span> weight <span class="number">0.018</span></span><br><span class="line">&#125;</span><br><span class="line">root <span class="section">default</span> &#123;</span><br><span class="line">        id <span class="number">-1</span>           # do not change unnecessarily</span><br><span class="line">        # weight <span class="number">0.158</span></span><br><span class="line">        alg straw</span><br><span class="line">        hash <span class="number">0</span>  # rjenkins1</span><br><span class="line">        item ceph-node1 weight <span class="number">0.053</span></span><br><span class="line">        item ceph-node2 weight <span class="number">0.053</span></span><br><span class="line">        item ceph-node3 weight <span class="number">0.053</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Placement-Rule"><a href="#Placement-Rule" class="headerlink" title="Placement Rule"></a>Placement Rule</h3><p>数据分布策略：Placement Rule，用于利用cluster map完成数据映射，包括take，select和emit三种操作。</p><p>核心是select算法，将根据需求返回指定数量的叶子设备，并保证这些叶子设备位于不同的指定类型的容灾域之下。 </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">rule &lt;rulename&gt; &#123;</span><br><span class="line"></span><br><span class="line">        ruleset &lt;ruleset&gt;</span><br><span class="line">       <span class="built_in"> type </span>[ replicated | erasure ]</span><br><span class="line">        min_size &lt;min-size&gt;</span><br><span class="line">        max_size &lt;max-size&gt;</span><br><span class="line">        <span class="keyword">step</span> take &lt;bucket-name&gt;</span><br><span class="line">        <span class="keyword">step</span> [choose|chooseleaf] [firstn|indep] &lt;N&gt; &lt;bucket-type&gt;</span><br><span class="line">        <span class="keyword">step</span> emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CRUSH算法的缺陷"><a href="#CRUSH算法的缺陷" class="headerlink" title="CRUSH算法的缺陷"></a>CRUSH算法的缺陷</h3><p>CRUSH算法本身仍然存在缺陷，从基本选择算法上看，每次选择都是计算单个条目被选中的独立概率，但是CRUSH所要求的副本策略使得针对同一输入、多个副本之间的选择变成了计算条件概率，所以CRUSH无法处理好多副本模式下的副本均匀分布问题；</p><p>这导致了在Ceph集群中，特别是异构集群中，出现大量磁盘数据分布悬殊的情况，因此需要对其计算结果进行人工干预。除了本身的weight，人工设置一个reweight值，叫做过载测试。</p><h3 id="CRUSH规则修改常用命令"><a href="#CRUSH规则修改常用命令" class="headerlink" title="CRUSH规则修改常用命令"></a>CRUSH规则修改常用命令</h3><p>CRUSH修改满足如下流程：</p><ol><li><a href="http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#getcrushmap" target="_blank" rel="noopener">Get the CRUSH map</a>.</li><li><a href="http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#decompilecrushmap" target="_blank" rel="noopener">Decompile</a> the CRUSH map.</li><li>Edit at least one of <a href="http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmapdevices" target="_blank" rel="noopener">Devices</a>, <a href="http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmapbuckets" target="_blank" rel="noopener">Buckets</a> and <a href="http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmaprules" target="_blank" rel="noopener">Rules</a>.</li><li><a href="http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#compilecrushmap" target="_blank" rel="noopener">Recompile</a> the CRUSH map.</li><li>Test it.</li><li><a href="http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#setcrushmap" target="_blank" rel="noopener">Set the CRUSH map</a>.</li></ol><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.</span></span><br><span class="line"><span class="string">ceph </span><span class="string">osd </span><span class="string">getcrushmap </span>-o &#123;<span class="string">compiled </span><span class="string">crush </span><span class="string">map </span><span class="string">file&#125;</span></span><br><span class="line"><span class="comment"># 2.</span></span><br><span class="line"><span class="string">crushtool </span>-d &#123;<span class="string">compiled </span><span class="string">crush </span><span class="string">map </span><span class="string">file&#125;</span> -o &#123;<span class="string">decompiled </span><span class="string">crush </span><span class="string">map </span><span class="string">file&#125;</span></span><br><span class="line"><span class="comment"># 3.</span></span><br><span class="line"><span class="comment">#4.</span></span><br><span class="line"><span class="string">crushtool </span>-c &#123;<span class="string">decompiled </span><span class="string">crush </span><span class="string">map </span><span class="string">file&#125;</span> -o &#123;<span class="string">compiled </span><span class="string">crush </span><span class="string">map </span><span class="string">file&#125;</span></span><br><span class="line"><span class="comment"># 5. 模拟测试</span></span><br><span class="line"><span class="string">crushtool </span>-i <span class="string">mycrushmap </span><span class="built_in">--test</span> <span class="built_in">--min-x</span> - <span class="built_in">--max-x</span> 9 <span class="built_in">--num-rep</span> 3 <span class="built_in">--ruleset</span> 0 <span class="built_in">--show_mappings</span></span><br><span class="line"><span class="built_in">crushtool</span> -i <span class="string">mycrushmap </span><span class="built_in">--test</span> <span class="built_in">--min-x</span> - <span class="built_in">--max-x</span> 9 <span class="built_in">--num-rep</span> 3 <span class="built_in">--ruleset</span> 0 <span class="built_in">--show_utilization</span></span><br><span class="line"><span class="built_in">#</span> 6.</span><br><span class="line"><span class="string">ceph </span><span class="string">osd </span><span class="string">setcrushmap </span>-i &#123;<span class="string">compiled </span><span class="string">crush </span><span class="string">map </span><span class="string">file&#125;</span></span><br></pre></td></tr></table></figure><h3 id="定制CRUSH规则"><a href="#定制CRUSH规则" class="headerlink" title="定制CRUSH规则"></a>定制CRUSH规则</h3><ol><li><p>修改容灾域（如下为以rack作为容灾域）</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rules</span></span><br><span class="line">rule replicated_ruleset &#123;</span><br><span class="line">ruleset 0</span><br><span class="line"><span class="built_in">type </span>replicated</span><br><span class="line">min_size 1</span><br><span class="line">max_size 10</span><br><span class="line"><span class="keyword">step</span> take default</span><br><span class="line"><span class="keyword">step</span> chooseleaf firstn 0<span class="built_in"> type </span>rack</span><br><span class="line"><span class="keyword">step</span> emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>2.限制只选择特定范围的OSD，比如下面只选择rack2的OSD</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rules</span></span><br><span class="line">rule replicated_ruleset &#123;</span><br><span class="line">ruleset 0</span><br><span class="line"><span class="built_in">type </span>replicated</span><br><span class="line">min_size 1</span><br><span class="line">max_size 10</span><br><span class="line"><span class="keyword">step</span> take rack2</span><br><span class="line"><span class="keyword">step</span> chooseleaf firstn 0<span class="built_in"> type </span>host</span><br><span class="line"><span class="keyword">step</span> emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li><p>构建虚拟隔离池</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">host virtualhost &#123;</span><br><span class="line">id -10# <span class="keyword">do</span> <span class="keyword">not</span> change unnecessarily</span><br><span class="line"># weight 0.053</span><br><span class="line">alg straw2</span><br><span class="line">hash 0# rjenkins1</span><br><span class="line">item osd.3 weight 0.018</span><br><span class="line">item osd.6 weight 0.018</span><br><span class="line">item osd.7 weight 0.018</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># rules</span></span><br><span class="line">rule replicated_ruleset &#123;</span><br><span class="line">ruleset 0</span><br><span class="line"><span class="built_in">type </span>replicated</span><br><span class="line">min_size 1</span><br><span class="line">max_size 10</span><br><span class="line"><span class="keyword">step</span> take virtualhost</span><br><span class="line"><span class="keyword">step</span> chooseleaf firstn 0<span class="built_in"> type </span>osd</span><br><span class="line"><span class="keyword">step</span> emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="数据重平衡"><a href="#数据重平衡" class="headerlink" title="数据重平衡"></a>数据重平衡</h3><p>人为干预可以通过reweight来调节数据分布：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd reweight &#123;osd_numeric_id&#125; &#123;reweight&#125;</span><br></pre></td></tr></table></figure><p>也可以批量调整，比如按照<code>reweight-by-utilizaiton</code> 或 <code>reweight-by-pg</code>,可以调整前通过如下命令测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">test</span>-reweight-by-utilization &#123;overload&#125; &#123;max_change&#125; &#123;max_osds&#125; &#123;--no-incresing&#125;</span><br><span class="line">[root@ceph-node1 ~]<span class="comment"># ceph osd test-reweight-by-utilization 105 .2 4 --no-increasing</span></span><br><span class="line">no change</span><br><span class="line">moved 6 / 384 (1.5625%)</span><br><span class="line">avg 42.6667</span><br><span class="line">stddev 5.4365 -&gt; 4.49691 (expected baseline 6.1584)</span><br><span class="line">min osd.7 with 33 -&gt; 37 pgs (0.773438 -&gt; 0.867188 * mean)</span><br><span class="line">max osd.6 with 52 -&gt; 48 pgs (1.21875 -&gt; 1.125 * mean)</span><br><span class="line"></span><br><span class="line">oload 105</span><br><span class="line">max_change 0.2</span><br><span class="line">max_change_osds 4</span><br><span class="line">average 0.007894</span><br><span class="line">overload 0.008289</span><br><span class="line">osd.6 weight 1.000000 -&gt; 0.843857</span><br><span class="line">osd.4 weight 1.000000 -&gt; 0.940109</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>overload</td><td>可选，默认值为120，为大于100的数，当且仅当OSD的空间利用与大于等于集群平均利用率的overload/100时，调整reweight</td></tr><tr><td>max_change</td><td>[0,1],每次调整reweight的最大幅度，默认0.05</td></tr><tr><td>max_osds</td><td>整型，默认 4；每次至多调整的OSD数目</td></tr><tr><td>–no-incresing</td><td>可选，不将reweight进行上调</td></tr></tbody></table></div><h2 id="其它可配置信息"><a href="#其它可配置信息" class="headerlink" title="其它可配置信息"></a>其它可配置信息</h2><h3 id="主OSD的选择"><a href="#主OSD的选择" class="headerlink" title="主OSD的选择"></a>主OSD的选择</h3><p>可以通过给OSD一个[0,1]之间的权重，来选择该OSD是否适合作为主OSD，0代表不作为主OSD，1作为可以被选为主OSD。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd primary-affinity <span class="tag">&lt;<span class="name">osd-id</span>&gt;</span> <span class="tag">&lt;<span class="name">weight</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Tunable"><a href="#Tunable" class="headerlink" title="Tunable"></a>Tunable</h3><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ceph osd crush tunables optimal</span></span><br></pre></td></tr></table></figure><ul><li><code>legacy</code>: the legacy behavior from argonaut and earlier.</li><li><code>argonaut</code>: the legacy values supported by the original argonaut release</li><li><code>bobtail</code>: the values supported by the bobtail release</li><li><code>firefly</code>: the values supported by the firefly release</li><li><code>optimal</code>: the current best values</li><li><code>default</code>: the current default values for a new cluster</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>《Ceph设计与实现原理》</li><li><a href="https://ceph.com/wp-content/uploads/2016/08/weil-crush-sc06.pdf" target="_blank" rel="noopener">《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》</a></li><li><a href="http://docs.ceph.com/docs/jewel/rados/operations/crush-map/" target="_blank" rel="noopener">http://docs.ceph.com/docs/jewel/rados/operations/crush-map/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/22.jpg&quot; alt=&quot;聊聊Ceph中的核心数据分布选择算法——CRUSH&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="云存储" scheme="http://blog.a-stack.com/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    
      <category term="云存储" scheme="http://blog.a-stack.com/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
      <category term="Ceph" scheme="http://blog.a-stack.com/tags/Ceph/"/>
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Ceph-install-by-ceph-ansible</title>
    <link href="http://blog.a-stack.com/2018/08/04/Ceph-install-by-ceph-ansible/"/>
    <id>http://blog.a-stack.com/2018/08/04/Ceph-install-by-ceph-ansible/</id>
    <published>2018-08-04T11:12:55.000Z</published>
    <updated>2018-08-14T14:59:47.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/banner/08.jpg" alt="Test Picture"></p><p><strong>摘要：</strong></p><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>操作系统：Centos 7.2</li><li>Vagrant：2.1.2</li><li>VirtualBox-5.1 </li><li>Ansible：2.6.2</li><li>pip</li><li>git</li></ul><h2 id="安装Vagrant"><a href="#安装Vagrant" class="headerlink" title="安装Vagrant"></a>安装Vagrant</h2><p>第1步：在CentOS 7上安装VirtualBox 5.1</p><p>尽管在www.howtoing.com上有几个关于安装virtualBox的教程（例如， <a href="https://www.howtoing.com/install-virtualbox-on-redhat-centos-fedora/" target="_blank" rel="noopener">在CentOS 7上安装VirtualBox</a> ），但是，我将很快通过virtualbox 5.1安装。</p><p>首先安装VirtualBox依赖项。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum -y install gcc dkms make qt libgomp patch </span></span><br><span class="line"><span class="comment"># yum -y install kernel-headers kernel-devel binutils glibc-headers glibc-devel font-forge</span></span><br></pre></td></tr></table></figure><p>接下来添加VirtualBox库。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /etc/yum.repos.d/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> wget http://download.virtualbox.org/virtualbox/rpm/rhel/virtualbox.repo</span></span><br></pre></td></tr></table></figure><p>现在安装和构建内核模块。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y VirtualBox-5.1</span></span><br><span class="line"><span class="comment"># /sbin/rcvboxdrv setup</span></span><br></pre></td></tr></table></figure><blockquote><p>Bug fix： yum install kernel-devel-3.10.0-862.3.3.el7.x86_64</p></blockquote><p>第2步：在CentOS 7上安装Vagrant</p><p>在这里，我们将使用<a href="https://www.howtoing.com/20-linux-yum-yellowdog-updater-modified-commands-for-package-mangement/" target="_blank" rel="noopener">yum命令</a>下载并安装Vagrant的最新版本（即在编写时为1.9.6）。</p><p>—————- For 64-bit machine —————-</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum -y install https://releases.hashicorp.com/vagrant/2.1.2/vagrant_2.1.2_x86_64.rpm</span></span><br></pre></td></tr></table></figure><h2 id="安装pip"><a href="#安装pip" class="headerlink" title="安装pip"></a>安装pip</h2><blockquote><p>需要先安装扩展源EPEL。EPEL(<a href="http://fedoraproject.org/wiki/EPEL" target="_blank" rel="noopener">http://fedoraproject.org/wiki/EPEL</a>) 是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS、Scientific Linux 等提供高质量软件包的项目。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sudo yum -y install epel-release</span></span><br><span class="line"><span class="comment"># sudo yum -y install python-pip</span></span><br></pre></td></tr></table></figure><h2 id="Ceph-Ansible"><a href="#Ceph-Ansible" class="headerlink" title="Ceph-Ansible"></a>Ceph-Ansible</h2><p>You can install directly from the source on GitHub by following these steps:</p><ul><li><p>Clone the repository:</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/ceph/ceph-ansible.git</span><br></pre></td></tr></table></figure></li><li><p>Next, you must decide which branch of <code>ceph-ansible</code> you wish to use. There are stable branches to choose from or you could use the master branch:</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">git</span> checkout <span class="variable">$branch</span></span><br></pre></td></tr></table></figure></li><li><p>Next, use pip and the provided requirements.txt to install ansible and other needed python libraries:</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> -r requirements.txt</span><br></pre></td></tr></table></figure></li></ul><h2 id="Ansible-on-RHEL-and-CentOS"><a href="#Ansible-on-RHEL-and-CentOS" class="headerlink" title="Ansible on RHEL and CentOS"></a>Ansible on RHEL and CentOS</h2><p>You can acquire Ansible on RHEL and CentOS by installing from <a href="https://access.redhat.com/articles/3174981" target="_blank" rel="noopener">Ansible channel</a>.</p><p>On RHEL:</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subscription-manager repos --enable=rhel<span class="number">-7</span>-server-ansible<span class="number">-2</span>-rpms</span><br></pre></td></tr></table></figure><p>(CentOS does not use subscription-manager and already has “Extras” enabled by default.)</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum <span class="keyword">install</span> ansible</span><br></pre></td></tr></table></figure><h2 id="Ansible-on-Ubuntu"><a href="#Ansible-on-Ubuntu" class="headerlink" title="Ansible on Ubuntu"></a>Ansible on Ubuntu</h2><p>You can acquire Ansible on Ubuntu by using the <a href="https://launchpad.net/~ansible/+archive/ubuntu/ansible" target="_blank" rel="noopener">Ansible PPA</a>.</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo<span class="built_in"> add-apt-repository </span>ppa:ansible/ansible</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install ansible</span><br></pre></td></tr></table></figure><h1 id="Releases"><a href="#Releases" class="headerlink" title="Releases"></a>Releases</h1><p>The following branches should be used depending on your requirements. The <code>stable-*</code>branches have been QE tested and sometimes recieve backport fixes throughout their lifecycle. The <code>master</code> branch should be considered experimental and used with caution.</p><ul><li><code>stable-3.0</code> Support for ceph versions <code>jewel</code> and <code>luminous</code>. This branch supports ansible versions <code>2.4</code> and <code>2.5</code>.</li><li><code>stable-3.1</code> Support for ceph version <code>luminous</code> and <code>mimic</code>. This branch supports ansible versions <code>2.4</code> and <code>2.5</code></li><li><code>master</code> Support for ceph versions <code>luminous</code>, and <code>mimic</code>. This branch supports ansible version 2.5``.</li></ul><p>We will customize the following sample files for sandbox deployment: vagrant_variables.yml.sample<br>options that ought to be supplied to Vagrantfile the instructions from Vagrantfile<br>need to be built and the actions to provision them. site.yml.sample<br>: This file presents a sample of configuration to describe the types of virtual machine that<br>. The vagrant command uses : This represents a set of sample Ansible tasks as a part of<br>main Ansible playbook. The vagrant provisioner that runs after you start all the virtual machines is tasked with the responsibility of running this playbook. This playbook is responsible for completing the final step of bringing the Ceph cluster up. group_vars/all.yml.sample<br>: The all .yml.sample present within group_vars/<br>directory contains an Ansible-specific configuration that applies to all virtual machine hostgroups. These variables will be consumed by the Ansible playbook when the vagrant provisioner runs it.</p><blockquote><p>在VM中需要配置V-TX；</p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://docs.ceph.com/ceph-ansible/master/index.html" target="_blank" rel="noopener">http://docs.ceph.com/ceph-ansible/master/index.html</a></li><li>​</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/banner/08.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="云存储" scheme="http://blog.a-stack.com/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    
      <category term="云存储" scheme="http://blog.a-stack.com/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
      <category term="部署" scheme="http://blog.a-stack.com/tags/%E9%83%A8%E7%BD%B2/"/>
    
      <category term="Ceph" scheme="http://blog.a-stack.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>经典网络复现之VGGNet</title>
    <link href="http://blog.a-stack.com/2018/07/12/2018-07-9-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0%E4%B9%8BVGG/"/>
    <id>http://blog.a-stack.com/2018/07/12/2018-07-9-经典网络复现之VGG/</id>
    <published>2018-07-12T05:58:12.000Z</published>
    <updated>2018-07-20T14:53:24.141Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/images/2018-07-09-经典网络复现之VGG/VGGNet.jpg" alt="VGGNet"></p><p><strong>摘要：</strong> 本文记录利用ImageNet数据集复现经典网络VGGNet的过程，并记录在大型数据集训练过程中需要考虑的问题。</p><a id="more"></a><p>为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容：</p><ul><li>ImageNet数据集及数据准备</li><li>AlexNet网络</li><li><strong>VGG网络</strong></li><li>GoogLeNet网络</li><li>ResNet网络</li><li>SqueezeNet网络</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>VGGNet是英国牛津大学2014年ImageNet ILSVRC第二名的网络，由于其良好的泛化性能，在被迁移到其它深度学习任务中体现出来良好的效果，是目前迁移学习中用的最多的网络。但由于网络规模和参数量实在庞大，从头训练一个网络充满难度。</p><ul><li>论文：<ul><li><a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></li></ul></li><li>数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见<a href="/2018/07/06/ImageNet-DataSet/">ImageNet-DataSet</a>）</li><li>计算框架： MxNet</li><li>算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour)<ul><li>总计训练时间&gt;10天左右</li></ul></li></ul><h2 id="重头训练一个VGGNet"><a href="#重头训练一个VGGNet" class="headerlink" title="重头训练一个VGGNet"></a>重头训练一个VGGNet</h2><p><img src="/qnsource/images/2018-07-09-经典网络复现之VGG/parameter_vgg.png" alt="parameter_vgg"></p><ol><li>由于网络深度太深、参数太多，所以从头按部就班的训练VGGNet是很漫长的过程，为此VGG的作者使用了一种<code>pre-training</code>的方式。通过不断训练小一号的网络架构，将训练后的参数作为后续较大网络的初始参数来逐步逼近完整的网络。比如为了训练VGG16，作者先训练了A网络VGG11，然后利用A网络的参数逐步训练B VGG13，最后才是D网络 VGG16。这种方式使用了<code>warmed pre-trained up</code>层参数来推进后续的网络训练。</li><li>虽然上述方法是个很好的技巧，但训练N个网络才能达到目的实在是一个痛苦的过程，随着深度学习技术的发展，尤其是参数初始化技术的发展，为训练VGG提供了更加高效的策略——使用高级的初始化策略，如Xavier或MSRA，同时配合使用参数化ReLU可以抛弃1中的预训练方式。</li></ol><h3 id="训练VGGNet"><a href="#训练VGGNet" class="headerlink" title="训练VGGNet"></a>训练VGGNet</h3><p>训练用的脚本和前面几个网络一致。</p><blockquote><p><strong>几个注意点：</strong></p><ol><li><code>batchSize = config.BATCH_SIZE * config.NUM_DEVICES</code>; 使用k80 12GB先存，选择batchSzie=<strong>32</strong>；</li><li>初始学习率为1e-2，动量0.9，L2权重正则化参数0.0005；rescale参数尤为关键，根据批的大小放大梯度：rescale_grad=1.0 / batchSize；</li><li>使用了MSRA进行参数初始化，initializer=mx.initializer.MSRAPrelu()；</li><li>同时使用参数PReLU代替ReLU作为激活函数；</li></ol></blockquote><h4 id="学习率的控制"><a href="#学习率的控制" class="headerlink" title="学习率的控制"></a>学习率的控制</h4><div class="table-container"><table><thead><tr><th>Epoch</th><th>学习率</th></tr></thead><tbody><tr><td>1-50</td><td>1e-2</td></tr><tr><td>51-70</td><td>1e-3</td></tr><tr><td>71-80</td><td>1e-4</td></tr></tbody></table></div><blockquote><ol><li>控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率；</li><li>调整学习率 <code>python train_alexnet.py --checkpoints checkpoints --prefix alexnet \ --start-epoch 50</code></li></ol></blockquote><p><img src="/qnsource/images/2018-07-09-经典网络复现之VGG/training.png" alt="training"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>1.选取40轮的训练结果，在测试集数据上进行验证，结果如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-1</span>: 71<span class="selector-class">.42</span>% </span><br><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-5</span>: 90<span class="selector-class">.03</span>%</span><br></pre></td></tr></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>为了训练VGGNet，一定要使用高级初始化参数方法，并配合参数化ReLU使用；</li><li>训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源；</li><li>训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型；</li><li>先使用ReLU作为激活函数获得baseline，再选择提花为ELU来获得提升；</li><li>训练深层网络时，考虑使用MSRA/HE的初始化参数，并配合PReLU一起使用，效果更好；</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/images/2018-07-09-经典网络复现之VGG/VGGNet.jpg&quot; alt=&quot;VGGNet&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 本文记录利用ImageNet数据集复现经典网络VGGNet的过程，并记录在大型数据集训练过程中需要考虑的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="动手实践营" scheme="http://blog.a-stack.com/categories/%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E8%90%A5/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="网络复现" scheme="http://blog.a-stack.com/tags/%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>经典网络复现之SqueezeNet</title>
    <link href="http://blog.a-stack.com/2018/07/12/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0%E4%B9%8BSqueezeNet/"/>
    <id>http://blog.a-stack.com/2018/07/12/经典网络复现之SqueezeNet/</id>
    <published>2018-07-12T05:58:12.000Z</published>
    <updated>2018-07-20T15:52:08.839Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/Fire-Module.PNG" alt="Fire-Module"></p><p><strong>摘要：</strong> 本文记录利用ImageNet数据集复现经典网络SqueezeNet的过程，并记录在大型数据集训练过程中需要考虑的问题。</p><a id="more"></a><p>为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容：</p><ul><li>ImageNet数据集及数据准备</li><li>AlexNet网络</li><li>VGG网络</li><li>GoogLeNet网络</li><li>ResNet网络</li><li><strong>SqueezeNet网络</strong></li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>SqueezeNet正如作者在论文题目中特别强调的那样，主要针对模型大小进行了深度优化，在保证模型精度没有太多损失的情况下，极大的缩小了所需模型的尺寸，为在嵌入式设备部署提供了强力支撑。SqueezeNet为我们裁剪模型和设计可在资源受限设备上运行的高性能模型提供了指导，特别是Fire Module的设计原理值得思考。</p><ul><li>论文：SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB  model size，2016.<ul><li>URL: <a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noopener">https://arxiv.org/abs/1602.07360</a></li></ul></li><li>代码链接：<a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="noopener">https://github.com/DeepScale/SqueezeNet</a></li><li>数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见<a href="/2018/07/06/ImageNet-DataSet/">ImageNet-DataSet</a>）</li><li>计算框架： MxNet</li><li>算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour)<ul><li>总计训练时间1天左右</li><li>每轮迭代时间：~1060秒</li><li>在ImageNet数据集上的效果：</li></ul></li></ul><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><ul><li>microarchitecture and macro architecture<ul><li>在设计深度网络架构的过程中，如果手动选择每一层的滤波器显得过于繁复。通常先构建由几个卷积层组成的小模块，再将模块堆叠形成完整的网络。定义这种模块的网络为CNN microarchitecture。</li><li>与模块相对应，定义完整的网络架构为CNN macroarchitecture。在完整的网络架构中，深度是一个重要的参数。</li></ul></li><li>比如FPGA中只有10MB的片上内存空间，没有片下存储，对模型大小要求较高；ASICs也会有相同的需求；</li><li>​</li></ul><h3 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h3><p>作者在论文中给出SqueezeNet的三条设计原则：</p><ul><li>尽量用1x1卷积替代3x3卷积，目的自然是降低参数量；</li><li>降低输入到3x3卷积对象的channel数目，在SqueezeNet中是使用squeeze模块来实现的；</li><li>尽量在网络后面层次中进行降采样，这样可以是卷积层获得更大的激活地图，从而获得更高的精度；</li></ul><h3 id="Fire-Module"><a href="#Fire-Module" class="headerlink" title="Fire Module"></a>Fire Module</h3><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/Fire-Module.PNG" alt="Fire-Module"></p><p>Fire Module是本文的核心构件，思想非常简单，就是将原来简单的一层conv层变成两层：squeeze层+expand层，各自带上Relu激活层。Fire Moduel主要包括squeeze模块和expand模块，其中squeeze模块全部由1x1卷积组成，而expand模块由1x1卷积和3x3卷积组成：</p><ul><li>squeeze模块中卷积核个数一定要小于expand模块中卷积核的个数，只有这样才能达到降参和压缩的目的；</li><li>squeeze中的1x1卷积起到了降维的效果；</li><li>为了实现expand模块中1x1卷积和3x3卷积之后的数据能够concatenate在一起，对于输入3x3的对象需要增加1的zero-padding操作；</li><li>在squeeze模块和expand模块之后通过ReLU进行激活；</li></ul><h3 id="模型架构及参数"><a href="#模型架构及参数" class="headerlink" title="模型架构及参数"></a>模型架构及参数</h3><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/architechtureofSqueezeNet.png" alt="architechtureofSqueezeNet"></p><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/parameters-in-SqueezeNet.PNG" alt="parameters-in-SqueezeNet"></p><p>如图所示，</p><ul><li>SqueezeNet中彻底放弃了全连接网络；</li><li>作者论文中初始学习率很大，为0.04，实际实验中，发现这么大的学习率震荡太厉害，降为0.01；</li><li>降维的操作尽量放在了模型后半段，为了满足第三条设计原则；</li><li>参数方面：使用了8个Fire Module，每个Fire Module中，squeeze的卷积核数目是expand的1/8；</li><li>fire9之后采用了dropout，其中keep_prob=0.5;</li></ul><h3 id="分析结果"><a href="#分析结果" class="headerlink" title="分析结果"></a>分析结果</h3><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/lab_results.png" alt="lab_results"></p><p>如图所示，SqueezeNet模型本身是AlexNet的1/50，却可以获得相近的准确率，通过利用Deep Compression技术可以进一步压缩模型，实现1/510模型大小情况下的相同性能（当然实际中，由于Deep Compression采取了编码策略，需要编码本，会带来一定的额外开销）。</p><h3 id="扩展阅读-Deep-Compression"><a href="#扩展阅读-Deep-Compression" class="headerlink" title="[扩展阅读]Deep Compression"></a>[扩展阅读]Deep Compression</h3><ul><li>Deep compression: Compressing DNNs with pruning, trained quantization and huffman coding， 2015</li><li>常见的模型压缩技术<ul><li>奇异值分解(singular value decomposition (SVD))<a href="https://blog.csdn.net/csdnldp/article/details/78648543#fn:1" target="_blank" rel="noopener">1</a> </li><li>网络剪枝（Network Pruning）<a href="https://blog.csdn.net/csdnldp/article/details/78648543#fn:2" target="_blank" rel="noopener">2</a>：使用网络剪枝和稀疏矩阵 </li><li>深度压缩（Deep compression）<a href="https://blog.csdn.net/csdnldp/article/details/78648543#fn:3" target="_blank" rel="noopener">3</a>：使用网络剪枝，数字化和huffman编码 </li><li>硬件加速器（hardware accelerator）<a href="https://blog.csdn.net/csdnldp/article/details/78648543#fn:4" target="_blank" rel="noopener">4</a></li></ul></li></ul><h2 id="重头训练一个SqueezeNet"><a href="#重头训练一个SqueezeNet" class="headerlink" title="重头训练一个SqueezeNet"></a>重头训练一个SqueezeNet</h2><ol><li>使用MxNet构建AlexNet代码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import the necessary packages</span></span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MxSqueezeNet</span>:</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squeeze</span><span class="params">(input, numFilter)</span>:</span></span><br><span class="line"><span class="comment"># the first part of a FIRE module consists of a number of 1x1</span></span><br><span class="line"><span class="comment"># filter squeezes on the input data followed by an activation</span></span><br><span class="line">squeeze_1x1 = mx.sym.Convolution(data=input, kernel=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">stride=(<span class="number">1</span>, <span class="number">1</span>), num_filter=numFilter)</span><br><span class="line">act_1x1 = mx.sym.LeakyReLU(data=squeeze_1x1,</span><br><span class="line">act_type=<span class="string">"elu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the activation for the squeeze</span></span><br><span class="line"><span class="keyword">return</span> act_1x1</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fire</span><span class="params">(input, numSqueezeFilter, numExpandFilter)</span>:</span></span><br><span class="line"><span class="comment"># construct the 1x1 squeeze followed by the 1x1 expand</span></span><br><span class="line">squeeze_1x1 = MxSqueezeNet.squeeze(input, numSqueezeFilter)</span><br><span class="line">expand_1x1 = mx.sym.Convolution(data=squeeze_1x1,</span><br><span class="line">kernel=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), num_filter=numExpandFilter)</span><br><span class="line">relu_expand_1x1 = mx.sym.LeakyReLU(data=expand_1x1,</span><br><span class="line">act_type=<span class="string">"elu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct the 3x3 expand</span></span><br><span class="line">expand_3x3 = mx.sym.Convolution(data=squeeze_1x1, pad=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), num_filter=numExpandFilter)</span><br><span class="line">relu_expand_3x3 = mx.sym.LeakyReLU(data=expand_3x3,</span><br><span class="line">act_type=<span class="string">"elu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the output of the FIRE module is the concatenation of the</span></span><br><span class="line"><span class="comment"># activation for the 1x1 and 3x3 expands along the channel</span></span><br><span class="line"><span class="comment"># dimension</span></span><br><span class="line">output = mx.sym.Concat(relu_expand_1x1, relu_expand_3x3,</span><br><span class="line">dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the output of the FIRE module</span></span><br><span class="line"><span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(classes)</span>:</span></span><br><span class="line"><span class="comment"># data input</span></span><br><span class="line">data = mx.sym.Variable(<span class="string">"data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #1: CONV =&gt; RELU =&gt; POOL</span></span><br><span class="line">conv_1 = mx.sym.Convolution(data=data, kernel=(<span class="number">7</span>, <span class="number">7</span>),</span><br><span class="line">stride=(<span class="number">2</span>, <span class="number">2</span>), num_filter=<span class="number">96</span>)</span><br><span class="line">relu_1 = mx.sym.LeakyReLU(data=conv_1, act_type=<span class="string">"elu"</span>)</span><br><span class="line">pool_1 = mx.sym.Pooling(data=relu_1, kernel=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">stride=(<span class="number">2</span>, <span class="number">2</span>), pool_type=<span class="string">"max"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #2-4: (FIRE * 3) =&gt; POOL</span></span><br><span class="line">fire_2 = MxSqueezeNet.fire(pool_1, numSqueezeFilter=<span class="number">16</span>,</span><br><span class="line">numExpandFilter=<span class="number">64</span>)</span><br><span class="line">fire_3 = MxSqueezeNet.fire(fire_2, numSqueezeFilter=<span class="number">16</span>,</span><br><span class="line">numExpandFilter=<span class="number">64</span>)</span><br><span class="line">fire_4 = MxSqueezeNet.fire(fire_3, numSqueezeFilter=<span class="number">32</span>,</span><br><span class="line"> numExpandFilter=<span class="number">128</span>)</span><br><span class="line">pool_4 = mx.sym.Pooling(data=fire_4, kernel=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">stride=(<span class="number">2</span>, <span class="number">2</span>), pool_type=<span class="string">"max"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #5-8: (FIRE * 4) =&gt; POOL</span></span><br><span class="line">fire_5 = MxSqueezeNet.fire(pool_4, numSqueezeFilter=<span class="number">32</span>,</span><br><span class="line">numExpandFilter=<span class="number">128</span>)</span><br><span class="line">fire_6 = MxSqueezeNet.fire(fire_5, numSqueezeFilter=<span class="number">48</span>,</span><br><span class="line">numExpandFilter=<span class="number">192</span>)</span><br><span class="line">fire_7 = MxSqueezeNet.fire(fire_6, numSqueezeFilter=<span class="number">48</span>,</span><br><span class="line">numExpandFilter=<span class="number">192</span>)</span><br><span class="line">fire_8 = MxSqueezeNet.fire(fire_7, numSqueezeFilter=<span class="number">64</span>,</span><br><span class="line">numExpandFilter=<span class="number">256</span>)</span><br><span class="line">pool_8 = mx.sym.Pooling(data=fire_8, kernel=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">stride=(<span class="number">2</span>, <span class="number">2</span>), pool_type=<span class="string">"max"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #9-10: FIRE =&gt; DROPOUT =&gt; CONV =&gt; RELU =&gt; POOL</span></span><br><span class="line">fire_9 = MxSqueezeNet.fire(pool_8, numSqueezeFilter=<span class="number">64</span>,</span><br><span class="line">numExpandFilter=<span class="number">256</span>)</span><br><span class="line">do_9 = mx.sym.Dropout(data=fire_9, p=<span class="number">0.5</span>)</span><br><span class="line">conv_10 = mx.sym.Convolution(data=do_9, num_filter=classes,</span><br><span class="line">kernel=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">relu_10 = mx.sym.LeakyReLU(data=conv_10, act_type=<span class="string">"elu"</span>)</span><br><span class="line">pool_10 = mx.sym.Pooling(data=relu_10, kernel=(<span class="number">13</span>, <span class="number">13</span>),</span><br><span class="line">pool_type=<span class="string">"avg"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax classifier</span></span><br><span class="line">flatten = mx.sym.Flatten(data=pool_10)</span><br><span class="line">model = mx.sym.SoftmaxOutput(data=flatten, name=<span class="string">"softmax"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the network architecture</span></span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><blockquote><p><strong>几点注意点：</strong></p><ol><li>使用ELU替代了ReLU激活函数；在ImageNet数据集中，ELU表现效果由于ReLU；</li><li>3x3 expand中有个pad=（1，1）</li><li>最有使用的全局平均池化层，kernel=（13，13）</li><li>全局池化输出被flatten之后直接丢给softmax</li></ol></blockquote><h3 id="SqueezeNet的TensorFLow实现"><a href="#SqueezeNet的TensorFLow实现" class="headerlink" title="SqueezeNet的TensorFLow实现"></a>SqueezeNet的TensorFLow实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqueezeNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, nb_classes=<span class="number">1000</span>, is_training=True)</span>:</span></span><br><span class="line">        <span class="comment"># conv1</span></span><br><span class="line">        net = tf.layers.conv2d(inputs, <span class="number">96</span>, [<span class="number">7</span>, <span class="number">7</span>], strides=[<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                                 padding=<span class="string">"SAME"</span>, activation=tf.nn.relu,</span><br><span class="line">                                 name=<span class="string">"conv1"</span>)</span><br><span class="line">        <span class="comment"># maxpool1</span></span><br><span class="line">        net = tf.layers.max_pooling2d(net, [<span class="number">3</span>, <span class="number">3</span>], strides=[<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                                      name=<span class="string">"maxpool1"</span>)</span><br><span class="line">        <span class="comment"># fire2</span></span><br><span class="line">        net = self._fire(net, <span class="number">16</span>, <span class="number">64</span>, <span class="string">"fire2"</span>)</span><br><span class="line">        <span class="comment"># fire3</span></span><br><span class="line">        net = self._fire(net, <span class="number">16</span>, <span class="number">64</span>, <span class="string">"fire3"</span>)</span><br><span class="line">        <span class="comment"># fire4</span></span><br><span class="line">        net = self._fire(net, <span class="number">32</span>, <span class="number">128</span>, <span class="string">"fire4"</span>)</span><br><span class="line">        <span class="comment"># maxpool4</span></span><br><span class="line">        net = tf.layers.max_pooling2d(net, [<span class="number">3</span>, <span class="number">3</span>], strides=[<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                                      name=<span class="string">"maxpool4"</span>)</span><br><span class="line">        <span class="comment"># fire5</span></span><br><span class="line">        net = self._fire(net, <span class="number">32</span>, <span class="number">128</span>, <span class="string">"fire5"</span>)</span><br><span class="line">        <span class="comment"># fire6</span></span><br><span class="line">        net = self._fire(net, <span class="number">48</span>, <span class="number">192</span>, <span class="string">"fire6"</span>)</span><br><span class="line">        <span class="comment"># fire7</span></span><br><span class="line">        net = self._fire(net, <span class="number">48</span>, <span class="number">192</span>, <span class="string">"fire7"</span>)</span><br><span class="line">        <span class="comment"># fire8</span></span><br><span class="line">        net = self._fire(net, <span class="number">64</span>, <span class="number">256</span>, <span class="string">"fire8"</span>)</span><br><span class="line">        <span class="comment"># maxpool8</span></span><br><span class="line">        net = tf.layers.max_pooling2d(net, [<span class="number">3</span>, <span class="number">3</span>], strides=[<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                                      name=<span class="string">"maxpool8"</span>)</span><br><span class="line">        <span class="comment"># fire9</span></span><br><span class="line">        net = self._fire(net, <span class="number">64</span>, <span class="number">256</span>, <span class="string">"fire9"</span>)</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        net = tf.layers.dropout(net, <span class="number">0.5</span>, training=is_training)</span><br><span class="line">        <span class="comment"># conv10</span></span><br><span class="line">        net = tf.layers.conv2d(net, <span class="number">1000</span>, [<span class="number">1</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                               padding=<span class="string">"SAME"</span>, activation=tf.nn.relu,</span><br><span class="line">                               name=<span class="string">"conv10"</span>)</span><br><span class="line">        <span class="comment"># avgpool10</span></span><br><span class="line">        net = tf.layers.average_pooling2d(net, [<span class="number">13</span>, <span class="number">13</span>], strides=[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                          name=<span class="string">"avgpool10"</span>)</span><br><span class="line">        <span class="comment"># squeeze the axis</span></span><br><span class="line">        net = tf.squeeze(net, axis=[<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        self.logits = net</span><br><span class="line">        self.prediction = tf.nn.softmax(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fire</span><span class="params">(self, inputs, squeeze_depth, expand_depth, scope)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            squeeze = tf.layers.conv2d(inputs, squeeze_depth, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                       strides=[<span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>,</span><br><span class="line">                                       activation=tf.nn.relu, name=<span class="string">"squeeze"</span>)</span><br><span class="line">            <span class="comment"># squeeze</span></span><br><span class="line">            expand_1x1 = tf.layers.conv2d(squeeze, expand_depth, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                          strides=[<span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>,</span><br><span class="line">                                          activation=tf.nn.relu, name=<span class="string">"expand_1x1"</span>)</span><br><span class="line">            expand_3x3 = tf.layers.conv2d(squeeze, expand_depth, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                          strides=[<span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>,</span><br><span class="line">                                          activation=tf.nn.relu, name=<span class="string">"expand_3x3"</span>)</span><br><span class="line">            <span class="keyword">return</span> tf.concat([expand_1x1, expand_3x3], axis=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h3 id="训练SqueezeNet"><a href="#训练SqueezeNet" class="headerlink" title="训练SqueezeNet"></a>训练SqueezeNet</h3><p>训练用的脚本和前面几个网络一致。</p><blockquote><p><strong>几个注意点：</strong></p><ol><li><code>batchSize = config.BATCH_SIZE * config.NUM_DEVICES</code>; 使用k80 12GB先存，选择batchSzie=128；</li><li>优化算法使用SGD，初始学习率为1e-2，动量0.9，L2权重正则化参数0.0002；rescale参数尤为关键，根据批的大小放大梯度：rescale_grad=1.0 / batchSize；</li><li>model中的<code>ctx</code>参数用于指定用于训练的GPU；</li><li>使用了Xavier进行参数初始化，与原模型略有不同，Xavier是目前CNN网络常采用的参数初始化方式；initializer=mx.initializer.Xavier()；</li><li>In Keras, the ELU activation uses a default α value of 1.0. • But in mxnet, the ELU α value defaults to 0.25.</li></ol></blockquote><h4 id="学习率的控制"><a href="#学习率的控制" class="headerlink" title="学习率的控制"></a>学习率的控制</h4><div class="table-container"><table><thead><tr><th>Epoch</th><th>学习率</th></tr></thead><tbody><tr><td>1-64</td><td>1e-2</td></tr><tr><td>65-80</td><td>1e-3</td></tr><tr><td>81-89</td><td>1e-4</td></tr><tr><td>90-100</td><td>1e-5</td></tr></tbody></table></div><blockquote><ol><li>控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率；</li><li>调整学习率 <code>python train_alexnet.py --checkpoints checkpoints --prefix alexnet \ --start-epoch 50</code></li><li>在8个GPU上，每轮用时1000多秒；</li></ol></blockquote><ol><li><p>第一遍训练采用1e-2的学习率训练75轮，发现65轮以后，验证集准确率已经不再增加；为此在65轮之后调整学习率为1e-3；</p><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/0-86-accuracy.png" alt="0-86-accuracy"></p><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/0-86-loss.png" alt="0-86-loss"></p></li><li><p>将学习率调整到1e-3之后，验证集准确率有大幅提升，代表调整有效，80轮之后再度饱和，降低学习率到1e-4;到89轮验证集结果如下：</p><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/0-89-accuracy.png" alt="0-89-accuracy"></p><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/0-89-loss.png" alt="0-89-loss"></p><p>​</p></li><li><p>进一步降低学习率到1e-5，发现整个网络没有性能提升，结束该批次参数训练过程。</p></li></ol><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol><li>初始学习率的选择</li></ol><p>   <img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/lr=0.04.PNG" alt="lr=0.04"></p><ol><li><p>BN的效果</p><p>在SqueezeNet中没有效果</p></li><li><p>ReLU vs ELU ？</p><p>用ELU替代ReLU，提升1-2%的性能；</p></li><li><p>选取90轮的训练结果，在测试集数据上进行验证，结果如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-1</span>: 55<span class="selector-class">.44</span>%</span><br><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-5</span>: 78<span class="selector-class">.10</span>%</span><br></pre></td></tr></table></figure></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>今天终于完成了所有预定神经网络的训练过程，虽然有些网络由于本身对网络结构的认知或者是实际训练过程中没有很好的把握节奏，导致训练效果不是十分理想，但从这次的训练过程中还是学到了很多东西。整个过程大概持续了半个月左右的时间，得益于AWS上8 GPU的资源，使得很多网络的训练比原想进度要快很多，但费用也是惊人，看了下账单，发生在虚拟机上的费用大概有1万3千多，主要由于8 GPU服务器要85元人民币每个小时的费用不是所有人都可以承受的起的。所以我一直认为短期内，深度学习还是很难真正产业化的一个泡沫，很难有应用业务可以抵消这个昂贵的训练成本。我这只是复现网络，真实训练一个产品级的网络、所需要测试的参数十倍百倍于此。</p><p><img src="/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/cost_AWS.png" alt="cost_AWS"></p><p>付了昂贵的学费，学习的动力也更加充足，将几种网络的训练过程进行一下简单的横向比较，同时总结一下大型网络训练过程中注意的主要内容。</p><h3 id="几种网络在训练过程的简单比较"><a href="#几种网络在训练过程的简单比较" class="headerlink" title="几种网络在训练过程的简单比较"></a>几种网络在训练过程的简单比较</h3><div class="table-container"><table><thead><tr><th>网络名称</th><th>模型大小</th><th>每轮训练时间（8GPU Tesla K80）</th><th>总迭代数+训练时间</th><th>初始化参数选择</th><th>初始学习率</th><th>激活函数</th><th>最终测试集准确率（Top-1/Top-5）</th></tr></thead><tbody><tr><td>AlexNet</td><td>239MB</td><td>~670s</td><td>~1.5天</td><td>Xavier</td><td>1e-2,动量0.9，L2 0.0005</td><td>ELU</td><td>60.20%/81.99%</td></tr><tr><td>VGG-16</td><td>529MB</td><td>~</td><td>~10天</td><td>MSRA</td><td>1e-2,动量0.9，L2 0.0005</td><td>PReLU</td><td></td></tr><tr><td>GoogLeNet</td><td>28MB</td><td>~1200s</td><td>~2天</td><td>Xavier</td><td>1e-3，Adam，L2 0.0002</td><td>ReLU</td><td>69.58%/89.01%</td></tr><tr><td>ResNet</td><td>98MB</td><td>~3500s</td><td>~3天</td><td>MSRA</td><td>1e-1,动量0.9，L2 0.0001</td><td>ReLU</td><td>71.49%/89.96%</td></tr><tr><td>SqueezeNet</td><td>5MB</td><td>~1060s</td><td>~1.5天</td><td>Xavier</td><td>1e-2，动量0.9，L2 0.0002</td><td>ELU</td><td>55.44%/78.10%</td></tr></tbody></table></div><h3 id="训练心得"><a href="#训练心得" class="headerlink" title="训练心得"></a>训练心得</h3><ol><li>训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源；</li><li><strong>原则：</strong>训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型；</li><li>网络参数如何入手？<ol><li>逆机器学习流程的思考：在机器学习算法的应用逻辑里，我们总是强调要快速的开始搭建第一个模型，设定测试的基准，然后再迭代更新模型。而训练大型神经网络，由于训练过程对时间和资源消耗十分巨大，为充分利用资源，切不可盲目起步；</li><li>由于可调参数终端，所以必须要想办法缩小可调参数范围，优先选择重要参数进行调试，同时通过查阅相似数据集文献中使用的参数来进一步缩小自己调试参数的范围；</li></ol></li><li>初始学习率选择：<ol><li>学习率参数是整个参数空间中最重要的一个，如果你只想调试一个参数，那么请选择它；</li><li>初始学习率需要根据网络模型的特点来决定，一般而言1e-2可能是一个合适的学习率，但不是对所有的网络都是最佳的学习率，比如ResNet支持更大的学习率，比如1e-1,GoogLeNet而言，1e-2有点太大，验证集准确率会发生比较大的震荡；</li></ol></li><li>学习率控制：<ol><li>初始学习率选择好以后，需要在第一次迭代中让网络多运行一段时间，观察验证集上的表现决定是否变更学习率，一般当验证集误差出现阻塞或者出现明显过拟合现象时，需要停止训练，调整学习率，从打断位置或前面位置继续训练；</li><li>学习率手段衰减一般采取对数衰减的策略，比如每次衰减十倍；</li></ol></li><li>优化算法选择：<ol><li>关于优化算法的选择，一般先从尝试经典的SGD算法、配合动量设置开始训练看网络是否能够有效学习参数空间信息；</li><li>进一步的可以调整为Adam来加快梯度的更新；</li><li>在大型网络训练中还经常使用梯度的rescale操作，根据batchsize大小，放大梯度：rescale_grad=1.0 / batchSize；</li></ol></li><li>激活函数选择：先使用ReLU作为激活函数获得baseline，再选择提花为ELU来获得提升；</li><li>初始化参数选择：训练深层网络时，考虑使用Xavier,MSRA/HE的初始化参数，并配合PReLU一起使用，效果更好。</li><li>训练过程的控制：<ol><li>训练过程要利用callback机制，随时记录训练参数、checkpoints；</li><li>通过Tensorboard等工具观察训练过程的精度、误差、损失函数变化情况；</li><li>有些网络需要预训练过程，通过预训练可以加速网络学习；</li></ol></li><li>参数调优的其它技巧：<ol><li>BN</li><li>DropOut</li><li>Data Augmentation</li><li>计算框架：TensorFlow or MxNet</li><li>论文阅读与讨论</li><li>​…</li></ol></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/images/2018-07-12-经典网络复现之SqueezeNet/Fire-Module.PNG&quot; alt=&quot;Fire-Module&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 本文记录利用ImageNet数据集复现经典网络SqueezeNet的过程，并记录在大型数据集训练过程中需要考虑的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="动手实践营" scheme="http://blog.a-stack.com/categories/%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E8%90%A5/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="网络复现" scheme="http://blog.a-stack.com/tags/%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>经典网络复现之GoogleNet</title>
    <link href="http://blog.a-stack.com/2018/07/12/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0%E4%B9%8BGoogleNet/"/>
    <id>http://blog.a-stack.com/2018/07/12/经典网络复现之GoogleNet/</id>
    <published>2018-07-12T05:58:12.000Z</published>
    <updated>2018-07-20T15:52:31.842Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/qnsource/images/2018-07-10-经典网络复现之GoogleNet/inception.jpg" alt="inception"></p><p><strong>摘要：</strong> 本文记录利用ImageNet数据集复现经典网络GoogLeNet的过程，并记录在大型数据集训练过程中需要考虑的问题。</p><a id="more"></a><p>为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容：</p><ul><li>ImageNet数据集及数据准备</li><li>AlexNet网络</li><li>VGG网络</li><li><strong>GoogLeNet网络</strong></li><li>ResNet网络</li><li>SqueezeNet网络</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>GoogleNet作为2012年ILSVRC的冠军，最近几年在原先Inception v1版本上也在不断的演化发展，目前已经到了v4版本。GoogleNet模型设计在保证准确率的同时大大降低了模型的大小，与VGG接近500MB相比，只有28MB左右的模型大小。</p><ul><li>论文：<ul><li>Christian Szegedy et al. “Going Deeper with Convolutions”. In: Computer Vision and Pattern Recognition (CVPR). 2015. URL: <a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">http://arxiv.org/abs/1409.4842</a> </li></ul></li><li>数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见<a href="/2018/07/06/ImageNet-DataSet/">ImageNet-DataSet</a>）</li><li>计算框架： MxNet</li><li>算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour)<ul><li>总计训练时间2天左右</li><li>每轮迭代时间：~1200秒(~20分钟)</li></ul></li></ul><h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p>GoogleNet的主要创新点体现在：</p><p>一、打破了常规的卷积层串联的模式，提出了将1x1，3x3，5x5的卷积层和3x3的pooling池化层并联组合后concatenate组装在一起的设计思路。</p><p>二、InceptionV1降低了参数量，目的有两个：1 参数越多模型越大，需要提供模型学习的数据量就越大，数据量不大的情况下容易过拟合；2 参数越多，耗费的计算资源也会更大。</p><p>三、去除了最后的全连接层，用全局平均池化层来取代它。全连接层几乎占据了AlexNet或VGGNet中90%的参数量，而且会引起过拟合，去除全连接层后模型训练更快并且减轻了过拟合。用全局平均池化层取代全连接层的做法借鉴了NetworkIn Network（以下简称NIN）论文。</p><p>四、InceptionV1中精心设计的InceptionModule提高了参数的利用效率。这一部分也借鉴了NIN的思想，形象的解释就是InceptionModule本身如同大网络中的一个小网络，其结构可以反复堆叠在一起形成大网络。InceptionV1比NIN更进一步的是增加了分支网络，NIN则主要是级联的卷积层和MLPConv层。一般来说卷积层要提升表达能力，主要依靠增加输出通道数，但副作用是计算量增大和过拟合。每一个输出通道对应一个滤波器，同一个滤波器共享参数，只能提取一类特征，因此一个输出通道只能做一种特征处理。而NIN中的MLPConv则拥有更强大的能力，允许在输出通道之间组合信息，因此效果明显。可以说，MLPConv基本等效于普通卷积层后再连接1*1的卷积和ReLU激活函数。</p><h2 id="重头训练一个GoogLeNet"><a href="#重头训练一个GoogLeNet" class="headerlink" title="重头训练一个GoogLeNet"></a>重头训练一个GoogLeNet</h2><p><img src="/qnsource/images/2018-07-10-经典网络复现之GoogleNet/parameters.png" alt="parameters"></p><ol><li>使用MxNet构建GoogleNet代码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MxGoogLeNet</span>:</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_module</span><span class="params">(data, K, kX, kY, pad=<span class="params">(<span class="number">0</span>, <span class="number">0</span>)</span>, stride=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span>:</span></span><br><span class="line"><span class="comment"># define the CONV =&gt; BN =&gt; RELU pattern</span></span><br><span class="line">conv = mx.sym.Convolution(data=data, kernel=(kX, kY),</span><br><span class="line">num_filter=K, pad=pad, stride=stride)</span><br><span class="line"><span class="comment">#bn = mx.sym.BatchNorm(data=conv)</span></span><br><span class="line">act = mx.sym.Activation(data=conv, act_type=<span class="string">"relu"</span>)</span><br><span class="line">bn = mx.sym.BatchNorm(data=act)</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the block</span></span><br><span class="line"><span class="keyword">return</span> bn</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_module</span><span class="params">(data, num1x1, num3x3Reduce, num3x3,</span></span></span><br><span class="line"><span class="function"><span class="params">num5x5Reduce, num5x5, num1x1Proj)</span>:</span></span><br><span class="line"><span class="comment"># the first branch of the Inception module consists of 1x1</span></span><br><span class="line"><span class="comment"># convolutions</span></span><br><span class="line">conv_1x1 = MxGoogLeNet.conv_module(data, num1x1, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the second branch of the Inception module is a set of 1x1</span></span><br><span class="line"><span class="comment"># convolutions followed by 3x3 convolutions</span></span><br><span class="line">conv_r3x3 = MxGoogLeNet.conv_module(data, num3x3Reduce, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">conv_3x3 = MxGoogLeNet.conv_module(conv_r3x3, num3x3, <span class="number">3</span>, <span class="number">3</span>,</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the third branch of the Inception module is a set of 1x1</span></span><br><span class="line"><span class="comment"># convolutions followed by 5x5 convolutions</span></span><br><span class="line">conv_r5x5 = MxGoogLeNet.conv_module(data, num5x5Reduce, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">conv_5x5 = MxGoogLeNet.conv_module(conv_r5x5, num5x5, <span class="number">5</span>, <span class="number">5</span>,</span><br><span class="line">pad=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the final branch of the Inception module is the POOL +</span></span><br><span class="line"><span class="comment"># projection layer set</span></span><br><span class="line">pool = mx.sym.Pooling(data=data, pool_type=<span class="string">"max"</span>, pad=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">conv_proj = MxGoogLeNet.conv_module(pool, num1x1Proj, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># concatenate the filters across the channel dimension</span></span><br><span class="line">concat = mx.sym.Concat(*[conv_1x1, conv_3x3, conv_5x5,</span><br><span class="line">conv_proj])</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the block</span></span><br><span class="line"><span class="keyword">return</span> concat</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(classes)</span>:</span></span><br><span class="line"><span class="comment"># data input</span></span><br><span class="line">data = mx.sym.Variable(<span class="string">"data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #1: CONV =&gt; POOL =&gt; CONV =&gt; CONV =&gt; POOL</span></span><br><span class="line">conv1_1 = MxGoogLeNet.conv_module(data, <span class="number">64</span>, <span class="number">7</span>, <span class="number">7</span>,</span><br><span class="line">pad=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">pool1 = mx.sym.Pooling(data=conv1_1, pool_type=<span class="string">"max"</span>,</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>), kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">conv1_2 = MxGoogLeNet.conv_module(pool1, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">conv1_3 = MxGoogLeNet.conv_module(conv1_2, <span class="number">192</span>, <span class="number">3</span>, <span class="number">3</span>,</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">pool2 = mx.sym.Pooling(data=conv1_3, pool_type=<span class="string">"max"</span>,</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>), kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #3: (INCEP * 2) =&gt; POOL</span></span><br><span class="line">in3a = MxGoogLeNet.inception_module(pool2, <span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>,</span><br><span class="line"><span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">in3b = MxGoogLeNet.inception_module(in3a, <span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>,</span><br><span class="line"><span class="number">96</span>, <span class="number">64</span>)</span><br><span class="line">pool3 = mx.sym.Pooling(data=in3b, pool_type=<span class="string">"max"</span>,</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>), kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #4: (INCEP * 5) =&gt; POOL</span></span><br><span class="line">in4a = MxGoogLeNet.inception_module(pool3, <span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>,</span><br><span class="line"><span class="number">48</span>, <span class="number">64</span>)</span><br><span class="line">in4b = MxGoogLeNet.inception_module(in4a, <span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>,</span><br><span class="line"><span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">in4c = MxGoogLeNet.inception_module(in4b, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>,</span><br><span class="line"><span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">in4d = MxGoogLeNet.inception_module(in4c, <span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>,</span><br><span class="line"><span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">in4e = MxGoogLeNet.inception_module(in4d, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>,</span><br><span class="line"><span class="number">128</span>, <span class="number">128</span>,)</span><br><span class="line">pool4 = mx.sym.Pooling(data=in4e, pool_type=<span class="string">"max"</span>,</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>), kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #5: (INCEP * 2) =&gt; POOL =&gt; DROPOUT</span></span><br><span class="line">in5a = MxGoogLeNet.inception_module(pool4, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>,</span><br><span class="line"><span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">in5b = MxGoogLeNet.inception_module(in5a, <span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>,</span><br><span class="line"><span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">pool5 = mx.sym.Pooling(data=in5b, pool_type=<span class="string">"avg"</span>,</span><br><span class="line">kernel=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">do = mx.sym.Dropout(data=pool5, p=<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax classifier</span></span><br><span class="line">flatten = mx.sym.Flatten(data=do)</span><br><span class="line">fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=classes)</span><br><span class="line">model = mx.sym.SoftmaxOutput(data=fc1, name=<span class="string">"softmax"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the network architecture</span></span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><blockquote><p><strong>几点注意点：</strong></p><ol><li>与原网络不同，我们将BN放在了activation之后，提升了模型效果；</li><li>Dropout原网络使用40%，当然现在更常见的是使用50%；</li><li>​Inception Module中几个分支网络通过concat连接在一起，这点与ResNet的本质区别；</li></ol></blockquote><h3 id="训练GoogLeNet"><a href="#训练GoogLeNet" class="headerlink" title="训练GoogLeNet"></a>训练GoogLeNet</h3><p>训练用的脚本和前面几个网络一致。</p><blockquote><p><strong>几个注意点：</strong></p><ol><li><code>batchSize = config.BATCH_SIZE * config.NUM_DEVICES</code>; 使用k80 12GB先存，选择batchSzie=<strong>128</strong>；</li><li>优化算法使用Adam，初始学习率为1e-3，L2权重正则化参数0.0002；rescale参数尤为关键，根据批的大小放大梯度：rescale_grad=1.0 / batchSize；（使用SGD训练效果一般）</li><li>使用了Xavier进行参数初始化，initializer=mx.initializer.Xavier(),；</li></ol></blockquote><h4 id="学习率的控制"><a href="#学习率的控制" class="headerlink" title="学习率的控制"></a>学习率的控制</h4><blockquote><p>训练ResNet由于初始学习率高，所以最终完成训练所需的迭代数目要小很多；</p></blockquote><div class="table-container"><table><thead><tr><th>Epoch</th><th>学习率</th></tr></thead><tbody><tr><td>1-31</td><td>1e-3</td></tr><tr><td>32-40</td><td>1e-4</td></tr><tr><td>41-47</td><td>1e-5</td></tr></tbody></table></div><blockquote><ol><li>控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率；</li><li>调整学习率 <code>python train_alexnet.py --checkpoints checkpoints --prefix alexnet \ --start-epoch 50</code></li><li>在8个GPU上，每轮用时1200多秒；</li></ol></blockquote><ol><li><p>第一遍训练采用1e-3的学习率配合Adam算法训练31轮，发现饱和现象，降低学习率到1e-4，继续训练到42轮；</p><p>​</p><p><img src="/qnsource/images/2018-07-10-经典网络复现之GoogleNet/42epoch-accuracy.png" alt="42epoch-accuracy"></p><p><img src="/qnsource/images/2018-07-10-经典网络复现之GoogleNet/42epoch-loss.png" alt="42epoch-loss"></p></li><li><p>发现40轮之后就已经出现了验证准确率的饱和现象，在40轮之后调整学习率为1e-5继续训练几轮完成：</p><p><img src="/qnsource/images/2018-07-10-经典网络复现之GoogleNet/47epoch-accuracy.png" alt="47epoch-accuracy"></p><p><img src="/qnsource/images/2018-07-10-经典网络复现之GoogleNet/47epoch-loss.png" alt="47epoch-loss"></p></li></ol><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol><li><p>Adam or SGD</p><p>如果使用SGD + 1e-2的经典开始配合，在GoogleNet上效果并不好，抖动剧烈，虽然这是作者推荐的参数，如下：</p><p><img src="/qnsource/images/2018-07-10-经典网络复现之GoogleNet/1e-2lr.png" alt="1e-2lr"></p><p>首先修改初始学习率为1e-3，尽管抖动降低了，但学习太慢，效果也不佳，为此进一步的调整了优化算法为Adam算法。</p></li><li><p>使用BN在激活函数之后</p><p>取得了1%以上的性能提升</p></li></ol><ol><li>选取40轮的训练结果，在测试集数据上进行验证，结果如下：</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-1</span>: 69<span class="selector-class">.58</span>%</span><br><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-5</span>: 89<span class="selector-class">.01</span>%</span><br></pre></td></tr></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>GoogleNet很难复现作者的效果，实际上还没有VGG的效果好，可能是作者论文中某些参数没有有效标注出来；</li><li>训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源；</li><li>训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型；</li><li>先使用ReLU作为激活函数获得baseline，再选择提花为ELU来获得提升；</li><li>训练深层网络时，考虑使用MSRA/HE的初始化参数，并配合PReLU一起使用，效果更好；</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/qnsource/images/2018-07-10-经典网络复现之GoogleNet/inception.jpg&quot; alt=&quot;inception&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 本文记录利用ImageNet数据集复现经典网络GoogLeNet的过程，并记录在大型数据集训练过程中需要考虑的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="动手实践营" scheme="http://blog.a-stack.com/categories/%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E8%90%A5/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="网络复现" scheme="http://blog.a-stack.com/tags/%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>经典网络复现之ResNet</title>
    <link href="http://blog.a-stack.com/2018/07/12/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0%E4%B9%8BResNet/"/>
    <id>http://blog.a-stack.com/2018/07/12/经典网络复现之ResNet/</id>
    <published>2018-07-12T05:58:12.000Z</published>
    <updated>2018-07-20T15:52:21.624Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://p4ygzcmtw.bkt.clouddn.com/images/2018-05-21-ResNet/residual-Module-2.PNG" alt="Residual Module"></p><p><strong>摘要：</strong> 本文记录利用ImageNet数据集复现经典网络ResNet的过程，并记录在大型数据集训练过程中需要考虑的问题。</p><a id="more"></a><p>为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容：</p><ul><li>ImageNet数据集及数据准备</li><li>AlexNet网络</li><li>VGG网络</li><li>GoogLeNet网络</li><li><strong>ResNet网络</strong></li><li>SqueezeNet网络</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>ResNet在整个深度神经网络发展过程中国起到了里程碑式的意义，残差模块的提出为训练数千层的神经网络提供了方法，后续许多网络也纷纷在其网络架构中增加残差模块，力图提升训练效率。</p><ul><li>论文：<ul><li><a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a></li><li><a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">Identity Mappings in Deep Residual Networks</a></li></ul></li><li>数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见<a href="/2018/07/06/ImageNet-DataSet/">ImageNet-DataSet</a>）</li><li>计算框架： MxNet</li><li>算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour)<ul><li>总计训练时间3天左右</li><li>每轮迭代时间：~3500秒(~1小时)</li></ul></li></ul><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>由于之前准备写过一篇博客来分析ResNet，再次不再展开说明了，详细内容参见：<a href="/2018/05/21/ResNet/">经典网络归纳： ResNet</a></p><h2 id="重头训练一个ResNet"><a href="#重头训练一个ResNet" class="headerlink" title="重头训练一个ResNet"></a>重头训练一个ResNet</h2><p>我们训练的标的是ResNet50，即50层的残差网络，50代表了整个网络中所有含参数层的数目：</p><p>1 + （3x3） + (4x3) + (6x3) + (3x3) + 1 = 50</p><p>第一个卷积层使用7x7的卷积核，用于快速降低网络尺寸，配合紧接着的池化层，将输入224x224的尺寸降低到56x56。</p><p><img src="/qnsource/images/2018-07-10-经典网络复现之ResNet/ResNet—Architecture.png" alt="ResNet—Architecture"></p><ol><li>使用MxNet构建ResNet代码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MxResNet</span>:</span></span><br><span class="line"><span class="comment"># uses "bottleneck" module with pre-activation (He et al. 2016)</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">residual_module</span><span class="params">(data, K, stride, red=False, bnEps=<span class="number">2e-5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">bnMom=<span class="number">0.9</span>)</span>:</span></span><br><span class="line"><span class="comment"># the shortcut branch of the ResNet module should be</span></span><br><span class="line"><span class="comment"># initialized as the input (identity) data</span></span><br><span class="line">shortcut = data</span><br><span class="line"></span><br><span class="line"><span class="comment"># the first block of the ResNet module are 1x1 CONVs</span></span><br><span class="line">bn1 = mx.sym.BatchNorm(data=data, fix_gamma=<span class="keyword">False</span>,</span><br><span class="line">eps=bnEps, momentum=bnMom)</span><br><span class="line">act1 = mx.sym.Activation(data=bn1, act_type=<span class="string">"relu"</span>)</span><br><span class="line">conv1 = mx.sym.Convolution(data=act1, pad=(<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">kernel=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), num_filter=int(K * <span class="number">0.25</span>),</span><br><span class="line">no_bias=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the second block of the ResNet module are 3x3 CONVs</span></span><br><span class="line">bn2 = mx.sym.BatchNorm(data=conv1, fix_gamma=<span class="keyword">False</span>,</span><br><span class="line">eps=bnEps, momentum=bnMom)</span><br><span class="line">act2 = mx.sym.Activation(data=bn2, act_type=<span class="string">"relu"</span>)</span><br><span class="line">conv2 = mx.sym.Convolution(data=act2, pad=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=stride, num_filter=int(K * <span class="number">0.25</span>),</span><br><span class="line">no_bias=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the third block of the ResNet module is another set of 1x1</span></span><br><span class="line"><span class="comment"># CONVs</span></span><br><span class="line">bn3 = mx.sym.BatchNorm(data=conv2, fix_gamma=<span class="keyword">False</span>,</span><br><span class="line">eps=bnEps, momentum=bnMom)</span><br><span class="line">act3 = mx.sym.Activation(data=bn3, act_type=<span class="string">"relu"</span>)</span><br><span class="line">conv3 = mx.sym.Convolution(data=act3, pad=(<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">kernel=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), num_filter=K, no_bias=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># if we are to reduce the spatial size, apply a CONV layer</span></span><br><span class="line"><span class="comment"># to the shortcut</span></span><br><span class="line"><span class="keyword">if</span> red:</span><br><span class="line">shortcut = mx.sym.Convolution(data=act1, pad=(<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">kernel=(<span class="number">1</span>, <span class="number">1</span>), stride=stride, num_filter=K,</span><br><span class="line">no_bias=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add together the shortcut and the final CONV</span></span><br><span class="line">add = conv3 + shortcut</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the addition as the output of the ResNet module</span></span><br><span class="line"><span class="keyword">return</span> add</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(classes, stages, filters, bnEps=<span class="number">2e-5</span>, bnMom=<span class="number">0.9</span>)</span>:</span></span><br><span class="line"><span class="comment"># data input</span></span><br><span class="line">data = mx.sym.Variable(<span class="string">"data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #1: BN =&gt; CONV =&gt; ACT =&gt; POOL, then initialize the</span></span><br><span class="line"><span class="comment"># "body" of the network</span></span><br><span class="line">bn1_1 = mx.sym.BatchNorm(data=data, fix_gamma=<span class="keyword">True</span>,</span><br><span class="line">eps=bnEps, momentum=bnMom)</span><br><span class="line">conv1_1 = mx.sym.Convolution(data=bn1_1, pad=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">kernel=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), num_filter=filters[<span class="number">0</span>],</span><br><span class="line">no_bias=<span class="keyword">True</span>)</span><br><span class="line">bn1_2 = mx.sym.BatchNorm(data=conv1_1, fix_gamma=<span class="keyword">False</span>,</span><br><span class="line">eps=bnEps, momentum=bnMom)</span><br><span class="line">act1_2 = mx.sym.Activation(data=bn1_2, act_type=<span class="string">"relu"</span>)</span><br><span class="line">pool1 = mx.sym.Pooling(data=act1_2, pool_type=<span class="string">"max"</span>,</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>), kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">body = pool1</span><br><span class="line"></span><br><span class="line"><span class="comment"># loop over the number of stages</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(stages)):</span><br><span class="line"><span class="comment"># initialize the stride, then apply a residual module</span></span><br><span class="line"><span class="comment"># used to reduce the spatial size of the input volume</span></span><br><span class="line">stride = (<span class="number">1</span>, <span class="number">1</span>) <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">body = MxResNet.residual_module(body, filters[i + <span class="number">1</span>],</span><br><span class="line">stride, red=<span class="keyword">True</span>, bnEps=bnEps, bnMom=bnMom)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loop over the number of layers in the stage</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, stages[i] - <span class="number">1</span>):</span><br><span class="line"><span class="comment"># apply a ResNet module</span></span><br><span class="line">body = MxResNet.residual_module(body, filters[i + <span class="number">1</span>],</span><br><span class="line">(<span class="number">1</span>, <span class="number">1</span>), bnEps=bnEps, bnMom=bnMom)</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply BN =&gt; ACT =&gt; POOL</span></span><br><span class="line">bn2_1 = mx.sym.BatchNorm(data=body, fix_gamma=<span class="keyword">False</span>,</span><br><span class="line">eps=bnEps, momentum=bnMom)</span><br><span class="line">act2_1 = mx.sym.Activation(data=bn2_1, act_type=<span class="string">"relu"</span>)</span><br><span class="line">pool2 = mx.sym.Pooling(data=act2_1, pool_type=<span class="string">"avg"</span>,</span><br><span class="line">global_pool=<span class="keyword">True</span>, kernel=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax classifier</span></span><br><span class="line">flatten = mx.sym.Flatten(data=pool2)</span><br><span class="line">fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=classes)</span><br><span class="line">model = mx.sym.SoftmaxOutput(data=fc1, name=<span class="string">"softmax"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the network architecture</span></span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><blockquote><p><strong>几点注意点：</strong></p><ol><li>在bottleneck版本的残差模块中，前两个卷积核的数目是第三个的1/4；</li><li>本次试验中使用了预激活版本的残差块；</li><li>整个网络中没有使用dropout层；</li><li>注意，网络结构一开始，先使用一个BN应用到输入数据，起到正则化的作用；</li><li>全局池化输出被flatten之后接一个FC层丢给softmax</li></ol></blockquote><h3 id="训练ResNet"><a href="#训练ResNet" class="headerlink" title="训练ResNet"></a>训练ResNet</h3><p>训练用的脚本和前面几个网络一致。</p><blockquote><p><strong>几个注意点：</strong></p><ol><li><code>batchSize = config.BATCH_SIZE * config.NUM_DEVICES</code>; 使用k80 12GB先存，选择batchSzie=<strong>32</strong>；</li><li>优化算法使用SGD，初始学习率为1e-1，动量0.9，L2权重正则化参数0.0001；rescale参数尤为关键，根据批的大小放大梯度：rescale_grad=1.0 / batchSize；（He论文的建议参数）</li><li>使用了MSRA进行参数初始化，initializer=mx.initializer.MSRAPrelu()；</li></ol></blockquote><h4 id="学习率的控制"><a href="#学习率的控制" class="headerlink" title="学习率的控制"></a>学习率的控制</h4><blockquote><p>训练ResNet由于初始学习率高，所以最终完成训练所需的迭代数目要小很多；</p></blockquote><div class="table-container"><table><thead><tr><th>Epoch</th><th>学习率</th></tr></thead><tbody><tr><td>1-64</td><td>1e-2</td></tr><tr><td>65-80</td><td>1e-3</td></tr><tr><td>81-89</td><td>1e-4</td></tr><tr><td>90-100</td><td>1e-5</td></tr></tbody></table></div><blockquote><ol><li>控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率；</li><li>调整学习率 <code>python train_alexnet.py --checkpoints checkpoints --prefix alexnet \ --start-epoch 50</code></li><li>在8个GPU上，每轮用时1000多秒；</li></ol></blockquote><ol><li><p>第一遍训练采用1e-1的学习率训练16轮，发现饱和现象，而且训练误差和测试误差约拉越大；</p><p><img src="/qnsource/images/2018-07-10-经典网络复现之ResNet/1-16.png" alt="1-16"></p><p><img src="/qnsource/images/2018-07-10-经典网络复现之ResNet/1-16-loss.png" alt="1-16-loss"></p></li><li><p>在第10个迭代调整学习率到1e-2，验证集准确率有大幅提升，代表调整有效，之后在16轮迭代之后再次降低学习率到1e-3,25轮调整学习率到1e-4，最终top-5的准确率为0.884073，top-1准确率为0.68338：</p><p><img src="/qnsource/images/2018-07-10-经典网络复现之ResNet/1-31-accuracy.png" alt="1-31-accuracy"><img src="/qnsource/images/2018-07-10-经典网络复现之ResNet/1-31-loss.png" alt="1-31-loss"></p></li></ol><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol><li><p>初始学习率的选择</p><p>初始学习率如果选择1e-2，与1e-1比，很早模型就出现过拟合现象，无法到达更高的训练精度；</p><p>比如使用1e-1，在第10轮调整学习率到1e-2之后，网络准确率会出现一个10%以上的跳变；</p></li><li><p>如果不采用预激活</p></li></ol><p>   3-5%点的性能下降，原因见作者原文分析；</p><ol><li>选取30轮的训练结果，在测试集数据上进行验证，结果如下：</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-1</span>: 71<span class="selector-class">.49</span>%</span><br><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-5</span>: 89<span class="selector-class">.96</span>%</span><br></pre></td></tr></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>效果不是十分理想，主要原因应该是第一阶段1e-1学习率的训练过程过早的认为过拟合打断了训练过程，应该让这个过程再持续一段时间观察效果，无奈训练一次ResNet实在太久，放弃继续尝试了；</li><li>训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源；</li><li>训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型；</li><li>先使用ReLU作为激活函数获得baseline，再选择提花为ELU来获得提升；</li><li>训练深层网络时，考虑使用MSRA/HE的初始化参数，并配合PReLU一起使用，效果更好；</li><li>对于ResNet要想再进一步提升性能，需要考虑加强正则化、更多的数据放大、合理的应用dropout等方式。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4ygzcmtw.bkt.clouddn.com/images/2018-05-21-ResNet/residual-Module-2.PNG&quot; alt=&quot;Residual Module&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 本文记录利用ImageNet数据集复现经典网络ResNet的过程，并记录在大型数据集训练过程中需要考虑的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="动手实践营" scheme="http://blog.a-stack.com/categories/%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E8%90%A5/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="网络复现" scheme="http://blog.a-stack.com/tags/%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>经典网络复现之AlexNet</title>
    <link href="http://blog.a-stack.com/2018/07/08/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0%E4%B9%8BAlexNet/"/>
    <id>http://blog.a-stack.com/2018/07/08/经典网络复现之AlexNet/</id>
    <published>2018-07-08T05:58:12.000Z</published>
    <updated>2018-07-20T15:52:42.648Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://p4ygzcmtw.bkt.clouddn.com/cover/12.jpg" alt="Test Picture"></p><p><strong>摘要：</strong> 本文记录利用ImageNet数据集复现经典网络AlexNet的过程，并记录在大型数据集训练过程中需要考虑的问题。</p><a id="more"></a><p>为了增强对state of the art深度神经网络模型的认知，准备进行一系列模型的复现工作，使用ImageNet的大规模图像分类数据集，从头训练各个经典的神经网络模型，同时结合作者论文对训练过程中的技巧进行归纳总结，主要安排如下几部分内容：</p><ul><li>ImageNet数据集及数据准备</li><li>AlexNet网络</li><li>VGG网络</li><li>GoogLeNet网络</li><li>ResNet网络</li><li>SqueezeNet网络</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>2012年的AlexNet是通用CNN的开山之作（早在1986年，Yann LeCun就利用卷积神经网络构建了LeNet并第一次将其运用到了生产环节——邮政编码识别，详见<a href="http://yann.lecun.com/exdb/lenet/a35.html" target="_blank" rel="noopener">http://yann.lecun.com/exdb/lenet/a35.html</a> ，但相关网络结构未能得到充分复制和发展），从此以后，CNN被广泛应用于各种机器视觉比赛、应用之中，得到了空前的发展。本文，主要根据AlexNet论文回顾其网络结构、设计技巧，并利用ImageNet的图像分类数据集复现在2012年ImageNet挑战赛中的结果。</p><ul><li>论文： Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classification with Deep Convolutional Neural Networks”. In: Advances in Neural Information Processing Systems 25. Edited by F. Pereira et al. Curran Associates, Inc., 2012, pages 1097–1105. URL: <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deepconvolutional-neural-networks.pdf" target="_blank" rel="noopener">http://papers.nips.cc/paper/4824-imagenet-classification-with-deepconvolutional-neural-networks.pdf</a></li><li>数据集： ImageNet 2012 ILSRVC数据集（数据集的获取及准备详见<a href="/2018/07/06/ImageNet-DataSet/">ImageNet-DataSet</a>）</li><li>计算框架： MxNet</li><li>算力资源：AWS云主机p2.8xlarge：8个Tesla K80 GPU ($7.20/hour)</li></ul><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/GPUs.png" alt="GPUs"></p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>AlexNet在2012年ImageNet ILSVRC比赛中top-5的错误率未15.3%，领先第二名10.9%个百分点，整个网络架构总共有8层参数网络层，其中5层卷积网络+3层全连接网络。本节将按照AlexNet论文描述的内容，整理AlexNet的特点，和极具借鉴性的创新成果。</p><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/AlexNet.png" alt="AlexNet"></p><p>AlexNet的网络架构如图所示，输入图像尺寸为（224，224，3），经过5个CNN层，3个FC层输出1000个分类结果，为了便于使网络训练采用GPU，所以整个网络分为上下两部分，分别在两个GPU上进行训练，其中第一层CNN和3个FC层在两个GPU上进行了参数共享。</p><p>AlexNet网络中拥有600万个参数，650 000个神经元。</p><h3 id="GPU的第一次使用"><a href="#GPU的第一次使用" class="headerlink" title="GPU的第一次使用"></a>GPU的第一次使用</h3><p>AlexNet是第一个使用GPU进行训练的深度神经网络，受当时硬件条件限制，使用了2块3GB内存的GTX 580 GPU，同时由于全部的网络参数超过了3GB的显存空间，所以将网络分成了两部分。训练时间上实际比单块GPU没有提升太多，但将错误率降低了1.2个百分点。</p><h3 id="激活函数的选择——从此ReLU函数得到重用"><a href="#激活函数的选择——从此ReLU函数得到重用" class="headerlink" title="激活函数的选择——从此ReLU函数得到重用"></a>激活函数的选择——从此ReLU函数得到重用</h3><p>AlexNet中经过实践测试发现，采用ReLU作为激活函数比tanh要快很多（在CIFAR-10上测试前者速度是后者的6倍多），这也奠定了ReLU称为后来CNN网络的首选。</p><h3 id="解决过拟合"><a href="#解决过拟合" class="headerlink" title="解决过拟合"></a>解决过拟合</h3><p>整个AlexNet都是围绕着计算效率提升和解决过拟合进行优化和尝试。AlexNet中的600万参数增强网络学习能力的同时也带来了潜在的过拟合问题，其中对解决过拟合的诸多尝试在未来的网络发展中被反复验证和使用。</p><h4 id="1-数据增强"><a href="#1-数据增强" class="headerlink" title="1. 数据增强"></a>1. 数据增强</h4><p>AlexNet中数据增强主要采用了两种方式，一种是随机扣取和水平翻转，另一种是白化。</p><ul><li>随机扣取和水平翻转：AlexNet将原始图像处理成256x256大小的统一图像，在训练阶段，随机从原始图像中扣取227x227的像素，并通过水平翻转来提升训练集的差异性，增强网络泛化性能；在测试阶段，对于输入测试图像分别从四个顶角和中心位置获得5张图像，并通过水平翻转形成10张图像，通过评价10张图像的预测结果作为最终输出结果，将网络性能提升了将近1个百分点；</li><li>白化： 利用PCA进行主成分抽取</li></ul><h4 id="2-Dropout"><a href="#2-Dropout" class="headerlink" title="2. Dropout"></a>2. Dropout</h4><p>这是Dropout的第一次正式使用，Dropout本身起到了模型组合的作用，同时起到正则化效果，可以避免过拟合，提升效果。当然代价是网络收敛时间被延长。</p><ul><li>在前两个FC层之后使用了Dropout</li></ul><h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><ul><li>优化算法选择SGD， 动量为0.9；</li><li>batch_size = 128</li><li>权重衰减为0.0005，这个很小的权重衰减对于模型学习起到了不可忽视的作用；</li><li>权重参数初始化为零均值，0.01标准差的高斯分布；</li><li>为了使得初始参数落在ReLU的正值区域，将偏差参数初始化为1；</li><li>初始学习率为0.01，当验证集错误率饱和时以对数形式降低学习率，一共降低3次，即1e-3,1e-4,1e-5；</li><li>训练了90个epoch，总共用时5-6天</li></ul><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul><li>使用5个CNN的组合，top-5错误率为16.4%​</li></ul><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>AlexNet还使用了局部响应归一化（Local Response Normalization）、重叠池化等优化策略，不过在未来的网络结构中没有得到良好的效果和应用普及。</p><h2 id="重头训练一个AlexNet"><a href="#重头训练一个AlexNet" class="headerlink" title="重头训练一个AlexNet"></a>重头训练一个AlexNet</h2><ol><li><p>ImageNet数据准备参见前序博文</p></li><li><p>由于现在GPU硬件和深度学习计算框架的发展，我们不需要将网络拆成两个部分，单独训练了，所以网络架构进行了微调，详见下图；</p><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/AlexNet-Details.png" alt="AlexNet-Details"></p><ol><li>使用MxNet训练网络比Keras + TensroFlow要快；</li></ol></li></ol><h3 id="使用MxNet构建AlexNet代码"><a href="#使用MxNet构建AlexNet代码" class="headerlink" title="使用MxNet构建AlexNet代码"></a>使用MxNet构建AlexNet代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MxAlexNet</span>:</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(classes)</span>:</span></span><br><span class="line"><span class="comment"># data input</span></span><br><span class="line">data = mx.sym.Variable(<span class="string">"data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #1: first CONV =&gt; RELU =&gt; POOL layer set</span></span><br><span class="line">conv1_1 = mx.sym.Convolution(data=data, kernel=(<span class="number">11</span>, <span class="number">11</span>),</span><br><span class="line">stride=(<span class="number">4</span>, <span class="number">4</span>), num_filter=<span class="number">96</span>)</span><br><span class="line">act1_1 = mx.sym.LeakyReLU(data=conv1_1, act_type=<span class="string">"elu"</span>)</span><br><span class="line">bn1_1 = mx.sym.BatchNorm(data=act1_1)</span><br><span class="line">pool1 = mx.sym.Pooling(data=bn1_1, pool_type=<span class="string">"max"</span>,</span><br><span class="line">kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">do1 = mx.sym.Dropout(data=pool1, p=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #2: second CONV =&gt; RELU =&gt; POOL layer set</span></span><br><span class="line">conv2_1 = mx.sym.Convolution(data=do1, kernel=(<span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">pad=(<span class="number">2</span>, <span class="number">2</span>), num_filter=<span class="number">256</span>)</span><br><span class="line">act2_1 = mx.sym.LeakyReLU(data=conv2_1, act_type=<span class="string">"elu"</span>)</span><br><span class="line">bn2_1 = mx.sym.BatchNorm(data=act2_1)</span><br><span class="line">pool2 = mx.sym.Pooling(data=bn2_1, pool_type=<span class="string">"max"</span>,</span><br><span class="line">kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">do2 = mx.sym.Dropout(data=pool2, p=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #3: (CONV =&gt; RELU) * 3 =&gt; POOL</span></span><br><span class="line">conv3_1 = mx.sym.Convolution(data=do2, kernel=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>), num_filter=<span class="number">384</span>)</span><br><span class="line">act3_1 = mx.sym.LeakyReLU(data=conv3_1, act_type=<span class="string">"elu"</span>)</span><br><span class="line">bn3_1 = mx.sym.BatchNorm(data=act3_1)</span><br><span class="line">conv3_2 = mx.sym.Convolution(data=bn3_1, kernel=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>), num_filter=<span class="number">384</span>)</span><br><span class="line">act3_2 = mx.sym.LeakyReLU(data=conv3_2, act_type=<span class="string">"elu"</span>)</span><br><span class="line">bn3_2 = mx.sym.BatchNorm(data=act3_2)</span><br><span class="line">conv3_3 = mx.sym.Convolution(data=bn3_2, kernel=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">pad=(<span class="number">1</span>, <span class="number">1</span>), num_filter=<span class="number">256</span>)</span><br><span class="line">act3_3 = mx.sym.LeakyReLU(data=conv3_3, act_type=<span class="string">"elu"</span>)</span><br><span class="line">bn3_3 = mx.sym.BatchNorm(data=act3_3)</span><br><span class="line">pool3 = mx.sym.Pooling(data=bn3_3, pool_type=<span class="string">"max"</span>,</span><br><span class="line">kernel=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">do3 = mx.sym.Dropout(data=pool3, p=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #4: first set of FC =&gt; RELU layers</span></span><br><span class="line">flatten = mx.sym.Flatten(data=do3)</span><br><span class="line">fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=<span class="number">4096</span>)</span><br><span class="line">act4_1 = mx.sym.LeakyReLU(data=fc1, act_type=<span class="string">"elu"</span>)</span><br><span class="line">bn4_1 = mx.sym.BatchNorm(data=act4_1)</span><br><span class="line">do4 = mx.sym.Dropout(data=bn4_1, p=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block #5: second set of FC =&gt; RELU layers</span></span><br><span class="line">fc2 = mx.sym.FullyConnected(data=do4, num_hidden=<span class="number">4096</span>)</span><br><span class="line">act5_1 = mx.sym.LeakyReLU(data=fc2, act_type=<span class="string">"elu"</span>)</span><br><span class="line">bn5_1 = mx.sym.BatchNorm(data=act5_1)</span><br><span class="line">do5 = mx.sym.Dropout(data=bn5_1, p=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax classifier</span></span><br><span class="line">fc3 = mx.sym.FullyConnected(data=do5, num_hidden=classes)</span><br><span class="line">model = mx.sym.SoftmaxOutput(data=fc3, name=<span class="string">"softmax"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># return the network architecture</span></span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><blockquote><p><strong>几点注意点：</strong></p><ol><li>主要采用了Caffe中实现的AlexNet版本；</li><li>使用ELU替代了ReLU激活函数；</li><li>每个block都使用了dropout，而不是原文只在两个FC层使用；其中CNN层参数为0.25，FC层为0.5；</li><li>使用了BN层加速网络训练，而且BN在激活函数之后使用；</li></ol></blockquote><h3 id="训练AlexNet"><a href="#训练AlexNet" class="headerlink" title="训练AlexNet"></a>训练AlexNet</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># USAGE</span></span><br><span class="line"><span class="comment"># python train_alexnet.py --checkpoints checkpoints --prefix alexnet</span></span><br><span class="line"><span class="comment"># python train_alexnet.py --checkpoints checkpoints --prefix alexnet --start-epoch 25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import the necessary packages</span></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> imagenet_alexnet_config <span class="keyword">as</span> config</span><br><span class="line"><span class="keyword">from</span> pyimagesearch.nn.mxconv <span class="keyword">import</span> MxAlexNet</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct the argument parse and parse the arguments</span></span><br><span class="line">ap = argparse.ArgumentParser()</span><br><span class="line">ap.add_argument(<span class="string">"-c"</span>, <span class="string">"--checkpoints"</span>, required=<span class="keyword">True</span>,</span><br><span class="line">help=<span class="string">"path to output checkpoint directory"</span>)</span><br><span class="line">ap.add_argument(<span class="string">"-p"</span>, <span class="string">"--prefix"</span>, required=<span class="keyword">True</span>,</span><br><span class="line">help=<span class="string">"name of model prefix"</span>)</span><br><span class="line">ap.add_argument(<span class="string">"-s"</span>, <span class="string">"--start-epoch"</span>, type=int, default=<span class="number">0</span>,</span><br><span class="line">help=<span class="string">"epoch to restart training at"</span>)</span><br><span class="line">args = vars(ap.parse_args())</span><br><span class="line"></span><br><span class="line"><span class="comment"># set the logging level and output file</span></span><br><span class="line">logging.basicConfig(level=logging.DEBUG,</span><br><span class="line">filename=<span class="string">"training_&#123;&#125;.log"</span>.format(args[<span class="string">"start_epoch"</span>]),</span><br><span class="line">filemode=<span class="string">"w"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the RGB means for the training set, then determine the batch</span></span><br><span class="line"><span class="comment"># size</span></span><br><span class="line">means = json.loads(open(config.DATASET_MEAN).read())</span><br><span class="line">batchSize = config.BATCH_SIZE * config.NUM_DEVICES</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct the training image iterator</span></span><br><span class="line">trainIter = mx.io.ImageRecordIter(</span><br><span class="line">path_imgrec=config.TRAIN_MX_REC,</span><br><span class="line">data_shape=(<span class="number">3</span>, <span class="number">227</span>, <span class="number">227</span>),</span><br><span class="line">batch_size=batchSize,</span><br><span class="line">rand_crop=<span class="keyword">True</span>,</span><br><span class="line">rand_mirror=<span class="keyword">True</span>,</span><br><span class="line">rotate=<span class="number">15</span>,</span><br><span class="line">max_shear_ratio=<span class="number">0.1</span>,</span><br><span class="line">mean_r=means[<span class="string">"R"</span>],</span><br><span class="line">mean_g=means[<span class="string">"G"</span>],</span><br><span class="line">mean_b=means[<span class="string">"B"</span>],</span><br><span class="line">preprocess_threads=config.NUM_DEVICES * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct the validation image iterator</span></span><br><span class="line">valIter = mx.io.ImageRecordIter(</span><br><span class="line">path_imgrec=config.VAL_MX_REC,</span><br><span class="line">data_shape=(<span class="number">3</span>, <span class="number">227</span>, <span class="number">227</span>),</span><br><span class="line">batch_size=batchSize,</span><br><span class="line">mean_r=means[<span class="string">"R"</span>],</span><br><span class="line">mean_g=means[<span class="string">"G"</span>],</span><br><span class="line">mean_b=means[<span class="string">"B"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize the optimizer</span></span><br><span class="line">opt = mx.optimizer.SGD(learning_rate=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, wd=<span class="number">0.0005</span>,</span><br><span class="line">rescale_grad=<span class="number">1.0</span> / batchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct the checkpoints path, initialize the model argument and</span></span><br><span class="line"><span class="comment"># auxiliary parameters</span></span><br><span class="line">checkpointsPath = os.path.sep.join([args[<span class="string">"checkpoints"</span>],</span><br><span class="line">args[<span class="string">"prefix"</span>]])</span><br><span class="line">argParams = <span class="keyword">None</span></span><br><span class="line">auxParams = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if there is no specific model starting epoch supplied, then</span></span><br><span class="line"><span class="comment"># initialize the network</span></span><br><span class="line"><span class="keyword">if</span> args[<span class="string">"start_epoch"</span>] &lt;= <span class="number">0</span>:</span><br><span class="line"><span class="comment"># build the LeNet architecture</span></span><br><span class="line">print(<span class="string">"[INFO] building network..."</span>)</span><br><span class="line">model = MxAlexNet.build(config.NUM_CLASSES)</span><br><span class="line"></span><br><span class="line"><span class="comment"># otherwise, a specific checkpoint was supplied</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># load the checkpoint from disk</span></span><br><span class="line">print(<span class="string">"[INFO] loading epoch &#123;&#125;..."</span>.format(args[<span class="string">"start_epoch"</span>]))</span><br><span class="line">model = mx.model.FeedForward.load(checkpointsPath,</span><br><span class="line">args[<span class="string">"start_epoch"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># update the model and parameters</span></span><br><span class="line">argParams = model.arg_params</span><br><span class="line">auxParams = model.aux_params</span><br><span class="line">model = model.symbol</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile the model</span></span><br><span class="line">model = mx.model.FeedForward(</span><br><span class="line">ctx=[mx.gpu(<span class="number">0</span>), mx.gpu(<span class="number">1</span>), mx.gpu(<span class="number">2</span>), mx.gpu(<span class="number">3</span>), mx.gpu(<span class="number">4</span>), mx.gpu(<span class="number">5</span>), mx.gpu(<span class="number">6</span>), mx.gpu(<span class="number">7</span>)],</span><br><span class="line">symbol=model,</span><br><span class="line">initializer=mx.initializer.Xavier(),</span><br><span class="line">arg_params=argParams,</span><br><span class="line">aux_params=auxParams,</span><br><span class="line">optimizer=opt,</span><br><span class="line">num_epoch=<span class="number">90</span>,</span><br><span class="line">begin_epoch=args[<span class="string">"start_epoch"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize the callbacks and evaluation metrics</span></span><br><span class="line">batchEndCBs = [mx.callback.Speedometer(batchSize, <span class="number">500</span>)]</span><br><span class="line">epochEndCBs = [mx.callback.do_checkpoint(checkpointsPath)]</span><br><span class="line">metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=<span class="number">5</span>),</span><br><span class="line">mx.metric.CrossEntropy()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># train the network</span></span><br><span class="line">print(<span class="string">"[INFO] training network..."</span>)</span><br><span class="line">model.fit(</span><br><span class="line">X=trainIter,</span><br><span class="line">eval_data=valIter,</span><br><span class="line">eval_metric=metrics,</span><br><span class="line">batch_end_callback=batchEndCBs,</span><br><span class="line">epoch_end_callback=epochEndCBs)</span><br></pre></td></tr></table></figure><blockquote><p><strong>几个注意点：</strong></p><ol><li><code>batchSize = config.BATCH_SIZE * config.NUM_DEVICES</code>;</li><li>优化算法使用SGD，初始学习率为1e-2，动量0.9，L2权重正则化参数0.0005；rescale参数尤为关键，根据批的大小放大梯度；</li><li>model中的<code>ctx</code>参数用于指定用于训练的GPU；</li><li>使用了Xavier进行参数初始化，与原模型略有不同，Xavier是目前CNN网络常采用的参数初始化方式；</li></ol></blockquote><h4 id="学习率的控制"><a href="#学习率的控制" class="headerlink" title="学习率的控制"></a>学习率的控制</h4><div class="table-container"><table><thead><tr><th>Epoch</th><th>学习率</th></tr></thead><tbody><tr><td>1-80</td><td>1e-2</td></tr><tr><td>81-100</td><td>1e-3</td></tr><tr><td>86-100</td><td>1e-4</td></tr></tbody></table></div><blockquote><ol><li>控制每轮学习率修改的观察窗口要在10-15个epoch之后再下结论，确定该阶段验证集准确率饱和了再行降低学习率；</li><li>调整学习率 <code>python train_alexnet.py --checkpoints checkpoints --prefix alexnet \ --start-epoch 50</code></li><li>在8个GPU上，每轮用时600多秒（当然我batch size设的比较小，这个速度可以提升一个数量级）；</li></ol></blockquote><ol><li><p>第一遍训练采用1e-2的学习率训练90轮，发现80轮以后，验证集准确率已经不再增加；为此在80轮之后调整学习率；80轮的性能参数如下：</p><ul><li><p>INFO:root:Epoch[78] Validation-accuracy=0.517843</p><p>INFO:root:Epoch[78] Validation-top_k_accuracy_5=0.759725</p><p>INFO:root:Epoch[78] Validation-cross-entropy=2.123822</p><p>INFO:root:Saved checkpoint to “checkpoints/1//alexnet01-0080.params”</p></li></ul><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/1st_try_accuracy.png" alt="1st_try_accuracy"></p><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/1st_try_loss.png" alt="1st_try_loss"></p></li><li><p>将学习率调整到1e-3之后，验证集准确率有大幅提升，代表调整有效，100轮之后再度饱和，降低学习率到1e-4;105轮验证集结果如下：</p><ul><li><p>INFO:root:Epoch[105] Validation-accuracy=0.564901</p><p>INFO:root:Epoch[105] Validation-top_k_accuracy_5=<strong>0.796509</strong></p><p>INFO:root:Epoch[105] Validation-cross-entropy=1.880376</p></li></ul><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/105epoch-accuracy.png" alt="105epoch-accuracy"></p><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/105epoch.png" alt="105epoch"></p></li><li><p>保持1e-4学习率，训练到125轮，进一步降低学习率到1e-5，发现整个网络没有性能提升，结束该批次参数训练过程。</p></li></ol><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/130epoch-accuracy.png" alt="130epoch-accuracy"></p><p><img src="/qnsource/images/2018-07-08-经典网络复现之AlexNet/130epoch-loss.png" alt="130epoch-loss"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol><li><p>BN放在激活之前还是之后？</p><p>调整BN位置，准确率从77.9%—&gt; 79.6%</p></li><li><p>ReLU vs ELU ？</p><p>用ELU替代ReLU，提升1-2%的性能；</p></li><li><p>选取125轮的训练结果，在测试集数据上进行验证，结果如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-1</span>: 60<span class="selector-class">.20</span>%</span><br><span class="line"><span class="selector-attr">[INFO]</span> <span class="selector-tag">rank-5</span>: 81<span class="selector-class">.99</span>%</span><br></pre></td></tr></table></figure><p>​</p></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>训练一个大型数据集的深度神经网络是个费时费力的活，为了获得最优的参数，一般需要进行10-100次参数实验，需要极大的耐心和计算资源；</li><li>训练深度神经网络的目的不是找寻全局最优解，因为一般很难找到这个解，我们只是在探寻一个比上次效果更好的模型；</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p4ygzcmtw.bkt.clouddn.com/cover/12.jpg&quot; alt=&quot;Test Picture&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt; 本文记录利用ImageNet数据集复现经典网络AlexNet的过程，并记录在大型数据集训练过程中需要考虑的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="动手实践营" scheme="http://blog.a-stack.com/categories/%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E8%90%A5/"/>
    
    
      <category term="算法" scheme="http://blog.a-stack.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="文献" scheme="http://blog.a-stack.com/tags/%E6%96%87%E7%8C%AE/"/>
    
      <category term="人工智能" scheme="http://blog.a-stack.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="网络复现" scheme="http://blog.a-stack.com/tags/%E7%BD%91%E7%BB%9C%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
</feed>
